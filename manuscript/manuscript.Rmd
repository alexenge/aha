---
title             : "Instant effects of semantic information on visual perception"
shorttitle        : "Semantically informed perception"
author: 
  - name          : "Alexander Enge"
    affiliation   : "1,2"
    corresponding : yes
    address       : "Rudower Chaussee 18, 12489 Berlin, Germany"
    email         : "alexander.enge@hu-berlin.de"
  - name          : "Franziska Süß"
    affiliation   : "3"
  - name          : "Rasha Abdel Rahman"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Humboldt-Universität zu Berlin"
  - id            : "2"
    institution   : "Max Planck Institute for Human Cognitive and Brain Sciences"
  - id            : "3"
    institution   : "Fachhochschule des Mittelstands"
authornote: |
  \addORCIDlink{Alexander Enge}{0000-0003-0100-2297}
  
  \addORCIDlink{Rasha Abdel Rahman}{0000-0002-8438-1570}
  
  The authors declare no competing financial interests.
  The preprocessed data and analaysis code for this study are openly available at https://osf.io/uksbc/.
abstract: |
  Does our perception of an object change as soon as we discover what function it serves?
  This question matters not only when we encounter novel tools or gadgets in our everyday lives, but also pertains to the long-standing debate around the (im)penetrability of perception by higher cognitive capacities.
  Here, we showed human participants (*n* = 48) pictures of unfamiliar objects either together with words matching their function, leading to semantically informed perception, or together with non-matching words, resulting in naive perception.
  We measured event-related potentials (ERPs) to investigate at which stages in the visual processing hierarchy these two types of object perception differed from one another.
  We found that semantically informed as compared to naive perception was associated with larger amplitudes in the N170 component (150--200 ms) and reduced amplitudes in the N400 component (400--700 ms).
  When the same objects were presented once more without any information, the N400 effect persisted and we also observed enlarged amplitudes in the P1 component (100--150 ms) in response to objects for which semantically informed perception had taken place.
  Consistent with previous work, our results suggest that obtaining semantic information about previously unfamiliar objects alters aspects of their lower-level visual perception (P1 component), higher-level visual perception (N170 component), and semantic processing (N400 component).
  We demonstrate for the first time that these effects are instantaneous in the sense that they can be observed within the same trial as well as one trial after the information has been given, providing strong evidence that cognition affects perception.
keywords          : "objects, semantic knowledge, visual perception, event-related potentials"
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
bibliography      : ["references.bib", "r-references.bib"]
csl               : "apa.csl"
documentclass     : "apa7"
classoption       : "man,donotrepeattitle"
fontsize          : "10pt"
output:
  papaja::apa6_pdf:
    latex_engine: xelatex
  papaja::apa6_docx: default
editor_options: 
  chunk_output_type: console
header-includes:
  - \geometry{a4paper}
  - \fancyheadoffset[R,L]{0pt}
  - \raggedbottom
  - \usepackage[all]{nowidow}
  - \usepackage[bottom]{footmisc}
  - \interfootnotelinepenalty=10000
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \usepackage{makecell}
  - \renewcommand{\cellset}{\renewcommand{\arraystretch}{0.7}}
  - \captionsetup{font={stretch=1}, belowskip=15pt}
---

```{r, setup, include=FALSE}
# Load packages
library(buildmer)
library(cowplot)
library(emmeans)
library(here)
library(jsonlite)
library(knitr)
library(lme4)
library(lmerTest)
library(magick)
library(MASS)
library(papaja)
library(parallel)
library(reticulate)
library(scales)
library(tidyverse)
library(magrittr)

# Load helper functions
source(here("manuscript", "plots_tables.R"))

# Write R package citations to bibliography
r_refs(here("manuscript", "r-references.bib"), append = FALSE)

# Set global chunk options
opts_chunk$set(fig.width = 10, include = FALSE, out.width = "100%")

# Re-run preprocessing and/or model fitting?
run <- list(
  preprocessing = FALSE,
  model_fitting = FALSE,
  time_frequency = FALSE
)

# File names for easier input/output
deps <- c("logRT", "P1", "N170", "N400") # dependent variables
io <- list(
  montage = "results/montage.csv",
  obj_per_cond = "results/obj-per-cond.csv",
  trials = "results/trials.csv",
  ave_grand = "results/ave_grand.csv",
  ave_tfr_flat = "results/ave-tfr_flat.csv",
  ave_tfr_diff = "results/ave-tfr_diff.csv",
  models_lme = "results/models_lme.RDS",
  models_brm = str_c("results/model_brm_", deps, ".RDS") %>% set_names(deps),
  model_brm_prior = "results/model_brm_prior.RDS",
  models_control = "results/models_control.RDS",
  tfr_stats = "results/tfr_stats.json",
  tfr_grand_ave = "results/tfr_grand_ave.csv"
)
```

# Introduction

Does our perception of an object change as soon as we discover what function it serves?
This question speaks not only to our everyday experiences, where we encounter novel tools and gadgets in our dynamic working and private environments.
It also pertains to the long-standing debate about the cognitive (im)penetrability [@pylyshyn1999] of perception by higher-level capacities such as semantic knowledge or language.
According to one view, these cognitive capacities kick in only after the retinal input has been processed by a specialized module for visual perception [@firestone2016; @fodor1983].
This module is encapsulated from higher-level inputs and therefore processes the visual information in a purely feedforward fashion, progressing from lower areas with small receptive field sizes to areas representing increasingly complex shapes and, eventually, whole objects [@dicarlo2012].
This architecture has been mirrored in classical [e.g., @marr1982] and contemporary [e.g., @krizhevsky2012] computer vision models, allowing them to achieve human-level perfomance in predicting object category labels from images [for review, see @lindsay2020].

The cognitive impenetrability hypothesis is challenged by the alternative view that perception dynamically interacts with different aspects of cognition from early on [@churchland1994; @lupyan2015].
This view is supported by a variety of theoretical, behavioral, and neurophysiological accounts.

On the theoretical level, the reverse hierarchy theory [@ahissar2004; @hochstein2002] posits that conscious processing initally occurs at the level of whole objects or object categories.
Only after this high-level interpretation has been obtained, more fine grained visual details---if relevant for the current task---are being accessed via top-down connections.
Along similar lines, Bayesian inference and predictive coding theories [e.g., @clark2013; @panichello2013; @yuille2006] propose bi-directional loops of predictions and prediction errors between visual and non-visual processes.

On the behavioral level, these ideas are supported by psychological studies that showed differences in ratings, detection rates, or reaction times for visual stimuli depending on their emotional [e.g., @phelps2016], motivational [e.g., @balcetis2010], linguistic [e.g., @boutonnet2015] or semantic [e.g., @gauthier2003] background.
However, some of these studies confounded tentantive high-level effects with low-level differences between conditions.
Furthermore, many studies were not able to discern between perceptual and post-perceptual (e.g., memory-related) effects [@firestone2016].

On the neurophysiological level, event-related potentials (ERPs) measured from the human EEG can mitigate most of these concerns:
Their excellent temporal resolution allows us to probe how early we can detect influences of high-level (e.g., semantic) information.
To this end, participants were typically trained to associate visual objects with different amounts of semantic information.
After training, an orthogonal task was used to compare the ERPs in response to semantically trained objects versus non-semantically trained objects.
This revealed differences not only in late ERP components associated with semantic processing (i.e., the N400 component) but also in the visual P1 component [@abdelrahman2008; @maier2019; @samaha2018; @weller2019].
The early peak of this component and its source in the occipital cortex [@abdelrahman2008] point to an immediate effect of semantic knowledge on visual perception.

Here we measured ERPs in response to unfamiliar objects instantly while participants gained an semantic understanding of their function.
To this end, we presented half of the objects together with matching keywords, thus allowing participants to understand what kind of object they were viweing, or together with non-matching keywords, thus keeping the perception of the object semantically naive.
We then presented the same objects again to test for downstream effects of semantic information as in previous studies.
We examined the influence of semantic information on ERPs associated with lower-level visual perception (P1 component), higher-level visual perception (N170 component), and semantic processing (N400 component).

# Materials and Methods

## Participants

Participants were 48 German native speakers (31 female, 17 male) with a mean age of 23.5 years (range 18--32) and no history of psychological disorder or treatment.
No a priori power analysis was carried out and the sample size was chosen in line with previous EEG studies on object processing in our lab.
All participants were right-handed according to the Edinburgh inventory [@oldfield1971] and reported normal or corrected-to-normal vision.
They provided written informed consent before starting the experiment and received a compensation of €8 per hour for participating.

## Materials

Stimuli consisted of 240 grayscale photographs of real-world objects.
Of these, 120 were well-known everyday objects (e.g., a bicycle, a toothbrush) and served as filler stimuli of no interest.
The other 120 were rare objects presumed to be unfamiliar to the majority of participants (e.g., a galvanometer, an udu drum; see Appendix A).
All stimuli were presented on a light blue background with a size of 207 × 207 pixels on a 19-inch LCD monitor with a resolution of 1,280 × 1,024 pixels and a refresh rate of 75 Hz.
At a standardized viewing distance of 90 cm, the images subtended approximately 3.9 degrees of participants' horizontal and vertical visual angle.

For each unfamiliar object, a pair of German keywords---a noun and a verb---was selected, describing the typical function or use of the object in a way that could typically be related to its visual features and their configuration (e.g., voltage, measuring; clay pot, drumming).
As our central experimental manipulation, the presentation of half of the objects was preceded by keywords that correctly matched their respective function, whereas the presentation of the other half of the objects was preceded by non-matching keywords belonging to one of the other objects.
The matching keywords were expected to induce semantically informed perception (i.e., participants suddenly understanding what kind of object they were viewing), whereas the non-matching keywords were expected to prevent such an understanding and keep the perception of the object semantically naive.
All participants saw each unfamiliar object with only one type of keywords (matching or non-matching).
This assignment of keywords to objects was counterbalanced across participants so that each object was presented with matching keywords (leading to semantically informed perception) and non-matching keywords (leading to naive perception) to an equal number of participants.
The experiment was programmed and displayed using Presentation® software (Neurobehavioral Systems, Inc., Berkeley, CA, www.neurobs.com).

## Experimental Design

The experiment consisted of three parts (see Figure \@ref(fig:fig1)A).
In the *pre-insight* part, after written informed consent had been obtained and the EEG had been prepared, all 240 familiar and unfamiliar objects were presented once in random order and without any keywords.
Each trial consisted of a fixation cross presented in the middle of the screen for 0.5 s, followed by the presentation of the object until participants made a response or until a time out after 3 s.
The inter-trial interval was 0.5 s and participants took a self-timed break after each block of 60 objects.
The task, which was kept the same across all three parts, was to classify each object using one of four response alternatives: (a) "I know what this is or have a strong assumption," (b) "I have an assumption what this is," (c) "I have rather no assumption what this is," or (d) "I don't know what this is and have no assumption." Participants were asked to respond as quickly and as accurately as possible by pressing one out of four buttons with the index or middle finger of their left or right hand, respectively.
The mapping of the rating scale to the four buttons (left to right or right to left) was counterbalanced across participants.

```{r, preprocessing}
# List EEG files and behavioral log files
fname_vhdr <- c(
  list.files("data/raw/eeg/exp1", pattern = ".vhdr", full.names = TRUE),
  list.files("data/raw/eeg/exp2", pattern = ".vhdr", full.names = TRUE)
)
fname_log <- c(
  list.files("data/raw/rt/exp1", pattern = ".txt", full.names = TRUE),
  list.files("data/raw/rt/exp2", pattern = ".txt", full.names = TRUE)
)

# Generate new BIDS-style subject IDs (sub-01, sub-02, ...)
seq_along(fname_vhdr) %>%
  str_pad(width = 2, pad = 0) %>%
  str_c("sub-", .) -> subject_id

# Define ERP components of interest
erp_components <- tibble(
  name = c("P1", "N170", "N400"),
  tmin = c(0.100, 0.150, 0.400),
  tmax = c(0.150, 0.200, 0.700),
  roi = list(
    c("PO3", "PO4", "POz", "O1", "O2", "Oz"),
    c("P7", "P8", "PO7", "PO8", "PO9", "PO10"),
    c("C1", "C2", "Cz", "CP1", "CP2", "CPz")
  )
)

# Define frequencies and number of cycles for TFR analysis
tfr_freqs <- seq(4, 50, by = 2)
tfr_cycles <- seq(3, 10, length.out = length(tfr_freqs))

# Run preprocessing for all subjects using MNE-Python (see preprocessing.py)
# Will run only if `run$preprocessing = TRUE` and if raw data are available
if (run$preprocessing) {

  # Import the custom Python functions for preprocessing
  pp <- import_from_path("preprocessing", path = here("manuscript"))

  # Do the actual preprocessing
  tibble(fname_vhdr, fname_log, subject_id) %>%
    pwalk(
      pp$preprocess,
      output_dir = "data/preprocessed",
      resample_sfreq = 250,
      eog_names = c("HEOG", "VEOG"),
      eog_anodes = c("F9", "Auge_u"),
      eog_cathodes = c("F10", "Fp1"),
      montage_kind = "easycap-M1",
      ica_components = as.integer(15),
      ica_method = "fastica",
      random_seed = as.integer(1234),
      hipass = 0.1,
      lowpass = 30,
      event_id = dict(match = as.integer(221), mismatch = as.integer(222)),
      tmin = -0.5,
      tmax = 1.489,
      baseline = tuple(-0.2, 0),
      reject = dict(eeg = 200e-6),
      erp_components = erp_components,
      average_by = c("part", "condition", "subject_id"),
      tfr_baseline = tuple(-0.3, -0.1),
      tfr_equalize = TRUE,
      condition_col = "condition",
      item_col = "item_id",
      drop_conditions = c("Excl_informed", "Excl_naive", "Excl_known"),
      tfr_freqs = tfr_freqs,
      tfr_cycles = tfr_cycles,
      tfr_crop = tuple(-0.3, 0.7)
    )

  # Combine single-trial data for all subjects
  dir.create("results", showWarnings = FALSE)
  str_c("data/preprocessed/", subject_id, "_trials.csv") %>%
    map(read_csv, col_types = cols()) %>%
    bind_rows() %>%
    write_csv(io$trials)

  # Get standard EasyCap electrode positions and project them into 2D space
  read_csv("data/preprocessed/sub-01_tfr-ave.csv", col_types = cols()) %>%
    select(-c(part, condition, subject_id, time, freq)) %>%
    colnames() -> ch_names
  import("mne.channels._standard_montage_utils")$MONTAGE_PATH %>%
    str_c("/easycap-M1.txt") %>%
    read_tsv(col_types = cols()) %>%
    rename(electrode = Site) %>%
    inner_join(tibble(electrode = ch_names), ., by = "electrode") %>%
    mutate(
      x = eegUtils:::deg2rad(Theta) * cos(eegUtils:::deg2rad(Phi)),
      y = eegUtils:::deg2rad(Theta) * sin(eegUtils:::deg2rad(Phi)),
    ) %>%
    mutate(across(c(x, y), round, 3)) %>%
    write_csv(io$montage)

  # Determine number of objects per condition for each participant
  fname_log %>%
    map2(subject_id, pp$read_log) %>%
    bind_rows() %>%
    with(table(subject_id, condition) / 3) %>%
    as.data.frame.matrix() %>%
    rownames_to_column("subject_id") %>%
    write_csv(io$obj_per_cond)
}
```

```{python, grand_averaging, eval=run$preprocessing}
import numpy as np
import pandas as pd

# Read evoked *potentials* for all subjects
subs = r.subject_id
fnames_ave = [f"data/preprocessed/{sub}_ave.csv" for sub in subs]
ave = [pd.read_csv(fname) for fname in fnames_ave]
ave = pd.concat(ave)

# Average across subjects
id_vars = ["part", "condition", "time"]
ave = ave.groupby(id_vars, as_index=False).mean()
ave.to_csv(r.io["ave_grand"], float_format="%.3f", index=False)

# Read evoked *power* for all subjects
fnames_ave_tfr = [f"data/preprocessed/{sub}_tfr-ave.csv" for sub in subs]
ave_tfr = [pd.read_csv(fname) for fname in fnames_ave_tfr]
ave_tfr = pd.concat(ave_tfr)

# # Subject 18 is weird!
# ave_tfr = ave_tfr[ave_tfr["subject_id"] != "sub-18"]

# Compute difference in power between conditions
id_vars = ["part", "condition", "subject_id", "time", "freq"]
electrodes = [col for col in ave_tfr.columns if (col not in id_vars)]
ave_tfr_diff = (
    ave_tfr.query("condition == 'Informed'")[electrodes]
    - ave_tfr.query("condition == 'Naive'")[electrodes].values
)
ave_tfr_diff[id_vars] = ave_tfr.query("condition == 'Informed'")[id_vars]

# Compute flat average across conditions
id_vars.remove("condition")
ave_tfr_flat = ave_tfr.groupby(id_vars, as_index=False).mean()

# Average across subjects and save
id_vars.remove("subject_id")
dfs = [ave_tfr_flat, ave_tfr_diff]
fnames_tfr = [r.io["ave_tfr_flat"], r.io["ave_tfr_diff"]]
for df, fname in zip(dfs, fnames_tfr):
    df = df.groupby(id_vars, as_index=False).mean()
    df.to_csv(fname, float_format="%.3f", index=False)
del df, dfs, ave, ave_tfr, ave_tfr_diff, ave_tfr_flat
```

```{r, models_lme}
# Compute average number of objects per condition
(obj_per_cond <- read_csv(io$obj_per_cond, col_types = cols()) %>%
  summarise(across(.cols = -subject_id, .fns = mean)) %>%
  as.list())

# Read single-trial data for all subjects
str_c("data/preprocessed/", subject_id, "_trials.csv") %>%
  map(read_csv, col_types = cols()) %>%
  bind_rows() %>%
  mutate(
    part = factor(part, levels = c("I", "II", "III")),
    subject_id = factor(subject_id),
    item_id = factor(item_id),
    RT = ifelse(RT < 200, NA, RT),
    logRT = log(RT)
  ) -> trials

# Record number of rejected ERP epochs per participant
(rejected <- trials %>%
  count(subject_id) %>%
  mutate(n = 360 - n) %>%
  pull(n))

# Remove trials with unrealistically short RTs or invalid conditions
sum(is.na(trials$RT))
sum(is.na(trials$RT)) / nrow(trials)
trials %<>% na.omit()

# Contrast coding for parts
cbind(c("I" = -1, "II" = 1, "III" = 0), c("I" = 0, "II" = -1, "III" = 1)) %>%
  t() %>%
  ginv() -> contrasts(trials$part)

# Remove irrelevant conditions for main analysis
trials_main <- mutate(
  trials,
  condition = factor(condition, levels = c("Informed", "Naive")),
)

# Contrast coding for conditions
c("Informed" = 1, "Naive" = -1) %>%
  t() %>%
  ginv() -> contrasts(trials_main$condition)

# Load fitted models or rerun them
if (!run$model_fitting) {

  # Load models
  models_lme <- readRDS(io$models_lme)
} else {

  # Define function for building a single mixed-effect model
  build_lme <- function(dep, formula, data, n_cores) {

    # Convert formula to terms list
    formula <- tabulate.formula(formula)

    # Automic model selection via likelihood ratio tests
    # using an alpha level of .20 (see Matuschek et al., 2017, J Mem Lang)
    n_cores <- detectCores()
    cl <- makeCluster(n_cores)
    build <- buildmer(
      buildmerControl = buildmerControl(
        formula,
        data,
        direction = c("backward", "backward"),
        cl = cl,
        crit = "LRT",
        elim = function(logp) exp(logp) >= .20,
        calc.anova = TRUE,
        ddf = "Satterthwaite",
        dep = dep,
        REML = FALSE,
      ),
      control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e6))
    )
    stopCluster(cl)

    # Compute follow-up comparisons
    emm_options(lmer.df = "Satterthwaite", lmerTest.limit = Inf)
    specs <- pairwise ~ condition | part
    em <- emmeans(build@model, specs, infer = TRUE)

    # Return a list of relevant model outputs
    return(
      list(
        model = build@model,
        anova = build@anova,
        summary = build@summary,
        means = as.data.frame(em$emmeans),
        contrasts = as.data.frame(em$contrasts)
      )
    )
  }

  # Compute models for all dependent variables in parallel
  form <- RT ~ part * condition +
    (part * condition | subject_id) +
    (part * condition | item_id)
  map(
    deps,
    build_lme,
    formula = form,
    data = trials_main,
    n_cores = n_cores
  ) %>%
    set_names(deps) %T>%
    saveRDS(io$models_lme) -> models_lme
}
```

```{r, models_brm, eval=FALSE}
# Theme for plots
bayesplot_theme_update(text = element_text(family = "Helvetica", size = 10))
color_scheme_set("purple")

# Define example model formula
formula_brm <- bf(
  P1 ~ part * condition +
    (part * condition | subject_id) +
    (part * condition | item_id),
  sigma ~ 1 + (1 | subject_id)
)

# Default prior for slopes and sigma
prior <- c(
  prior(cauchy(0, 0.707), class = b),
  prior(student_t(3, 0, 5.7), class = Intercept, dpar = sigma),
  prior(normal(0, 5), class = sd, group = subject_id, dpar = sigma)
)

# Hypotheses of comparisons for informed vs. naive within each part
hypotheses <- c(
  "I" = "condition1 - 2/3 * part1:condition1 - 1/3 * part2:condition1 = 0",
  "II" = "condition1 + 1/3 * part1:condition1 - 1/3 * part2:condition1 = 0",
  "III" = "condition1 + 1/3 * part1:condition1 + 2/3 * part2:condition1 = 0"
)

# Fit Bayesian mixed model using prior information only
fit_brm_prior <- brm(
  formula = formula_brm,
  data = trials,
  family = gaussian(),
  prior = prior,
  sample_prior = "only",
  save_pars = save_pars(all = TRUE),
  chains = 4,
  iter = 20000,
  warmup = 2000,
  file = io$model_brm_prior,
  file_refit = "never"
)

# Prior predictive checks for population-level parameters
prior_summary(fit_brm_prior)
summary(fit_brm_prior)
mcmc_plot(fit_brm_prior, "areas", pars = "^b_", prob_outer = 0.95)

# Compute posterior distributions for dependent variables
deps <- c("logRT", "P1", "N170", "N400")
map(deps, function(dep) {

  # Define model formula for the current dependent variable
  formula_dv <- str_c(
    dep, " ~ part * condition +
      (part * condition | subject_id) +
      (part * condition | item_id)"
  ) %>%
    formula()

  # Sample from the posterior distribution
  fit_brm <- update(
    fit_brm_prior,
    formula = formula_dv,
    newdata = trials,
    sample_prior = "yes",
    file = io$models_brm[[dep]],
    file_refit = "never"
  )

  # Compute follow-up comparisons with Bayes factors
  hyps <- hypothesis(fit_brm, hypotheses) %>% {
    mutate(
      .$hypothesis,
      bf01 = Evid.Ratio, bf10 = 1 / Evid.Ratio, .keep = "unused"
    )
  }

  # Return a list of relevant model outputs
  return(
    list(
      model = fit_brm,
      summary = summary(fit_brm),
      hypotheses = hyps
    )
  )
}) %>%
  set_names(deps) -> models_brm

# Posterior predictive checks for population-level parameters
example_model <- models_brm$N400
example_model$summary
example_model$hypotheses
mcmc_plot(example_model$model, "areas", pars = "^b_", prob_outer = 0.95) +
  theme(text = element_text(family = "Helvetica", size = 10))

# Posterior predictive checks subject by subject
pp_check(
  example_model$model,
  type = "dens_overlay_grouped",
  nsamples = 50,
  group = "subject_id"
)
pp_check(
  example_model$model,
  type = "stat_grouped",
  nsamples = 1000,
  group = "subject_id",
  stat = "sd"
)
```

```{r, fig1, include=TRUE, fig.height=10, fig.cap="(ref:figure-1-caption)"}
# Load montage and evoked data
montage <- read_csv(io$montage, col_types = cols())
evokeds <- read_csv(io$ave_grand, col_types = cols(), progress = FALSE)

# Create plot for Experiment 1
plot_fig1(erp_components, evokeds, trials_main, models_lme)
```

(ref:figure-1-caption) Experimental procedure and ERP results. (A) In the pre-insight part, participants were presented with 120 unfamiliar objects and indicated whether they knew what kind of object they were viewing.
In the insight part, half of these objects were presented with matching keywords (in purple color for illustration), leading to semantically informed perception, and the other half with non-matching keywords (in petrol color for illustration), leading to naive perception.
In the post-insight part, the same objects were presented again without keywords.
(B) ERP waveforms and scalp topographies are shown for objects with semantically informed versus naive perception within the three different parts.
Semantically informed perception was associated with more negative amplitudes in the N170 component during the insight part, less negative amplitudes in the N400 component during the insight and post-insight parts, and more positive amplitudes in the P1 component during the post-insight part. Ampl. = amplitude.\newline
\* *p* $<$ .05. \*\* *p* $<$ .01. \*\*\* *p* $<$ .001.

In the *insight* part, the 120 unfamiliar objects were presented for the second time, now preceded either by matching keywords (leading to semantically informed perception) or by non-matching keywords (leading to naive perception).
Each trial consisted of a fixation cross presented for 0.5 s, followed by the presentation of the keywords for 2.5 s.
Then, an asterisk was presented in the middle of the screen for another 0.5 s, followed by the presentation of the object until a response was made or until a time out after 3 s.
The objects were presented in blocks of 30 trials so that within each block (a) there were 15 objects from each of the two experimental conditions and (b) objects were heterogeneous in terms of their shape, visual complexity, and functional category (e.g., medical devices, musical instruments).

In the *post-insight* part, the unfamiliar objects were presented for a third time with the same trial structure as in the pre-insight part, that is, without any keywords.
The insight and post-insight parts were presented in an interleaved fashion so that after the presentation of one block of 30 objects in the insight part (with keywords), participants took a self-timed break and continued with the same block of 30 objects in the post-insight part (without keywords) before moving on to the next block consisting of 30 different objects.
They continued like this until all four blocks were completed in both parts.
In total, the experiment consisted of 480 trials (120 familiar objects in the pre-insight part and 120 unfamiliar objects in the pre-insight, insight, and post-insight parts).
It took participants approximately 35 minutes to complete.

## EEG Recording and Preprocessing

The continuous EEG was recorded from 62 Ag/AgCl scalp electrodes placed according to the extended 10--20 system [@americanelectroencephalographicsociety1991] and referenced online to an external electrode placed on the left mastoid (M1).
Two additional external electrodes were placed on the right mastoid (M2) and below the left eye (IO1), respectively.
Electrode impedances were kept below 5 kΩ.
An online band-pass filter with a high-pass time-constant of 10 s (0.016 Hz) and a low-pass cutoff frequency of 1000 Hz was applied before digitizing the signal at a sampling rate of 500 Hz.

The data were preprocessed offline using the MNE software [Version `r system("mne --version", intern = TRUE) %>% str_remove("MNE ")`; @gramfort2013] in Python [Version `r system("python3 --version", intern = TRUE) %>% str_remove("Python ")`; @vanrossum2009].
First, all scalp electrodes were re-referenced to the common average.
Next, artifacts resulting from blinks and eye movements were removed using independent component analysis (ICA).
The first 15 components were extracted with the FastICA algorithm [@hyvarinen1999] after temporarily low-pass filtering the data at 1 Hz.
Any components showing substantive correlations with either of two virtual EOG channels (VEOG: IO1 - Fp1, HEOG: F9 - F10) were removed automatically using the *find_bads_eog* function.
After artifact correction, a zero-phase, non-causal FIR filter with a lower pass-band edge at 0.1 Hz (transition bandwidth: 0.1 Hz) and an upper pass-band edge at 30 Hz (transition bandwidth: 7.5 Hz) was applied.
Next, the continuous EEG was epoched into segments of 2 s, starting 500 ms before the onset of the presentation of each unfamiliar object.
The epochs were baseline-corrected by subtracting the average voltage during the 200 ms before stimulus onset.
Epochs containing artifacts despite ICA, defined as peak-to-peak amplitudes exceeding 200 µV, were removed from further analysis.
This led to the exclusion of an average of `r format(round(mean(rejected), 1), nsmall = 1)` trials per participant (= `r percent(mean(rejected) / 360, accuracy = 0.1)`; range `r min(rejected)`--`r max(rejected)` trials).
Single-trial event-related potentials were computed as the mean amplitude across time windows and regions of interests (ROIs) defined a priori, namely 100--150 ms after object onset at electrodes PO3, PO4, POz, O1, O2, and Oz for the P1 component, 150--200 ms after object onset at electrodes P7, P8, PO7, PO8, PO9, and PO10 for the N170 component, and 400--700 ms after object onset at electrodes C1, C2, Cz, CP1, CP2, and CPz for the N400 component.

## Statistical Analyses

First, we excluded from all further analyses those objects for which the participant had responded with "I know what this is" in the pre-insight part (i.e., before any keywords were presented).
This led to the exclusion of an average of `r format(round(obj_per_cond[["Excl_known"]], 1), nsmall = 1)` objects per participant (= `r percent(obj_per_cond[["Excl_known"]] / 120, accuracy = 0.1)` of unfamiliar objects).
Next, to delineate semantically informed and naive perception, the assignment of all other objects to one of these two conditions was co-determined by our experimental manipulation (matching versus non-matching keywords in the insight part) and the behavioral responses of the participant (see Figure \@ref(fig:fig1)A).
Objects were assigned to the semantically informed condition only if they were presented with matching keywords *and* if the participant indicated knowing what the object was or having an assumption.
This was the case for an average of `r format(round(obj_per_cond[["Informed"]], 1), nsmall = 1)` objects per participant (= `r percent(obj_per_cond[["Informed"]]/60, accuracy = 0.1)` of objects presented with matching keywords).
Complementarily, objects were assigned to the naive condition only if they were presented with non-matching keywords *and* if the participant indicated not knowing what the object was or having rather no assumption.
This was the case for an average of `r format(round(obj_per_cond[["Naive"]], 1), nsmall = 1)` objects per participant (= `r percent(obj_per_cond[["Naive"]]/60, accuracy = 0.1)` of objects presented with non-matching keywords).
The same assignment of objects to conditions (based on the manipulation and responses in the insight part) was also used for analyzing the pre-insight and post-insight parts.
This allowed us to test, on the one hand, if the objects from both conditions differed in important aspects even before any keywords were presented (pre-insight part) and, on the other hand, if the semantic understanding acquired in the insight part had any down-stream effects on the subsequent perception of the objects (post-insight part).

The event-related potentials in response to objects from both conditions and all three parts were analyzed on the single-trial level using linear mixed-effects regression models [@baayen2008; @fromer2018].
Compared to traditional by-participant or by-item analyses of variance, these models allow tighter control over the false positive rate because they simultaneously account for by-participant and by-item non-independencies in the data [@burki2018; @judd2012]. Furthermore, they flexibly handle unbalanced data.
This was important in our case because of the rejection of bad ERP epochs as well as the way we constructed our experimental conditions (see above). 

Three separate models were computed predicting P1, N170, and N400 mean amplitudes, respectively.
All models included three fixed effects: (a) the part of the experiment, coded as a repeated contrast [i.e., subtracting the first part from the second part and the second part from the third part, the intercept being the grand mean across all three parts; @schad2020], (b) the condition of the object, coded as a scaled sum contrast (i.e., subtracting the naive condition from the semantically informed condition, the intercept being the grand mean across both conditions), and (c) the two-way interaction of part and condition.
To determine the random effects structure, we always started with the maximal model containing by-participant and by-item random intercepts and random slopes for all fixed effects [@barr2013].
We then performed a model selection algorithm proposed by @matuschek2017 to increase statistical power and avoid overparameterization: Iteratively, each random effect was removed and the resulting, more parsimonious model was compared to the previous, more complex model by means of a likelihood ratio test. Only if the parsimonious model explained the data similarly well as the complex model [determined by *p* $\geq$ .20; @matuschek2017] did we leave the random effect out, otherwise it was kept in the final model (see Appendix B for the resulting formulas after model selection).
All models were computed in R [Version `r as.character(packageVersion("base"))`; @R-base] using the lme4 package [Version `r as.character(packageVersion("lme4"))`; @R-lme4].
The optimizer function *bobyqa* with a maximum of 10^6^ iterations was used for maximum likelihood estimation.
The model selection algorithm via likelihood ratio tests was performed using the buildmer package [Version `r as.character(packageVersion("buildmer"))`; @R-buildmer].

To investiage if semantically informed perception had an influence on the ERPs within each part, planned follow-up comparisons were calculated, contrasting the semantically informed condition against the naive condition within the pre-insight, insight, and post-insight parts.
This was done using the emmeans package [Version `r as.character(packageVersion("emmeans"))`; @R-emmeans].
All *p*-values were computed by approximating the relevant denominator degrees of freedom using Satterthwaite's method as implemented in the lmerTest package [Version `r as.character(packageVersion("lmerTest"))`; @R-lmerTest].

The materials, single-trial preprocessed data, and code for data analysis are openly available at <https://osf.io/uksbc/>.

## Time-frequency analysis

Semantically informed perception may manifest itself not only in changes in evoked activity (as captured by the ERP analysis), but also in induced activity.
We therefore carried out an exploratory time-frequency analysis.
We first created new epochs from the ICA-corrected but unfiltered raw data.
To avoid differences between conditions due to a difference in noise levels, we equated the number of objects per condition for each participant by discarding a random sample of objects from the semantically naive condition.
Epochs that were marked as bad for the ERP analysis (defined as peak-to-peak amplitudes exceeding 200 µV) were also removed from the time-frequency analysis.
The remaining epochs were then convolved with a family of 24 Morlet wavelets using the short-time Fourier transform.
The frequency of these wavelets increased linearly from 4 Hz to 50 Hz in steps of 2 Hz and the number of cycles increased linearly from 3 cycles to 10 cycles in steps of 0.304 cycles [see @samaha2018].
The resulting single-trial time-frequency data were converted into percent signal change to adjust for power-law scaling.
This was achieved by first subtracting and then dividing by the mean acitivity during the baseline period, defined as 300--100 ms before object onset.

Because we did not have any a priori hypothesis for the time-frequency domain, we used cluster-based permutation tests [@maris2007] to explore the effects of semantically informed perception as compared to naive perception within each part of the experiment.
We first created condition averages for each participant, part, and condition.
Separately for each of the three parts, we then computed the by-participant differences between the semantically informed condition and the naive condition.
Permutation tests were then run on these differences to identify clusters across space (62 electrodes), frequencies (4–40 Hz in steps of 2 Hz), and time (restricted to 0–700 ms after obejct onset), using the *permutation_cluster_1samp_test* function in MNE.
This function (a) identified data points where the difference between conditions (one sample *t*-test) was significant at *p* $<$ .05, (b) combined neighboring data points where this was the case into observed clusters, and (c) computed a *p* value for each of these observed clusters by comparing its mass to an empirical null distribution of largest cluster masses obtained from 1,000 random permutations.
Data points were defined as neighboring each other based on the Delaunay triangulation in 2D channel space (function *find_ch_adjacency*) and based on lattice adjacency matrices in the frequency and time domains (i.e., considering only directly adjacent frequency bands and time points).
Observed clusters for which the cluster mass exceeded 95% of permuted cluster masses (i.e., non-parametric *p* < .05) were deemed as showing reliable effects of semantically informed perception in each part.

# Results

```{r, tab1, include=TRUE, results="asis"}
# Create table of LMM results
tab1 <- create_table(
  models = models_lme[c("P1", "N170", "N400")],
  stub_anova = c("Part", "Condition", "Pt. × con."),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption = paste(
    "Linear Mixed-Effects Regression Models",
    "\\smallskip"
  ),
  note = paste(
    "Pt. = part, con. = condition, est. = estimate, CI = 95\\%",
    "confidence interval."
  )
)
```

Averaged across conditions, P1, N170, and N400 amplitudes differed as a function of the part of the experiment (see Table \@ref(tab:tab1)).
In addition, N400 amplitudes differed between the informed and the naive condition averaged across the three parts of the experiment.
Crucially, the part × condition interaction was significant for all three components.
To answer our main research question, we decomposed these interactions into the simple effects of the semantically informed condition versus the naive condition within each of the three different parts of the experiment.

## ERPs Before Insight Was Occurring

In the pre-insight part, when objects were unfamiliar to participants and presented without keywords, no differences emerged between the semantically informed and the naive condition in the P1, N170, or N400 component (see Table \@ref(tab:tab1) and Figure \@ref(fig:fig1)B).
This was expected because given that (a) the critical presentation of the keywords (leading either to semantically informed perception or to naive perception) had not yet taken place and (b) the assignment of objects to conditions was counterbalanced across participants as to control for low-level visual differences between conditions.

## ERPs While Insight Was Occurring

In the insight part, half of the unfamiliar objects were presented with matching keywords, leading to semanitcally informed perception, and the other half were presented with non-matching keywords, keeping the perceptio nof the objects semantically naive.
During semantically informed perception, the amplitude of the N170 component was significantly enlarged (i.e., more negative) and the amplitude of the N400 component was significantly reduced (i.e., less negative).
There were no reliable differences in the P1 component.

## ERPs After Insight Had Occurred

In the post-insight part, the unfamiliar objects were presented for a third time, again without the keywords (as in the pre-insight part).
As in the insight part, the N400 component remained significantly reduced during semantically informed as compared to naive perception, whereas the effect in the N170 component did not reoccur.
Instead, the P1 component was significantly enlarged (i.e., more positive) in response to objects for which semantically informed perception had taken place.

## Control Analysis

```{r models_control}
# Use a different control condition ("failed insight")
trials_control <- mutate(
  trials,
  condition = factor(condition, levels = c("Informed", "Excl_informed")),
)

# Contrast coding for conditions
c("Informed" = 1, "Excl_informed" = -1) %>%
  t() %>%
  ginv() -> contrasts(trials_control$condition)


# Load fitted models or rerun them
if (!run$model_fitting) {

  # Load models
  models_control <- readRDS(io$models_control)
} else {

  # Compute models for all dependent variables in parallel
  form <- RT ~ part * condition +
    (part * condition | subject_id) +
    (part * condition | item_id)
  map(
    deps,
    build_lme,
    formula = form,
    data = trials_control,
    n_cores = n_cores
  ) %>%
    set_names(deps) %T>%
    saveRDS(io$models_control) -> models_control
}
```

```{r, tab2, include=TRUE, results="asis"}
# Create table of LMM results
tab2 <- create_table(
  models = models_control[c("P1", "N170", "N400")],
  stub_anova = c("Part", "Condition", "Pt. × con."),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption = paste(
    "Control Analysis",
    "\\smallskip"
  ),
  note = paste(
    "Pt. = part, con. = condition, est. = estimate, CI = 95\\%",
    "confidence interval."
  )
)
```

In the main analysis described above, objects in the semantically informed condition (matching keywords) were contrasted against objects in the naive condition (non-matching keywords) as the baseline condition.
This may raise the question if the N170 and/or N400 effects in the insight part could be due to a mismatch response for objects in the naive condition.
To rule out this alternative explanation, we repeated the analysis using a different baseline condition, namely those objects that were presented with matching keywords (as in the informed condition) but on which participants failed to capitalize according to their behavioral responses.
This was the case for `r percent(obj_per_cond[["Excl_informed"]] / 60, accuracy = 0.1)` of objects presented with matching keywords.
As before, there semantically informed perception led to a reliable increase in the N170 component and a reliable decrease in the N400 component (see Table \@ref(tab:tab2)).

## Time-frequency analysis

```{python, tfr, eval=run$time_frequency}
import numpy as np
import pandas as pd
from mne import read_epochs
from mne.channels import find_ch_adjacency
from mne.stats import combine_adjacency, permutation_cluster_1samp_test
from scipy import sparse

# Read evoked power for all subjects
fnames = [f"data/preprocessed/{sid}_tfr-ave.csv" for sid in r.subject_id]
tfrs = [pd.read_csv(fname) for fname in fnames]
tfrs = pd.concat(tfrs)

# Get electrode names
electrodes = r.montage["electrode"].tolist()

# Define non-channel columns to sort by
idx_cols = ["condition", "subject_id", "time", "freq"]

# Prepare empty lists to store outputs of permutation tests
t_obs = []
clusters = []
p_values = []
h0_max = []

# Loop over parts
parts = ["I", "II", "III"]
for part in parts:

    # Get data for the current part
    df = tfrs[tfrs["part"] == part]
    df = df.sort_values(idx_cols)

    # Restrict time points after object onset
    df = df[(df["time"] >= 0) & (df["time"] <= 700)]

    # Extract TFR data per condition
    dat_informed = df.loc[df["condition"] == "Informed", electrodes].values
    dat_naive = df.loc[df["condition"] == "Naive", electrodes].values
    dat_diff = dat_informed - dat_naive  # Shape (subs x times x freqs) x elecs

    # Reshape to X matrix for permutation test
    n_subjects = len(df["subject_id"].unique())
    n_times = len(df["time"].unique())
    n_freqs = len(df["freq"].unique())
    X = dat_diff.reshape(n_subjects, n_times, n_freqs, -1)

    # # Plot the data matrix, averaged across participants and channels
    # import matplotlib.pyplot as plt
    # plt.imshow(X.mean(axis=(0, 3)).T, aspect="auto", vmin=-0.5, vmax=0.5, origin='lower'); plt.show()

    # Define adjacency matrix for frequencies
    adjacency_freq = sparse.csr_matrix(np.ones((n_freqs, n_freqs)))

    # Define adjacency matrix for channels based on channel positions
    example_epochs = read_epochs("data/preprocessed/sub-01_epo.fif", preload=False)
    adjacency_chan, ch_names = find_ch_adjacency(example_epochs.info, ch_type="eeg")

    # Combine adjacency matrices for frequencies and channels
    adjacency = combine_adjacency(adjacency_freq, adjacency_chan)

    # # Plot the combined adjacency matrix
    # plt.imshow(adjacency.toarray(), origin='lower', interpolation='nearest'); plt.show()

    # Run permutation test and save results
    cluster_stats = permutation_cluster_1samp_test(
        X, adjacency=adjacency, n_permutations=1000
    )

    # Store outputs
    t_obs.append(cluster_stats[0])
    clusters.append(cluster_stats[1])
    p_values.append(cluster_stats[2])
    h0_max.append(cluster_stats[3])

# Gather outputs in DataFrame and save
cluster_stats_df = pd.DataFrame(
    {
        "part": parts,
        "t_obs": t_obs,
        "clusters": clusters,
        "p_values": p_values,
        "h0_max": h0_max,
    }
)
cluster_stats_df.to_json(r.io["tfr_stats"])

# Export averaged data for plotting
ave_cols = ["part", "condition", "time", "freq"]
tfrs_plotting = tfrs.groupby(ave_cols, as_index=False).mean()  # Across subjects
tfrs_plotting["power"] = tfrs_plotting[electrodes].mean(axis=1)  # Across channels
tfrs_plotting = tfrs_plotting.drop(electrodes, axis=1)
tfrs_plotting.to_csv(r.io["tfr_grand_ave"], index=False)
```

```{r, fig2, include=TRUE, fig.height=4, fig.cap="(ref:figure-2-caption)"}
# Load permutation test results
jsonlite::read_json(io$tfr_stats) %>%
  as_tibble() -> tfr_stats

# Get smallest cluster p value for each part
tfr_stats$p_values %>%
  map(function(p_values, digits = 3) {
    p_values %>%
      as.numeric() %>%
      min() %>%
      round(digits) %>%
      format(trim = TRUE, digits = digits, nsmall = digits) -> min_p
    min_p <- ifelse(min_p == "1.000", "1", str_replace(min_p, "0.", "."))
  }) %>%
  set_names(c("I", "II", "III")) -> tfr_min_ps

# Plot time-frequency results per part and condition
read_csv(io$tfr_grand_ave, col_types = cols(), progress = FALSE) %>%
  plot_fig2()
```

(ref:figure-2-caption) Time-frequency results. Each panel shows the time-frequency representation for one of the three parts of the experiment (left: pre-insight part, middle: insight part, right: post-insight part) and one of the two experimental conditions (top: semantically informed, bottom: naive).
For plotting, the time-frequency data were averaged across all 48 participants and all 62 EEG channels.
According to cluster-based permutation tests, there were no reliable differences between the two conditions in either of the three parts.

Cluster-based permutation tests were run on a time-frequency representation of the ERP data for each part, obtained using Morlet wavelet convolution.
Always comparing the semantically informed condition to the naive condition, there were no signicantly large clusters in the pre-insight part (all *p*s $\geq$ `r tfr_min_ps[["I"]]`), in the insight part (all *p*s $\geq$ `r tfr_min_ps[["II"]]`), or in the post-insight part (all *p*s $\geq$ `r tfr_min_ps[["III"]]`).

# Discussion

Here we investigated if obtaining a semantic understanding of previously unfamiliar objects has an influence on how we perceive them.
To this end, we measured ERPs while participants viewed unfamiliar objects before, while, and after receiving semantic information about them.
For half of the objects, this information was matching the object, thus leading to semantically informed perception, whereas for the other half of the objects, the information was non-matching, thus keeping the perception of the object semantically naive.
We found semantically informed perception to be accompanied by enlarged (i.e., more negative) N170 amplitudes and reduced (i.e., less negative) N400 amplitudes.
When the same objects were presented again without the semantic information, the N400 component remained significantly reduced and we also observed a modulation of the P1 component, which was enlarged (i.e., more positive) in response to objects that had previously triggered semantically informed perception.
We will discuss each of these effects in turn, starting with the latest (i.e., post-perceptual) effect and moving backward in time to the earlier (i.e., more perceptual) effects.

The reduction of the N400 component (400--700 ms after object onset) during semantically informed perception was the numerically largest and most robust effect.
Perhaps least controversially, this effect indicates that acquiring an understanding of the objects (in the insight part) lessened participants' demand for effortful semantic processing in comparison to the naive condition [@kutas2011]. This replicates previous work showing that N400 amplitudes are larger in response to pictures when they are either difficult to understand in and of themselves [e.g., @abdelrahman2008; @supp2005] or difficult to integrate into the preceding context [e.g., @barrett1990; @ganis1996; @hirschfeld2011]. The time course of this effect and the computational role of the N400 [@lau2008; @rabovsky2018] suggest that it has a post-perceptual locus.

In contrast to the N400, the N170 component (150--200 ms after object onset) was modulated while but not after the objects were presented together with the relevant semantic information.
As such, it can be seen as an online marker of semantic insight, that is, participants suddenly understanding the visual objects in the light of the additional information provided by the verbal keywords.
The N170 is typically associated with the holistic perception of faces [@eimer2011; @sagiv2001] and other stimuli of visual expertise [@rossion2002; @tanaka2001]. It being enlarged during semantically informed perception may therefore reflect that the additional semantic information made participants experience the configuration of the visual features of the objects in a new and meaningful way. This interpretation is supported by previous findings with a similar experimental paradigm in the domain of face perception [@bentin2002]: In this study, participants showed a face-like (i.e., enlarged) N170 response to a scrambled version of a schematic face after (but not before) they were primed with the intact version of the same face. This effect was absent when a visual control stimulus (a non-face object) was shown instead of the intact face. In the domain of non-face stimuli, enlarged N170 amplitudes have also been observed when participants were asked to discriminate between composite line drawings of meaningful objects as compared to composite line drawings of non-objects [@beaucousin2011].
Of note, this effect was present only when participants were asked to decide based on the global shape of the object and it was reversed in polarity when they were asked to decide based on the constituent parts of the object.
Together with the present study, these findings suggest an online impact of meaningfulness on the higher-level (i.e., holistic) perception of visual objects, integrating across their visual features.

The P1 component (100--150 ms after object onset), unlike the N400 and N170 components, was modulated by semantic information only one trial after the information had been obtained.
This is consistent with previous studies showing modulations of the P1 component when participants learned meaningful information about previously unfamiliar objects [@abdelrahman2008; @maier2018; @maier2019; @weller2019] or about familiar objects that were rendered difficult to recognize [@samaha2018]. What the present study adds to these findings is that the P1 effect does not take an extensive learning phase to develop (with multiple presentations of the objects together with the respective information). Instead, it can be observed as soon as one trial after semantic insight has happened. Because the P1 is typically associated with lower-level sensory processing [e.g., @johannes1995; @luck2014; @pratt2011], we take its susceptibility to semantic information as an indicator that knowledge about the function of an object can change how we perceive it visually.

Both the N170 and the P1 components therefore seem to be sensitive to the semantic meaningfulness of visual objects.
However, the finding that these two components were modulated in different parts of our experimental design suggests that they reflect different aspects of top-down processing with different time courses and neuroanatomical implementations.
It has been pointed out that the time course of the N170 component is consistent with a top-down influence of (non-visual) areas in the prefrontal and parietal cortices on visual areas, whereas modulations of the P1 component seem to reflect recurrent processing *within* the visual system [@wyatte2014].
Here we could show that the former pathway seems to be able to convey semantic information instantaneously (i.e., within the same trial), whereas the latter seems to take at least one---but apparently also not more than one---additional presentation of the visual object to emerge.

While the limited spatial resolution of the EEG precludes a precise localization of these effects within the ventral stream for object recognition, there is converging evidence coming from fMRI showing that semantic information can feed back into the earliest of visual areas.
@hsieh2010 showed participants indiscernible two-tone ("Mooney") versions of images before and after showing them the original versions.
They found that the brain responses to the original image were correlated more strongly with the second presentation of the Mooney image (after insight had taken place) than with the first presentation of the Mooney image (before insight had taken place).
This increase in representational similarity was not just observed in higher-level object-sensitive areas in the lateral occipital cortex (LOC), but also in early retinotopic cortex (areas V1, V2, and V3).
Both of these cortical regions are consistent with the neural generators of the N170 and P1 components in the ERP which we have found to be sensitive to the semantically informed perception of previously unfamiliar objects.

On a theoretical level, the top-down modulation of these visual ERPs by semantic information challenges a modular view of visual perception [@firestone2016; @fodor1983; @pylyshyn1999; but see @clarke2020]. However, proponents of such a modular view have pointed out important shortcomings of previous studies that claimed to demonstrate top-down effects of cognition on perception [@firestone2016; @machery2015].
We took care to address as many of these shortcomings as possible: First, we showed that no effect had been present before any semantic information was being presented (in the pre-insight part).
Second, we used an objective and time-resolved measure (ERPs) to disentangle effects with a perceptual locus from those with a post-perceptual locus.
Third, we reduced response and demand biases by keeping the manipulation (i.e., matching or non-matching keywords) obscure to the participants and by including well-known objects as filler stimuli.
Fourth, we precluded low-level visual differences between conditions by counterbalancing the assignment of objects to conditions across participants.
Fifth, we reduced priming and attentional effects by presenting all objects in a randomized order and at the same location.
Sixth, we reduced memory effects by using only unfamiliar objects and by measuring online ERPs rather than delayed behavioral responses.
We hope that these procedures have effectively ruled out some of the most important alternative explanations for the top-down effects that we have observed, thus making a more compelling case against the cognitive impenetrability of perception.

An interactive view of object vision with an abundance of top-down feedback also challenges the predominantly feed-forward models in computer vision [e.g., @marr1982; @lindsay2020]. In fact, the lack of a semantic knowledge base that dynamically interacts with the processing of lower-level visual features may be one key reason why even state-of-the-art deep-learning algorithms need orders of magnitude more training examples to achieve human-level performance in object recognition. For these network models, single-trial learning of previously unfamiliar objects, as was observed on the behavioral and on the neurophysiological level in the present study, seems to be out of reach until they overcome this "barrier of meaning" [@mitchell2020].
Drawing inspiration from cognitive psychology and human neuroscientific data may help to make these models more biologically plausible and, at the same time, more data efficient.

A theoretical framework that would explicitly predict or explain the observed P1 and N170 effects in our study is lacking at present.
The effects are consistent, however, with the reverse hierarchy theory [@ahissar2004; @hochstein2002] which posits that objects first enter our visual consciousness at an abstract, conceptual level. Once this initial "vision at a glance" has taken place, feedback connections to earlier layers of the visual system are being accessed to extract the relevant lower-level features ("vision with scrutiny"). This reverse trajectory down the visual hierarchy may explain (a) the semantically induced changes to the fMRI signal in LOC and retinotopic cortex [@hsieh2010] as well as (b) the modulations of early visual ERP components observed in the present study and others [@abdelrahman2008; @maier2014; @maier2019; @samaha2018; @weller2019]. Besides this specific theory, an important role of top-down mechanisms for vision or, more specifically, object recognition is also posited by the family of predictive coding and Bayesian inference theories [e.g., @clark2013; @lupyan2015; @panichello2013; @yuille2006]. Despite the theoretical advances, the mechanistic details of these top-down effects at the algorithmic and implementational level [@marr1982] remain to be clarified.

The lack of mechanistic insight into the top-down effects that we have observed is one limitation of the present study.
Another one is our reliance on rare and highly specialized objects (see Appendix A).
This was necessary to induce the experience of semantic insight in a population of undergraduate students but the results may not necessarily generalize to the way in which we learn about everyday objects.[^2]
One way of addressing this issue would be to adapt the present paradigm to younger participants, using everyday objects which they have not yet learned about.
Finally, the number participants (*n* = 24 per experiment) and trials (*k* $\approx$ 30 per part in the insight condition and *k* $\approx$ 50 per part in the naive condition) can be considered small by today's standards [@baker2020]. Our analysis may therefore not have been particularly sensitive, especially given that linear mixed-effects regression models tend to have limited statistical power under a wide range of circumstances [see, e.g., the simulations by @matuschek2017].
This lack of power may also explain why one of the effects that had been observed in the first experiment (i.e., the enlargement of the N170 component, *p* = `r tab1$conts["Insight part", "N170_p"]`) failed to reach statistical significance in the replication experiment (*p* = `r tab2$conts["Insight part", "N170_p"]`).
To discern if this was due to a lack of statistical power or due to the actual absence of an effect, it would take yet another (more highly powered) replication study and/or a different data-analytic approach where it is possible to estimate the evidence both for the alternative hypothesis and for the null hypothesis (e.g., Bayesian linear mixed models).
It should be noted, however, that the joint analysis of both data sets yielded a very robust N170 effect (*p* = `r tab1$conts["Insight part", "N170_p"]`).
This makes it less likely that the effect observed in Experiment 1 was due to a false positive and instead suggests that statistical power in Experiment 2 was insufficient to render the effect significant.
Also note that the effect in the P1 component was statistically significant in both experiments individually, making an even stronger case for the susceptibility of early cortical processing to newly acquired knowledge about objects.

[^2]: Note, however, the converging evidence coming from the complementary approach of rendering images of everyday objects difficult to recognize [@samaha2018].

Taken together, this study provides preliminary evidence that whenever we receive meaningful semantic information about a previously unfamiliar object, this information has an immediate influence on the processing of the object.
This influence is immediate in at least two different ways: First, it does not require extensive training and can instead be observed within the same trial in which the information has been presented (and/or one trial later).
Second, the time course of this influence suggests that it manifests itself not only at later, post-perceptual stages---typically associated with semantic processing---but also at earlier stages---typically associated with visual perception itself and happening within less than a fifth of a second after the object is presented to us.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

---
title             : "Instant effects of semantic information on visual perception"
shorttitle        : "Semantically informed perception"

author: 
  - name          : "Alexander Enge"
    affiliation   : "1,2"
    corresponding : "yes"
    address       : "Rudower Chaussee 18, 12489 Berlin, Germany"
    email         : "alexander.enge@hu-berlin.de"
  - name          : "Franziska Süß"
    affiliation   : "3"
  - name          : "Rasha Abdel Rahman"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Humboldt-Universität zu Berlin"
  - id            : "2"
    institution   : "Research Group Learning in Early Childhood, Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig"
  - id            : "3"
    institution   : "Fachhochschule des Mittelstands, Bamberg"

authornote: |
  Alexander Enge \orcidlink{0000-0003-0100-2297} [https://orcid.org/0000-0003-0100-2297](0000-0003-0100-2297)
  
  Rasha Abdel Rahman \orcidlink{0000-0002-8438-1570} [https://orcid.org/0000-0002-8438-1570](0000-0002-8438-1570)
  
  Number of pages: 23

  Number of figures: 4

  Number of tables: 0

  Number of words: 212 (abstract), 849 (introduction), 1,856 (discussion)

  The authors declare no competing financial interests.

  We would like to thank Nele Langosch for assistance with data collection as well as Guido Kiecker for technical support.
  The materials and code for this study are available at [https://osf.io/uksbc](https://osf.io/uksbc).

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

bibliography      : ["manuscript_files/references.bib", "manuscript_files/r-references.bib"]
csl               : "manuscript_files/citation_style.csl"

documentclass     : "apa6"
classoption       : "man,donotrepeattitle"
fontsize          : "10pt"

editor_options: 
  chunk_output_type: console
  
output:
  papaja::apa6_pdf:
    latex_engine: xelatex
  papaja::apa6_docx: default

header-includes:
  - \geometry{a4paper}
  - \raggedbottom
  - \usepackage{orcidlink}
  - \usepackage[all]{nowidow}
  - \captionsetup[figure]{belowskip=10pt}
  - \captionsetup[figure]{font={small,stretch=1},textfont=normalfont}
  - \captionsetup[figure]{labelfont=bf,labelsep=period,name=Figure}
  - \captionsetup[table]{labelfont=bf,labelsep=period,textfont=bf}
  - \usepackage[export]{adjustbox}
  - \usepackage{makecell}
  - \usepackage{setspace}
---

```{r, setup, include=FALSE}
# Load R packages
library(buildmer)
library(emmeans)
library(here)
library(knitr)
library(lmerTest)
library(MASS)
library(scales)
library(papaja)
library(eegUtils)
library(cowplot)
library(parallel)
library(reticulate)
library(tidyverse)
library(magrittr)

# Set global options
options(readr.show_col_types = FALSE)
opts_chunk$set(
  fig.pos = "tp",
  fig.width = 12,
  message = FALSE,
  out.width = "100%",
  warning = FALSE
)

# Specify directory paths
data_dir <- here("data")
files_dir <- here("manuscript_files")
output_dir <- here("output")
report_dir <- here("output/qc_reports")

# Load custom helper functions for creating figures and tables
source(here(files_dir, "helper_functions.R"))

# Re-run steps that take a long time to compute?
run <- list(
  eeg_processing = FALSE,
  mixed_models = FALSE,
  control_analysis = FALSE,
  figures = FALSE
)

# Write R packages to bibliography
r_refs(here(files_dir, "r-references.bib"), append = FALSE)
```

# Abstract

Does our perception of an object change once we discover what function it serves?
We showed human participants (*n* = 48) pictures of unfamiliar objects either together with words matching their function, leading to semantically informed perception, or together with non-matching words, resulting in naive perception.
We measured event-related potentials (ERPs) to investigate at which stages in the visual processing hierarchy these two types of object perception differed from one another.
We found that semantically informed as compared to naive perception was associated with larger amplitudes in the N170 component (150--200 ms), reduced amplitudes in the N400 component (400--700 ms), and a late decrease in alpha/beta band power.
When the same objects were presented once more without any information, the N400 and event-related power effects persisted, and we also observed enlarged amplitudes in the P1 component (100--150 ms) in response to objects for which semantically informed perception had taken place.
Consistent with previous work, this suggests that obtaining semantic information about previously unfamiliar objects alters aspects of their lower-level visual perception (P1 component), higher-level visual perception (N170 component), and semantic processing (N400 component, event-related power).
Our study is the first to show that these effects are instantaneous, that is, observable within zero to one trial after any semantic information has been presented.

*Key words*: objects; semantic knowledge; visual perception; event-related potentials

\bigskip

# Significance Statement

There has been a long-standing debate about whether or not higher-level cognitive capacities such as semantic knowledge can influence lower-level perceptual processing in a top-down fashion.
Here we could show for the first time that information about the function of previously unfamiliar objects immediately influences cortical processing within less than 200 ms.
Of note, this influence does not require training or experience with the objects and related semantic information.
Therefore, our study is the first to show effects of cognition on perception while ruling out the possibility that prior knowledge merely acts by pre-activating or altering stored visual representations.
Instead, this knowledge seems to alter perception online, thus providing a compelling case against the impenetrability of perception by cognition.

\newpage

# Introduction

Does our perception of an object change once we discover what function it serves?
This question speaks not only to our everyday experiences, where we encounter novel tools and gadgets in our dynamic working and private environments.
It also pertains to the long-standing debate about the cognitive (im)penetrability [@pylyshyn1999] of perception by higher-level capacities such as semantic knowledge or language.
According to one view, these cognitive capacities kick in only after the retinal input has been processed by a specialized module for visual perception [@firestone2016; @fodor1983].
This module is encapsulated from higher-level inputs and therefore processes the visual information in a purely feed-forward fashion, progressing from lower areas with small receptive field sizes to areas representing increasingly complex shapes and, eventually, whole objects [@dicarlo2012].
This architecture has been mirrored in classical [e.g., @marr1982] and more contemporary [e.g., @krizhevsky2012] computer vision models, allowing them to achieve human-level performance in predicting object category labels from images [for review, see @lindsay2020].

The cognitive impenetrability hypothesis is challenged by the alternative view that perception dynamically interacts with different aspects of cognition from early on [@churchland1994; @lupyan2015].
This view is supported by a variety of theoretical, behavioral, and neurophysiological accounts.
On the theoretical level, the reverse hierarchy theory [@ahissar2004; @hochstein2002] posits that conscious processing initially occurs at the level of whole objects or object categories.
Only after this high-level interpretation has been obtained, more fine-grained visual details---if relevant for the current task---are being accessed via top-down connections.
Along similar lines, Bayesian inference and predictive coding theories [e.g., @yuille2006; @clark2013; @panichello2013] propose bi-directional loops of predictions and prediction errors between visual and non-visual processes.

On the behavioral level, these ideas are supported by psychological studies that found differences in ratings, detection rates, or reaction times for visual stimuli depending on their emotional [e.g., @phelps2016], motivational [e.g., @balcetis2010], linguistic [e.g., @boutonnet2015] or semantic [e.g., @gauthier2003] content.
However, some of these studies have received legitimate criticism for confounding tentative high-level effects with low-level differences between conditions, as well as for not being able to discern between perceptual and post-perceptual (e.g., memory-related) effects [@firestone2016].

On the neurophysiological level, event-related potentials (ERPs) measured from the human EEG can mitigate most of these concerns:
Their excellent temporal resolution makes it possible to probe how early one can detect influences of high-level (e.g., semantic) information.
To this end, participants in previous studies by our lab and others were trained to associate visual objects with different amounts of semantic information.
After training, an orthogonal task was used to compare the ERPs in response to semantically trained objects versus non-semantically trained objects.
This contrast revealed differences not only in late ERP components associated with semantic processing (i.e., the N400 component) but also in the visual P1 (or P100) component [@abdelrahman2008; @maier2019; @samaha2018; @weller2019].
The early peak of this component and its source in the occipital cortex [@abdelrahman2008] point to an immediate effect of semantic knowledge on visual perception.
It is less clear, however, if these effects arise immediately once participants obtain semantic information about an object or if, instead, they require more extensive amounts of training.
Closely related to this question is the debate if tentative top-down effects of semantic cognition on perception act in an online fashion (i.e., directly modulating perceptual processing), or more indirectly, by altering stored visual representations over the course of training, which would then get reactivated once the object is reencountered later on.

To answer these open questions, we measured ERPs in response to unfamiliar objects directly while participants gained a semantic understanding of their function.
To this end, we presented half of the objects together with matching keywords, thus allowing participants to understand what kind of object they were viewing, or together with non-matching keywords, thus keeping the perception of the object semantically naive.
We then presented the same objects again to test for downstream effects of semantic information, as in previous studies.
We examined the influence of semantic information on ERPs associated with lower-level visual perception (P1 component), higher-level visual perception (N170 component), and semantic processing (N400 component).
We hypothesized that semantic information would have instant effects on visual perception, by which we mean (a) that these effects can already be observed on the same trial as the semantic information is being presented for the first time, and (b) that these effects are found not only in later, higher-level cognition-related ERP components (i.e., N400), but also in earlier, perception-related ERP components (i.e., P1 and/or N170).
We furthermore conducted an exploratory time-frequency analysis to test if semantic information also has instant effects on event-related power which, unlike ERP effects, do not need to be tightly phase-locked to stimulus onset.

# Materials and Methods

## Participants

Participants were 48 German native speakers (31 female, 17 male) with a mean age of 23.5 years (range 18--32 years) and no history of psychological disorder or treatment.
No a priori power analysis was carried out and the sample size was chosen in line with previous EEG studies on object processing in our lab.
All participants were right-handed according to the Edinburgh inventory [@oldfield1971] and reported normal or corrected-to-normal vision.
They provided written informed consent before starting the experiment and received a compensation of €8 per hour for participating.

## Stimuli

Stimuli consisted of 240 grayscale photographs of real-world objects.
Of these, 120 stimuli were well-known everyday objects (e.g., a bicycle, a toothbrush).
These served as filler stimuli of no interest.
The other 120 stimuli were rare objects presumed to be unfamiliar to the majority of participants (e.g., a galvanometer, an udu drum; see Supplementary Table 1 for the full list).
All stimuli were presented on a light blue background with a size of 207 × 207 pixels on a 19-inch LCD monitor with a resolution of 1,280 × 1,024 pixels and a refresh rate of 75 Hz.
At a standardized viewing distance of 90 cm, the images subtended approximately 3.9 degrees of participants' horizontal and vertical visual angle.

For each unfamiliar object, a pair of German keywords (a noun and a verb) was selected, describing the typical function or use of the object in a way that could be related to its visual features and their configuration (e.g., electric current, measuring; clay pot, drumming; see Supplementary Table 1 for the full list).
As our central experimental manipulation, half of the objects were presented together with keywords that correctly matched their respective function, whereas the other half of the objects were presented together with non-matching keywords belonging to one of the other objects.
The matching keywords were expected to induce semantically informed perception, that is, participants suddenly understanding what kind of object they were viewing.
The non-matching keywords were expected to prevent such an understanding and keep the perception of the object semantically naive.
All participants saw each unfamiliar object with only one type of keywords (matching or non-matching).
This assignment of keywords to objects was counterbalanced across participants so that each object was presented with matching keywords (leading to semantically informed perception) and non-matching keywords (leading to naive perception) to an equal number of participants.
The experiment was programmed and displayed using Presentation® software (Neurobehavioral Systems, Inc., Berkeley, CA, www.neurobs.com).

## Experimental Design

The experiment consisted of three phases (see Figure \@ref(fig:fig1)A).
In the *pre-insight* phase, after written informed consent had been obtained and the EEG had been prepared, all 240 familiar and unfamiliar objects were presented once in random order and without any keywords.
Each trial consisted of a fixation cross presented in the middle of the screen for 0.5 s, followed by the presentation of the object until participants made a response or until a time out after 3 s.
The inter-trial interval was 0.5 s and participants took a self-timed break after each block of 60 objects.
The task, which was kept the same across all three phases of the experiment, was to classify each object using one of four response alternatives: (a) "I know what this is or have a strong assumption," (b) "I have an assumption what this is," (c) "I have rather no assumption what this is," or (d) "I don't know what this is and have no assumption." Participants were asked to respond as quickly and as accurately as possible by pressing one out of four buttons with the index or middle finger of their left or right hand, respectively.
The mapping of the rating scale to the four buttons (left to right or right to left) was counterbalanced across participants.

```{r, eeg_processing, include=FALSE}
# Define function for processing behavioral log files
read_log <- function(log_file) {

  # Read the file
  read_tsv(
    log_file,
    col_types = cols(),
    locale = locale(encoding = "latin1")
  ) %>%
    # Recode some columns
    transmute(

      # Phase of the experiment
      phase = factor(Wdh,
        levels = c(211, 212, 213),
        labels = c("Pre-insight", "Insight", "Post-insight")
      ),

      # Keyword conditions
      keywords = factor(Bed,
        levels = c("richtig", "falsch"),
        labels = c("Match", "Non-match")
      ),

      # Behavioral responses
      # 201: "I know what this is or have a strong assumption"  -> Recode as 4
      # 202: "I have an assumption what this is"                -> Recode as 3
      # 203: "I have rather no assumption what this is"         -> Recode as 2
      # 204: "I don't know what this is and have no assumption" -> Recode as 1
      response = 5 - (Tastencode - 200),

      # Reaction times
      rt = RT,

      # Item IDs
      item_id = factor(StimID)
    ) %>%
    # Remove filler items
    drop_na(keywords) -> log

  # Assign items to conditions based on manipulation (matching vs. non-
  # matching keywords) and the responses of the participants
  items_per_condition <- with(log, {
    list(

      # Informed condition: Matching keywords and positive response
      Informed = item_id[
        phase == "Insight" & keywords == "Match" & response %in% c(3, 4)
      ],

      # Naive condition: Non-matching keywords and negative response
      Naive = item_id[
        phase == "Insight" & keywords == "Non-match" & response %in% c(1, 2)
      ],

      # Exclude: Matching keywords and negative response
      Exclude_informed = item_id[
        phase == "Insight" & keywords == "Match" & response %in% c(1, 2)
      ],

      # Exclude: Non-matching keywords and positive response
      Exclude_naive = item_id[
        phase == "Insight" & keywords == "Non-match" & response %in% c(3, 4)
      ],

      # Exclude: Positive response *before* any info was presented
      Exclude_known = item_id[phase == "Pre-insight" & response == 4]
    )
  })

  # Assign these conditions to the trials of all three phases
  log$condition <- NA
  for (condition in names(items_per_condition)) {
    item_id <- items_per_condition[[condition]]
    log$condition[log$item_id %in% item_id] <- condition
  }

  # Sort columns
  log %>%
    select(item_id, phase, condition, response, rt)
}

# Get list of behavioral log files
log_files <- c(
  list.files(here(data_dir, "rt/exp1"), "*.txt", full.names = TRUE),
  list.files(here(data_dir, "rt/exp2"), "*.txt", full.names = TRUE)
)

# Get list of BrainVision EEG header files
vhdr_files <- c(
  list.files(here(data_dir, "eeg/exp1"), "*.vhdr", full.names = TRUE),
  list.files(here(data_dir, "eeg/exp2"), "*.vhdr", full.names = TRUE)
)

# Import EEG pipeline from Python
pipeline <- import("pipeline")

# Extract versions of Python packages
pipeline_version <- pipeline$`__version__`
mne_version <- import("mne")$`__version__`

# (Re-)do the actual preprocessing if requested
if (run$eeg_processing) {

  # Process behavioral data
  logs <- map(log_files, read_log)

  # Process EEG data
  res <- pipeline$group_pipeline(
    vhdr_files = vhdr_files,
    log_files = logs,
    output_dir = output_dir,
    report_dir = report_dir,
    downsample_sfreq = 125.0,
    bad_channels = NULL,
    ica_method = "fastica",
    ica_n_components = 0.99,
    highpass_freq = 0.1,
    lowpass_freq = 30.0,
    triggers = c(221, 222),
    epochs_tmin = -0.5,
    epochs_tmax = 1.5,
    baseline = c(-0.2, 0.0),
    reject_peak_to_peak = 150.0,
    components = list(
      name = list("P1", "N170", "N400"),
      tmin = list(0.1, 0.15, 0.4),
      tmax = list(0.15, 0.2, 0.7),
      roi = list(
        c("PO3", "PO4", "POz", "O1", "O2", "Oz"),
        c("P7", "P8", "PO7", "PO8", "PO9", "PO10"),
        c("C1", "C2", "Cz", "CP1", "CP2", "CPz")
      )
    ),
    average_by = c("phase/condition"),
    perform_tfr = TRUE,
    tfr_subtract_evoked = FALSE,
    tfr_freqs = seq(4, 40, by = 1.0),
    tfr_cycles = seq(2, 20, by = 0.5),
    tfr_mode = "percent",
    tfr_baseline = c(-0.45, -0.05),
    perm_contrasts = list(
      c("Pre-insight/Informed", "Pre-insight/Naive"),
      c("Insight/Informed", "Insight/Naive"),
      c("Post-insight/Informed", "Post-insight/Naive"),
      c("Insight/Informed", "Insight/Exclude_informed")
    ),
    tfr_components = list(
      name = list("alpha_beta"),
      tmin = list(0.6),
      tmax = list(1.2),
      fmin = list(8.0),
      fmax = list(20.0),
      roi = list(c("P3", "Pz", "P4", "PO3", "POz", "PO4"))
    ),
    perm_tmin = -0.4,
    perm_tmax = 1.4
  )
}

# Read single trial data
read_csv(here(output_dir, "trials.csv"), col_types = cols(
  participant_id = col_factor(),
  item_id = col_factor(),
  phase = col_factor(),
  condition = col_factor()
)) %>%
  # Mark unrealistically short RTs
  mutate(rt = ifelse(rt < 200, NA, rt)) -> trials

# Compute number of objects per condition for each participant
with(trials, table(participant_id, condition)) %>%
  `/`(3) %>%
  as.data.frame.matrix() %>%
  rownames_to_column("subject_id") -> objects_per_condition

# Read by-participant ERP averages and channel locations
evokeds <- read_csv(here(output_dir, "ave.csv"))

# Read pipeline configuration
config <- jsonlite::read_json(
  here(output_dir, "config.json"),
  simplifyVector = TRUE, simplifyMatrix = FALSE
)

# Count number of rejected epochs per participant
rejected_epochs <- lengths(config$auto_rejected_epochs)

# Read channel locations
channel_locations <- read_csv(here(output_dir, "channel_locations.csv"))

# Read grand-averaged power
tfr_grand_ave <- read_csv(here(output_dir, "tfr_grand_ave.csv"))

# Read results of cluster-based permutation tests for ERPs
clusters <- read_csv(here(output_dir, "clusters.csv"))

# Read results of cluster-based permutation tests for TFR
tfr_clusters <- read_csv(here(output_dir, "tfr_clusters.csv"))

# Define cluster-level p value threshold
p_cluster <- 0.05 / 3 # Bonferroni-corrected for three phases
```

```{r, mixed_models, include=FALSE}
# Construct model formula
form <- ~ phase * condition +
  (phase * condition | participant_id) +
  (phase * condition | item_id)

# Define helper function for fitting linear mixed models and computing contrasts
fit_mixed_models <- function(dep, formula, data) {

  # Modify the formula such that fixed effects are always retained
  formula %<>%
    tabulate.formula() %>%
    mutate(block = replace(block, is.na(grouping), "fixed"))

  # Start a CPU cluster so that multiple models can be fitted in parallel
  n_cores <- detectCores()
  cl <- makeCluster(n_cores)

  # Find the best fitting model (see Matuschek et al., 2017, *JML*)
  build <- buildmer(
    buildmerControl = buildmerControl(
      formula, data,
      args = list(control = lme4::lmerControl(
        optimizer = "bobyqa", optCtrl = list(maxfun = 1e6)
      )),
      direction = c("backward", "backward"),
      cl = cl,
      elim = LRTalpha(.20),
      calc.anova = TRUE,
      ddf = "Satterthwaite",
      dep = dep
    )
  )
  stopCluster(cl)

  # Compute marginal means and contrasts
  emm_options(lmer.df = "Satterthwaite", lmerTest.limit = Inf)
  means <- emmeans(build@model, specs = c("phase", "condition"))
  contrasts <- contrast(means, "pairwise", simple = "condition") %>%
    summary(by = NULL, adjust = "bonferroni")

  # Create list of returns
  list(
    model = build@model,
    summary = build@summary,
    anova = build@anova,
    means = means,
    contrasts = contrasts
  )
}

# (Re-)do the actual model fitting if requested
deps <- config$components$name
if (run$mixed_models) {

  # Filter relevant conditions for the main analysis
  trials %>%
    mutate(condition = factor(condition, levels = c("Informed", "Naive"))) %>%
    drop_na() -> trials_main

  # Contrast coding for phase
  cbind(
    c(`Pre-insight` = -1, `Insight` = 1, `Post-insight` = 0),
    c(`Pre-insight` = 0, `Insight` = -1, `Post-insight` = 1)
  ) %>%
    t() %>%
    ginv() -> contrasts(trials_main$phase)

  # Contrast coding for condition
  cbind(c(`Informed` = 1, `Naive` = -1)) %>%
    t() %>%
    ginv() -> contrasts(trials_main$condition)

  # Actually fit the model for each ERP component
  map(deps, fit_mixed_models, formula = form, data = trials_main) %>%
    set_names(deps) -> models

  # Save models
  saveRDS(models, here(output_dir, "models.RDS"))
} else {

  # Read fitted models
  models <- readRDS(here(output_dir, "models.RDS"))
}

# Extract relevant statistics to print in the main text
map(models, function(model) as_tibble(model$anova, rownames = "effect")) %>%
  bind_rows(.id = "component") %>%
  rename(f_value = `F value`, p_value = `Pr(>F)`) -> anova_table
map(models, function(model) as_tibble(model$contrasts)) %>%
  bind_rows(.id = "component") %>%
  rename(t_value = `t.ratio`, p_value = `p.value`) -> contrast_table
```

```{r, fig1, cache=!run$figures, fig.height=12, fig.cap="(ref:figure-1-caption)"}
# Create Figure 1
plot_fig1(files_dir, evokeds, config, channel_locations, models)
```

(ref:figure-1-caption) Experimental design and ERP results.
***A***, In the pre-insight phase, participants were presented with 120 unfamiliar objects and indicated if they knew what kind of object they were viewing.
In the insight phase, half of these objects were presented with matching keywords (in red color for illustration), leading to semantically informed perception, and the other half with non-matching keywords (in blue color for illustration), leading to naive perception.
In the post-insight phase, the same objects were presented again without keywords.
***B***, ***C***, ***D***, ERP waveforms and scalp topographies for the P1 component (100--150 ms, ***B***), for the N170 component (150--200 ms, ***C***), and for the N400 component (400--700 ms, ***D***) for objects with semantically informed perception versus naive perception within the three different phases.
Semantically informed perception was associated with more negative amplitudes in the N170 component during the insight phase, less negative amplitudes in the N400 component during the insight and post-insight phases, and more positive amplitudes in the P1 component during the post-insight phase.
The time windows and channels for quantifying the ERP components are highlighted by the gray areas in the ERP waveforms and by the black markers on the scalp topographies, respectively.
Colored ribbons around the ERP waveforms show ± 1 standard error of the mean across participants.
Ampl. = amplitude.\newline
\* $p_\text{corr} < .05$. \*\* $p_\text{corr} < .01$. \*\*\* $p_\text{corr} < .001$.

In the *insight* phase, the 120 unfamiliar objects were presented for a second time, now preceded either by matching keywords (leading to semantically informed perception) or by non-matching keywords (leading to naive perception).
Each trial consisted of a fixation cross presented for 0.5 s, followed by the presentation of the keywords for 2.5 s.
Then, an asterisk was presented in the middle of the screen for another 0.5 s, followed by the presentation of the object until a response was made or until a time out after 3 s.
The objects were presented in blocks of 30 trials so that within each block there were 15 objects from each of the two experimental conditions and so that objects were heterogeneous in terms of their shape, visual complexity, and functional category (e.g., medical devices, musical instruments).

In the *post-insight* phase, the unfamiliar objects were presented for a third time, with the same trial structure as in the pre-insight phase, that is, without any keywords.
The insight and post-insight phases were presented in an interleaved fashion so that after the presentation of one block of 30 objects in the insight phase (with keywords), participants took a self-timed break and continued with the same block of 30 objects in the post-insight phase (without keywords) before moving on to the next block consisting of 30 different objects.
They continued like this until all four blocks were completed in both phases.
In total, the experiment consisted of 480 trials (120 familiar objects in the pre-insight phase and 120 unfamiliar objects in the pre-insight, insight, and post-insight phases).
Participants took approximately 35 minutes to complete the entire experiment.

## Behavioral Data Analysis

We first excluded from all further analyses those objects for which the participant had responded with "I know what this is" in the pre-insight phase (i.e., before any keywords were presented).
This led to the exclusion of an average of `r mean(objects_per_condition$Exclude_known)` objects per participant (= `r percent(mean(objects_per_condition$Exclude_known) / 120, accuracy = 0.1)` of all unfamiliar objects).
Next, to delineate semantically informed and naive perception, the assignment of the remaining objects to one of these two conditions was co-determined by our experimental manipulation (matching versus non-matching keywords) and the behavioral response of the participant in the insight phase (see Figure \@ref(fig:fig1)A).
Objects were assigned to the semantically informed condition if and only if they were presented with matching keywords and the participant responded with knowing what the object was or having an assumption.
This was the case for an average of `r mean(objects_per_condition$Informed)` objects per participant (= `r percent(mean(objects_per_condition$Informed) / 60, accuracy = 0.1)` of objects presented with matching keywords).
Complementarily, objects were assigned to the naive condition if and only if they were presented with non-matching keywords and the participant responded with not knowing what the object was or having rather no assumption.
This was the case for an average of `r mean(objects_per_condition$Naive)` objects per participant (= `r percent(mean(objects_per_condition$Naive) / 60, accuracy = 0.1)` of objects presented with non-matching keywords).
This same assignment of objects conditions based on the insight phase was carried over to the pre-insight and post-insight phases.
This allowed us to test, on the one hand, if the objects from both conditions differed in important aspects even before any keywords were presented (pre-insight phase) and, on the other hand, if the semantic understanding acquired in the insight phase had any down-stream effects on the subsequent perception of the objects (post-insight phase).

## EEG Recording and Preprocessing

The continuous EEG was recorded from 62 Ag/AgCl scalp electrodes placed according to the extended 10--20 system [@americanelectroencephalographicsociety1991] and referenced online to an external electrode placed on the left mastoid (M1).
Two additional external electrodes were placed on the right mastoid (M2) and below the left eye (IO1), respectively.
Electrode impedances were kept below 5 kΩ.
An online band-pass filter with a high-pass time-constant of 10 s (0.016 Hz) and a low-pass cutoff frequency of 1000 Hz was applied before digitizing the signal at a sampling rate of 500 Hz.

The data were preprocessed offline using custom scripts, available in the hu-neuro-pipeline package (Version `r pipeline_version`; https://github.com/alexenge/hu-neuro-pipeline) and based on the MNE-Python software [Version `r mne_version`\; @gramfort2013] in Python [Version `r py_version()`\; @vanrossum2009].
First, the data were downsampled to `r config$downsample_sfreq` Hz and re-referenced to the common average of all scalp channels.
Next, artifacts resulting from blinks and eye movements were removed via independent component analysis (ICA) on a low-pass filtered copy of the data (cutoff = 1 Hz).
A variable number of components per participant were extracted from an initial principal component analysis (PCA) so that they explained at least `r percent(config$ica_n_components)` of the variance in the data (mean = `r mean(unlist(config$auto_ica_n_components))` components, range `r min(unlist(config$auto_ica_n_components))`--`r max(unlist(config$auto_ica_n_components))`).
Then the ICA was fitted based on these components using the FastICA algorithm [@hyvarinen1999].
Any independent components showing significant correlations with either of two virtual EOG channels (VEOG: IO1 - Fp1, HEOG: F9 - F10) were removed automatically using the MNE-Python's `find_bads_eog` method.
This was the case for an average of `r mean(lengths(config$auto_ica_bad_components))` components per participant (range = `r min(lengths(config$auto_ica_bad_components))`--`r max(lengths(config$auto_ica_bad_components))` components).

## Event-Related Potential Analysis

For the analysis of ERP amplitudes, a zero-phase, non-causal FIR filter with a lower pass-band edge at `r config$highpass_freq` Hz (transition bandwidth: 0.1 Hz) and an upper pass-band edge at `r config$lowpass_freq` Hz (transition bandwidth: 7.5 Hz) was applied.
Next, the continuous EEG was segmented into epochs ranging from `r config$epochs_tmin` s to `r config$epochs_tmax` s relative to the onset of the presentation of each unfamiliar object.
These epochs were baseline-corrected by subtracting the average voltage during the interval of `r config$baseline[1]` s to `r config$baseline[2]` s relative to stimulus onset.
Epochs containing artifacts despite ICA, defined as peak-to-peak amplitudes exceeding `r config$reject_peak_to_peak` µV, were removed from further analysis.
This led to the exclusion of an average of `r mean(rejected_epochs)` trials per participant (= `r percent(mean(rejected_epochs) / 360, accuracy = 0.1)`; range `r min(rejected_epochs)`--`r max(rejected_epochs)` trials).
Single-trial event-related potentials were computed as the mean amplitude across time windows and regions of interests (ROIs) defined a priori, namely 100--150 ms at channels PO3, POz, PO4, O1, Oz, and O2 and for the P1 component, 150--200 ms at channels P7, P8, PO7, PO8, PO9, and PO10 for the N170 component, and 400--700 ms at channels C1, Cz, C2, CP1, CPz, and CP2 for the N400 component.

The resulting mean ERP amplitudes were analyzed on the single-trial level using linear mixed-effects regression models because these models allow to control for repeated measures of participants and stimuli, while also being robust against an unbalanced number of trials per condition [@baayen2008; @burki2018; @fromer2018].
We computed three models predicting P1, N170, and N400 amplitudes, respectively.
All models included three fixed effects: (a) the phase of the experiment, coded as a repeated contrast [i.e., subtracting the first phase from the second phase and the second phase from the third phase, the intercept being the grand mean across all three phases\; @schad2020], (b) the condition of the object, coded as a scaled sum contrast (i.e., subtracting the naive condition from the semantically informed condition, the intercept being the grand mean across both conditions), and (c) the two-way interaction of phase and condition.
We determined the most parsimonious random effect structure supported by the data using the automatic procedure proposed by @matuschek2017.
This involved starting with a maximal model that contained all random parameters (intercepts, slopes, and correlations) and then iteratively removing terms as long as this did not result in a significant drop in model fit (likelihood ratio test, $p$ value cutoff = .20; see Supplementary Results 1 for the final model syntax and model outputs).
All models were fitted in R [Version `r as.character(packageVersion("base"))`\; @R-base] using the lme4 package [Version `r as.character(packageVersion("lme4"))`\; @R-lme4] with the optimizer function *bobyqa* and a maximum of 10^6^ iterations for maximum likelihood estimation.
The model selection algorithm via likelihood ratio tests was performed using the buildmer package [Version `r as.character(packageVersion("buildmer"))`\; @R-buildmer].

To investigate if semantically informed perception had an influence on the ERPs within each phase of the experiment, we calculated follow-up comparisons contrasting the semantically informed condition against the naive condition within the pre-insight, insight, and post-insight phases.
This was done using the emmeans package [Version `r as.character(packageVersion("emmeans"))`\; @R-emmeans].
All *p* values were computed by approximating the relevant denominator degrees of freedom using Satterthwaite's method as implemented in the lmerTest package [Version `r as.character(packageVersion("lmerTest"))`\; @R-lmerTest].

## Time-Frequency Analysis

For our exploratory analysis of event-related power, we first created new epochs from the ICA-corrected but unfiltered raw data.
Epochs that were marked as bad for the ERP analysis (i.e., with peak-to-peak amplitudes exceeding `r config$reject_peak_to_peak` µV) were also removed from the time-frequency analysis.
The remaining epochs were convolved with a family of Morlet wavelets, increasing linearly in their frequency from `r min(config$tfr_freqs)` Hz to `r max(config$tfr_freqs)` Hz in steps of `r config$tfr_freqs[2] - config$tfr_freqs[1]` Hz and in their width from `r min(config$tfr_cycles)` cycles to `r max(config$tfr_cycles)` cycles in steps of `r config$tfr_cycles[2] - config$tfr_cycles[1]` cycles.
To adjust for the typical $1/f$ shape of the EEG, the power values were transformed into percent signal change by first subtracting and then dividing by the average power at each frequency over the entire epoch [@grandchamp2011].
We then performed baseline correction by subtracting the average power during the pre-stimulus interval from `r config$tfr_baseline[1]` s to `r config$tfr_baseline[2]` s relative to object onset.

For statistical analysis of event-related power, we conducted cluster-based permutation tests [@maris2007], separately for each of the three phases of the experiment (pre-insight, insight, and post-insight).
First, we averaged trials belonging to the same condition and then subtracted these average responses from one another to compute the difference between the semantically informed condition and the naive condition for each participant.
We then conducted one-sample $t$ tests in a mass-univariate fashion and grouped significant results (cluster-forming threshold $p < .05$) into clusters if they occurred at neighboring time points, frequencies, or channels.
Neighboring channels were defined using the Delaunay triangulation based on 2D electrode locations as implemented in MNE-Python.
To obtain family-wise error-corrected $p$ values at the cluster level, we compared the cluster mass (i.e., the sum of the $t$ values) of each observed cluster to an empirical null distribution of cluster masses obtained from 5,000 permutations with random sign flips.
Clusters were considered to be statistically significant if their mass exceeded the `r (1 - p_cluster) * 100`^th^ percentile of this distribution (i.e., cluster-level threshold $p < .05$, Bonferroni-corrected for three separate tests).

## Data and Code Availability

The EEG data are available upon reasonable request from the corresponding author because we had not asked participants for their consent to make the data publicly available.
The materials and code for data analysis are available at [https://osf.io/uksbc/](https://osf.io/uksbc/).

In addition to the software mentioned above, our code relies on the tidyverse set of R packages [Version `r as.character(packageVersion("tidyverse"))`\; @R-tidyverse] for data wrangling, the ggplot2 [Version `r as.character(packageVersion("ggplot2"))`\; @R-ggplot2], cowplot [Version `r as.character(packageVersion("cowplot"))`\; @R-cowplot], and eegUtils [Version `r as.character(packageVersion("eegUtils"))`\; @R-eegUtils] packages for visualization, and the papaja package [Version `r as.character(packageVersion("papaja"))`\; @R-papaja] for statistical reporting.
We followed the workflow developed by @peikert2021 to ensure the long-term reproducibility of our analysis pipeline.

# Results

## Event-Related Potentials

Averaged across conditions, P1, N170, and N400 amplitudes differed as a function of the phase of the experiment (pre-insight, insight, or post-insight), all $F$s > `r min(filter(anova_table, effect == "phase")$f_value)`, all $p$s `r print_p(max(filter(anova_table, effect == "phase")$p_value))`.
In addition, N400 amplitudes differed as a function of the condition (semantically informed or naive), averaged across the three phases of the experiment, $F$(`r filter(anova_table, component == "N400" & effect == "condition")$NumDF`, `r filter(anova_table, component == "N400" & effect == "condition")$DenDF`) = `r filter(anova_table, component == "N400" & effect == "condition")$f_value`, $p$ `r print_p(filter(anova_table, component == "N400" & effect == "condition")$p_value)`.
Crucially, the phase × condition interaction was significant for all three ERP components, all $F$s > `r min(filter(anova_table, effect == "phase:condition")$f_value)`, all $p$s < `r print_p(max(filter(anova_table, effect == "phase:condition")$p_value))`.
To answer our main research question, we decomposed these interactions into simple effects of the semantically informed condition versus the naive condition within each of the three phases of the experiment.

In the pre-insight phase, when objects were unfamiliar to participants and presented without keywords, no differences emerged between the semantically informed condition and the naive condition in any of the the ERP components, all $|t|$s < `r max(abs(filter(contrast_table, phase == "Pre-insight")$t_value))`, all $p$s `r print_p(min(filter(contrast_table, phase == "Pre-insight")$p_value))` (Figure \@ref(fig:fig1)B--D).
This was expected given that the critical keywords (leading either to semantically informed perception or to naive perception) had not yet been presented, and given that the assignment of objects to conditions was counterbalanced across participants as to control for low-level visual differences.

In the insight phase, half of the unfamiliar objects were presented with matching keywords, leading to semantically informed perception, and the other half were presented with non-matching keywords, keeping the perception of the objects semantically naive.
Semantically informed perception in this phase was associated with enlarged (i.e., more negative) amplitudes in the N170 component, $b$ = `r filter(contrast_table, component == "N170" & phase == "Insight")$estimate` µV, $t$(`r filter(contrast_table, component == "N170" & phase == "Insight")$df`) = `r filter(contrast_table, component == "N170" & phase == "Insight")$t_value`, $p_\text{corr}$ = `r print_p(filter(contrast_table, component == "N170" & phase == "Insight")$p_value)`, and reduced (i.e., less negative) amplitudes in the N400 component, $b$ = `r filter(contrast_table, component == "N400" & phase == "Insight")$estimate` µV, $t$(`r filter(contrast_table, component == "N400" & phase == "Insight")$df`) = `r filter(contrast_table, component == "N400" & phase == "Insight")$t_value`, $p_\text{corr}$ `r print_p(filter(contrast_table, component == "N400" & phase == "Insight")$p_value)`.
There was no reliable difference between conditions in the P1 component, $b$ = `r filter(contrast_table, component == "P1" & phase == "Insight")$estimate` µV, $t$(`r filter(contrast_table, component == "P1" & phase == "Insight")$df`) = `r filter(contrast_table, component == "P1" & phase == "Insight")$t_value`, $p_\text{corr}$ = `r print_p(filter(contrast_table, component == "P1" & phase == "Insight")$p_value)`.

In the post-insight phase, the unfamiliar objects were presented for a third time and without any keywords, mirroring the pre-insight phase.
As in the insight phase, semantically informed perception was associated with reduced (i.e., less negative) amplitudes in the N400 component, $b$ = `r filter(contrast_table, component == "N400" & phase == "Post-insight")$estimate` µV, $t$(`r filter(contrast_table, component == "N400" & phase == "Post-insight")$df`) = `r filter(contrast_table, component == "N400" & phase == "Post-insight")$t_value`, $p_\text{corr}$ `r print_p(filter(contrast_table, component == "N400" & phase == "Post-insight")$p_value)`, while the effect in the N170 component did not recur, $b$ = `r filter(contrast_table, component == "N170" & phase == "Post-insight")$estimate` µV, $t$(`r filter(contrast_table, component == "N170" & phase == "Post-insight")$df`) = `r filter(contrast_table, component == "N170" & phase == "Post-insight")$t_value`, $p_\text{corr}$ = `r print_p(filter(contrast_table, component == "N170" & phase == "Post-insight")$p_value)`.
Instead, the P1 component was significantly enlarged (i.e., more positive) in response to objects for which semantically informed perception had taken place, $b$ = `r filter(contrast_table, component == "P1" & phase == "Post-insight")$estimate` µV, $t$(`r filter(contrast_table, component == "P1" & phase == "Post-insight")$df`) = `r filter(contrast_table, component == "P1" & phase == "Post-insight")$t_value`, $p_\text{corr}$ = `r print_p(filter(contrast_table, component == "P1" & phase == "Post-insight")$p_value)`.

## Event-Related Power

In an exploratory time-frequency analysis, we checked for differences in event-related power between semantically informed perception and naive perception within each of the three phases of the experiment.
Cluster-based permutation tests [@maris2007] revealed no significant clusters in the pre-insight phase (see Figure 2), all $p$s > `r print_p(min(filter(tfr_clusters, contrast == "Pre-insight/Informed - Pre-insight/Naive")$p_val))`, but one significant cluster in the insight phase (see Figure 3), $p_\text{cluster}$ = `r print_p(unique(filter(tfr_clusters, contrast == "Insight/Informed - Insight/Naive" & p_val < p_cluster)$p_val))`, and one significant cluster in the post-insight phase (see Figure 4), $p_\text{cluster}$ = `r print_p(unique(filter(tfr_clusters, contrast == "Post-insight/Informed - Post-insight/Naive" & p_val < p_cluster)$p_val))`.
These two significant clusters in the insight and post-insight phases were similar in their direction, latency, frequency range, and topographic distribution.
Both clusters had a negative sign, started around 600 ms after object onset [but see @sassenhagen2019] and continued all the way until the end of the analyzed period at 1,400 ms.
They spanned a broad range of frequencies in the alpha and lower beta range as well as a broad set of channels, but appeared to be most focal at around 15 Hz and parietal channels.
Thus, semantically informed perception seems to alter not only early, evoked activity (see Event-Related Potentials above) but also later, induced activity, in the form of a reduction of post-stimulus power at parietal channels in the range of alpha and lower beta frequencies.

```{r, fig2, cache=!run$figures, fig.height=11, fig.cap="(ref:figure-2-caption)"}
# Create topographic plots for event-related power in the pre-insight phase
plot_tfr_topos(
  filter(tfr_grand_ave, phase == "Pre-insight"),
  filter(tfr_clusters, contrast == "Pre-insight/Informed - Pre-insight/Naive"),
  p_cluster = p_cluster
)
```

(ref:figure-2-caption) Time-frequency results for the pre-insight phase.
Each topographic plot shows the difference in event-related power (in units of percent signal change) between the semantically informed condition and the naive condition, grand-averaged across participants.
A cluster-based permutation test indicated no clusters for which this difference was statistically significant (all $p$s > `r print_p(min(filter(tfr_clusters, contrast == "Pre-insight/Informed - Pre-insight/Naive")$p_val))`).

```{r, fig3, cache=!run$figures, fig.height=11, fig.cap="(ref:figure-3-caption)"}
# Create topographic plots for event-related power in the insight phase
plot_tfr_topos(
  filter(tfr_grand_ave, phase == "Insight"),
  filter(tfr_clusters, contrast == "Insight/Informed - Insight/Naive"),
  p_cluster = p_cluster
)
```

(ref:figure-3-caption) Time-frequency results for the insight phase.
Each topographic plot shows the difference in event-related power (in units of percent signal change) between the semantically informed condition and the naive condition, grand-averaged across participants.
Black dots highlight EEG channels that were part of a cluster for which this difference was statistically significant ($p_\text{cluster}$ = `r print_p(unique(filter(tfr_clusters, contrast == "Insight/Informed - Insight/Naive" & p_val < p_cluster)$p_val))`).

```{r, fig4, cache=!run$figures, fig.height=11, fig.cap="(ref:figure-4-caption)"}
# Create topographic plots for event-related power in the post-insight phase
plot_tfr_topos(
  filter(tfr_grand_ave, phase == "Post-insight"),
  filter(
    tfr_clusters, contrast == "Post-insight/Informed - Post-insight/Naive"
  ),
  p_cluster = p_cluster
)
```

(ref:figure-4-caption) Time-frequency results for the post-insight phase.
Each topographic plot shows the difference in event-related power (in units of percent signal change) between the semantically informed condition and the naive condition, grand-averaged across participants.
Black dots highlight EEG channels that were part of a cluster for which this difference was statistically significant ($p_\text{cluster}$ = `r print_p(unique(filter(tfr_clusters, contrast == "Post-insight/Informed - Post-insight/Naive" & p_val < p_cluster)$p_val))`).

## Control Analysis

```{r, control_analysis, include=FALSE}
# (Re-)do the actual model fitting if requested
if (run$control_analysis) {

  # Filter relevant conditions for the control analysis
  conditions_control <- c("Informed", "Exclude_informed")
  trials %>%
    mutate(condition = factor(condition, levels = conditions_control)) %>%
    drop_na() -> trials_control

  # Contrast coding for phase
  cbind(
    c(`Pre-insight` = -1, `Insight` = 1, `Post-insight` = 0),
    c(`Pre-insight` = 0, `Insight` = -1, `Post-insight` = 1)
  ) %>%
    t() %>%
    ginv() -> contrasts(trials_control$phase)

  # Contrast coding for condition
  cbind(c(`Informed` = 1, `Exclude_informed` = -1)) %>%
    t() %>%
    ginv() -> contrasts(trials_control$condition)

  # Fit the model for each ERP component
  map(deps, fit_mixed_models, formula = form, data = trials_control) %>%
    set_names(deps) -> models_control

  # Fit an additional model for event-related power, controlling for RTs
  form_rt <- ~ phase * condition + log(rt) +
    (phase * condition | participant_id) +
    (phase * condition | item_id)
  models_control$alpha_beta <-
    fit_mixed_models(form = form_rt, data = trials_main, dep = "alpha_beta")

  # Save models
  saveRDS(models_control, here(output_dir, "models_control.RDS"))
} else {

  # Read fitted models
  models_control <- readRDS(here(output_dir, "models_control.RDS"))
}

# Extract relevant statistics to print in the main text
map(models_control, function(mod) as_tibble(mod$contrasts)) %>%
  bind_rows(.id = "component") %>%
  rename(t_value = `t.ratio`, p_value = `p.value`) -> contrast_table_control
```

There are at least two important alternative explanations for our finding that semantic information instantly affects early ERP responses.
First, differences in the P1 and/or N170 components might be driven by low-level visual differences between object images rather than by our experimental manipulation (matching versus non-matching keywords in the insight phase).
We ruled out this alternative explanation by counter-balancing the assignment of objects to conditions across participants, meaning that the same visual object entered the analysis in both conditions on an approximately equal number of trials, barring differences in participants' classification of the object and differences in the number of rejected EEG trials (see Materials and Methods).
Furthermore, we did not detect reliable differences between the two conditions in the pre-insight phase, that is, before any keywords had been presented.
It therefore seems unlikely that low-level differences could account for the tentative effects of semantic information reported above.

Second, there is an alternative explanation specifically for the effects observed in the insight phase, when the objects were presented together with matching or non-matching keywords.
Namely, the early difference in the N170 component might in fact not be due to semantic information changing the processing of objects in the semantically informed condition (as we had hypothesized), but rather a mismatch response to the objects in the naive condition.
In this condition, objects were presented with non-matching keywords that led participants to indicate that they did not understand what kind of object they were viewing.
These non-matching keywords might have primed participants to expect a certain set of visual features that did actually not come true once the object was displayed, potentially provoking a fast and strong brain response within the first 200 ms.
However, in a control analysis for this alternative explanation, we could show that the N170 effect in the insight phase does not depend on the naive condition.
For this control analysis, we again used the semantically informed condition as defined above, but contrasted it against a different control condition.
Namely, we used those objects that were presented with matching keywords (as in the semantically informed condition) but for which participants failed to capitalize on the keywords and instead indicated that they still did not know what kind of object they were viewing (a kind of "failed insight").
This was the case for an average of `r mean(objects_per_condition$Exclude_informed)` objects per participant (= `r percent(mean(objects_per_condition$Exclude_informed) / 60, accuracy = 0.1)` of objects presented with matching keywords).
Contrasting the semantically informed condition and the failed insight condition, we again observed an enlarged (i.e., more negative) N170 component during semantically informed perception, $b$ = `r filter(contrast_table_control, component == "N170" & phase == "Insight")$estimate` µV, $t$(`r filter(contrast_table_control, component == "N170" & phase == "Insight")$df`) = `r filter(contrast_table_control, component == "N170" & phase == "Insight")$t_value`, $p_\text{corr}$ `r print_p(filter(contrast_table_control, component == "N170" & phase == "Insight")$p_value)`.
Likewise, the reduction of the N400 was replicated in this control analysis, $b$ = `r filter(contrast_table_control, component == "N400" & phase == "Insight")$estimate` µV, $t$(`r filter(contrast_table_control, component == "N400" & phase == "Insight")$df`) = `r filter(contrast_table_control, component == "N400" & phase == "Insight")$t_value`, $p_\text{corr}$ `r print_p(filter(contrast_table_control, component == "N400" & phase == "Insight")$p_value)`, as was the late reduction in event-related power ($p_\text{cluster}$ = `r print_p(unique(filter(tfr_clusters, contrast == "Insight/Informed - Insight/Exclude_informed" & p_val < p_cluster)$p_val))`; see Supplementary Figure 1).
This adds credibility to the notion that these effects are specific to those objects for which participants experienced a moment of insight induced by semantic information.

Finally, we had wondered if the reduction in alpha/beta power might merely reflect differences in the timing of motor preparation between conditions, given the well-established link between motor processes and beta power over sensorimotor cortex [@kivalik2013].
To rule this out, we extracted single trial percent signal change based on the distribution of clusters from the permutation tests (time window 600–1,200 ms, frequency range 8–20 Hz, electrodes P3, Pz, P4, PO3, POz, and PO4; see Figures 3 and 4).
We then subjected these to a linear-mixed effects model that controlled for log-transformed reaction time as a covariate of no interest.
This confirmed the significant reduction in alpha/beta power for the semantically informed condition as compared to the naive condition in the insight phase, $b$ = `r filter(contrast_table_control, component == "alpha_beta" & phase == "Insight")$estimate`%, $t$(`r filter(contrast_table_control, component == "alpha_beta" & phase == "Insight")$df`) = `r filter(contrast_table_control, component == "alpha_beta" & phase == "Insight")$t_value`, $p_\text{corr}$ `r print_p(filter(contrast_table_control, component == "alpha_beta" & phase == "Insight")$p_value)`, as well as in the post-insight phase, $b$ = `r filter(contrast_table_control, component == "alpha_beta" & phase == "Post-insight")$estimate`%, $t$(`r filter(contrast_table_control, component == "alpha_beta" & phase == "Post-insight")$df`) = `r filter(contrast_table_control, component == "alpha_beta" & phase == "Post-insight")$t_value`, $p_\text{corr}$ `r print_p(filter(contrast_table_control, component == "alpha_beta" & phase == "Post-insight")$p_value)`.

# Discussion

We investigated if gaining a semantic understanding of visual objects has an influence on how we perceive them.
To this end, we measured ERPs while participants viewed unfamiliar objects before, while, and after receiving semantic information about them.
For half of the objects, this information was matching the function of the object, thus leading to semantically informed perception.
For the other half of the objects, the information was non-matching, thus keeping the perception of the object semantically naive.
We found that semantically informed perception was immediately accompanied by enlarged (i.e., more negative) ERP amplitudes in the N170 component and reduced (i.e., less negative) ERP amplitudes in the N400 component.
When the same objects were presented once more (without the semantic information), the N400 component remained significantly reduced and we also observed a modulation of the P1 component, which was enlarged (i.e., more positive) in response to objects that had previously triggered semantically informed perception.
Finally, an exploratory time-frequency analysis revealed that semantically informed perception was accompanied by a late reduction in event-related power in the alpha and lower beta ranges.
Like the ERP effects, this difference immediately occurred on the first trial on which the objects were presented together with the critical semantic information, and it recurred when the objects were presented once more without any information.

The reduction of the N400 component (400--700 ms after object onset) during semantically informed perception was the numerically largest effect.
It indicates that acquiring an understanding of the objects in the insight phase lessened participants' demand for effortful semantic processing in comparison to the naive condition [@kutas2011].
This finding replicates previous work showing that N400 amplitudes are larger in response to pictures when they are either difficult to understand in and of themselves [e.g., @supp2005; @abdelrahman2008] or difficult to integrate into the preceding context [e.g., @barrett1990; @ganis1996; @hirschfeld2011].
The latency of this effect and the computational role of the N400 component [@bornkessel-schlesewsky2019; @lau2008; @rabovsky2018] suggest that it has a post-perceptual locus in the semantic system.

In contrast to the N400, the N170 component (150--200 ms after object onset) was modulated only on those initial trials on which the objects were presented together with the relevant semantic information.
It therefore constitutes an online marker of semantic insight, that is, of participants suddenly understanding the visual objects in the light of the additional information provided by the verbal keywords.
The N170 is typically associated with the holistic perception of faces [@eimer2011; @sagiv2001] and other stimuli of visual expertise [@rossion2002; @tanaka2001].
It being enlarged during semantically informed perception may reflect that the additional semantic information made participants experience the configuration of the visual features of the objects in a new and meaningful way.
This interpretation is supported by previous findings with similar experimental paradigms in the domain of face perception [@bentin2002].
In this study, participants showed a face-like (i.e., enlarged) N170 response to a scrambled version of a schematic face after but not before they were primed with the intact version of the same face.
This effect was absent when a visual control stimulus (a non-face object) was shown instead of the intact face.
In the domain of non-face stimuli, enlarged N170 amplitudes have also been observed when participants were asked to discriminate between composite line drawings of meaningful objects as compared to composite line drawings of non-objects [@beaucousin2011].
Of note, this effect was present only when participants were asked to decide based on the global shape of the object and it was reversed in polarity when they were asked to decide based on the constituent parts of the object.
Together with the present study, these findings suggest an online impact of meaningfulness on the higher-level perception of visual objects, integrating across their visual features.

The P1 component (100--150 ms after object onset), unlike the N400 and N170 components, was modulated by semantic information only one trial after this information had been obtained.
This is consistent with previous studies showing modulations of the P1 component when participants had learned meaningful information about previously unfamiliar objects [@abdelrahman2008; @maier2018; @maier2019; @weller2019] or about familiar objects that were rendered difficult to recognize [@samaha2018].
What the present study adds to these findings is that the P1 effect does not take an extensive learning history to develop.
Instead, it can be observed as soon as one trial after semantic insight has happened. Because the P1 is typically associated with lower-level sensory processing [e.g., @johannes1995; @luck2014; @pratt2011], we take its susceptibility to semantic information as an indicator that knowledge about the function of an object can change how we perceive it visually.

Both the N170 and the P1 components therefore seem to be sensitive to the semantic meaningfulness of visual objects.
However, the finding that these two components were modulated in different phases of our experimental design suggests that they reflect different aspects of top-down processing with different time courses and neuroanatomical implementations.
It has been pointed out that the time course of the N170 component is consistent with a top-down influence of (non-visual) areas in the prefrontal and parietal cortices on visual areas, whereas modulations of the P1 component seems to reflect recurrent processing *within* the visual system [@wyatte2014].
Here we could show that the former pathway seems to be able to convey semantic information instantaneously (i.e., within the same trial), whereas the latter pathway seems to take at least one additional encounter with the visual object to emerge.

While the limited spatial resolution of the EEG precludes a precise localization of these effects within the ventral stream for object recognition, there is converging evidence coming from fMRI showing that semantic information can feed back into the earliest of visual areas.
@hsieh2010 showed participants indiscernible two-tone ("Mooney") versions of images before and after showing them the original versions. They found that the brain responses to the original image were correlated more strongly with the second presentation of the Mooney image (after insight had taken place) than with the first presentation of the Mooney image (before insight had taken place).
This increase in representational similarity was observed not only in higher-level object-sensitive areas in the lateral occipital cortex (LOC), but also in early retinotopic cortex (areas V1, V2, and V3).
These cortical regions are consistent with the neural generators of the N170 and P1 components in the ERP, which we have found to be sensitive to the semantically informed perception of previously unfamiliar objects.

On a theoretical level, the top-down modulation of these visual ERPs by semantic information challenges a modular view of visual perception [@firestone2016; @fodor1983; @pylyshyn1999; but see @clarke2020].
However, proponents of the modular view have pointed out important shortcomings of previous studies that had claimed to demonstrate top-down effects of cognition on perception [@firestone2016; @machery2015].
We took care to address as many of these shortcomings as possible in the present study:
First, we showed that no difference between conditions had been present before any semantic information was presented.
Second, we used an objective and time-resolved measure (ERPs) to disentangle effects with an early, more perceptual locus from those with a late, more post-perceptual locus.
Third, we reduced response and demand biases by keeping the manipulation (i.e., matching or non-matching keywords) obscure to the participants and by including well-known objects as filler stimuli.
Fourth, we precluded low-level visual differences between conditions by counterbalancing the assignment of objects to conditions across participants.
Fifth, we reduced priming and attentional effects by presenting all objects in a randomized order and at the same location.
Sixth, we reduced memory effects by using only unfamiliar objects and by measuring online ERPs rather than delayed behavioral responses.
We hope that these measures were effective in ruling out many alternative explanations for the top-down effects that we have observed, thus making a more compelling case against the claim that visual perception is "cognitively impenetrable."

An interactive view of object vision with an abundance of top-down feedback also challenges the predominantly feed-forward models in computer vision [e.g., @marr1982; @serre2007].
In fact, the lack of a semantic knowledge base that dynamically interacts with the processing of lower-level visual features may be one key reason why even state-of-the-art deep-learning algorithms need orders of magnitude more training examples to achieve human-level performance in object recognition.
For these models, single-trial learning of previously unfamiliar objects, as was observed on the behavioral and on the neurophysiological level in the present study, seems to be out of reach until the models overcome this "barrier of meaning" [@mitchell2020].
Drawing inspiration from cognitive psychology and human neuroscientific data may help to make these models more biologically plausible and, at the same time, more data efficient.

A theoretical framework that would explicitly predict or explain the observed P1 and N170 effects in our study is lacking at present.
The effects are consistent, however, with the reverse hierarchy theory [@ahissar2004; @hochstein2002], which posits that objects first enter our visual consciousness at an abstract, conceptual level.
Once this initial "vision at a glance" has taken place, feedback connections to earlier layers of the visual system are being accessed to extract the relevant lower-level features ("vision with scrutiny").
This reverse trajectory down the visual hierarchy may explain (a) the semantically induced changes to the fMRI signal in LOC and retinotopic cortex [@hsieh2010] as well as (b) the modulations of early visual ERP components observed in the present study and others [@abdelrahman2008; @maier2014; @maier2019; @samaha2018; @weller2019]. 
Besides this specific theory, an important role of top-down mechanisms for vision, or object recognition more specifically, is also posited by the family of predictive coding and Bayesian inference theories [e.g., @yuille2006; @clark2013; @lupyan2015; @panichello2013; @xu2007].
Despite the theoretical advances, a detailed description of these top-down effects at the algorithmic and implementational level remains a challenge for future work [@marr1982].

Taken together, the present study provides preliminary evidence that whenever we receive meaningful semantic information about a previously unfamiliar object, this information has an immediate influence on our visual processing of this object.
The immediacy of this influence is remarkable in at least two different ways:
First, it does not require extensive training but can instead be observed within the same trial in which the information has been presented and/or a single trial later.
Second, the time course of this influence suggests that it manifests itself not only at later, post-perceptual stages (> 400 ms), typically associated with semantic processing, but also at much earlier stages (< 200 ms), typically associated with visual perception itself.

# References

\bigskip

<div id="refs" custom-style="Bibliography"></div>

\newpage
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}

# Supplementary Information

\bigskip

```{r tabs1, results="asis"}
# Print table of materials
read_csv(here("example_stimuli", "stimuli.csv")) %>%
  mutate(
    match_ger = paste0(
      "\\textit{", match_noun_ger, "}, \\textit{", match_verb_ger, "}"
    ),
    match_en = paste0("{[", match_noun_en, ", ", match_verb_en, "]}"),
    nonmatch_ger = paste0(
      "\\textit{", nonmatch_noun_ger, "}, \\textit{", nonmatch_verb_ger, "}"
    ),
    nonmatch_en = paste0("{[", nonmatch_noun_en, ", ", nonmatch_verb_en, "]}"),
  ) %>%
  transmute(
    img = paste0(
      "\\includegraphics[valign=c,scale=0.25]{example_stimuli/", item, ".png}"
    ),
    match = paste(match_ger, match_en, sep = "\n") %>%
      kableExtra::linebreak(align = "l", linebreaker = "\n"),
    nonmatch = paste(nonmatch_ger, nonmatch_en, sep = "\n") %>%
      kableExtra::linebreak(align = "l", linebreaker = "\n")
  ) %>%
  apa_table(
    booktabs = TRUE,
    col.names = c(
      "Object", "Matching keywords", "Non-matching keywords"
    ),
    escape = FALSE,
    longtable = TRUE,
    caption = "Example stimuli\\smallskip",
    font_size = "footnotesize"
  )
```

\newpage

## Results S1. *Linear Mixed-Effects Models*

## *P1 Component (100--150 ms)*

\small
\singlespacing

```{r, ress1a}
# Helper function for printing model summary, ANOVA (F tests), and contrasts
print_model_outputs <- function(model_object) {
  model <- model_object$model
  model@call$data <- NULL # Prevents cluttering the `summary` output
  print(summary(model))
  cat("\n")
  print(model_object$anova)
  cat("\nFollow-Up Contrasts (Simple Effects)\n")
  print(model_object$contrasts)
  cat("\n\n")
}

# Print model outputs
print_model_outputs(models$P1)
```

\newpage
\normalsize
\doublespacing

## *N170 Component (150--200 ms)*

\small
\singlespacing

```{r, ress1b}
# Print model outputs
print_model_outputs(models$N170)
```

\newpage
\normalsize
\doublespacing

## *N400 Component (400--700 ms)*

\small
\singlespacing

```{r, ress1c}
# Print model outputs
print_model_outputs(models$N400)
```

\newpage

```{r, figs1, cache=!run$figures, fig.height=11, fig.cap="(ref:figure-s1-caption)"}
# Create topographic plots for control analysis of event-related power
plot_tfr_topos(
  filter(tfr_grand_ave, phase == "Insight"),
  filter(
    tfr_clusters, contrast == "Insight/Informed - Insight/Exclude_informed"
  ),
  p_cluster = p_cluster,
  condition_plus = "Informed",
  condition_minus = "Exclude_informed"
)
```

(ref:figure-s1-caption) Time-frequency results for the control analysis.
Each topographic plot shows the difference in event-related power (in units of percent signal change) between the semantically informed condition and the "failed insight" condition (see Results: Control Analysis in the main text), grand-averaged across participants.
Black dots highlight EEG channels that were part of a cluster for which this difference was statistically significant ($p_\text{cluster}$ = `r print_p(unique(filter(tfr_clusters, contrast == "Insight/Informed - Insight/Exclude_informed" & p_val < p_cluster)$p_val))`).

\newpage

```{r, figclusters, fig.height=8, include=FALSE}
# Plot thresholded cluster images
clusters %>%
  filter(p_val < .05) %>%
  ggplot(aes(x = time, y = channel, fill = cluster)) +
  facet_grid(~contrast, drop = FALSE) +
  geom_raster() +
  theme(legend.position = "top")
```

```{r, figtfrclusters, fig.height=12, include=FALSE}
# Plot thresholded cluster images for the insight phase
tfr_clusters %>%
  filter(contrast == "Insight/Informed - Insight/Naive" & p_val < .05) %>%
  ggplot(aes(x = time, y = freq, fill = cluster)) +
  facet_wrap(~channel) +
  geom_raster() +
  theme(legend.position = "top")

# Plot thresholded cluster images for the post-insight phase
tfr_clusters %>%
  filter(
    contrast == "Post-insight/Informed - Post-insight/Naive" & p_val < .05
  ) %>%
  ggplot(aes(x = time, y = freq, fill = cluster)) +
  facet_wrap(~channel) +
  geom_raster() +
  theme(legend.position = "top")

# Plot thresholded cluster images for the control analysis
tfr_clusters %>%
  filter(
    contrast == "Insight/Informed - Insight/Exclude_informed" & p_val < .05
  ) %>%
  ggplot(aes(x = time, y = freq, fill = cluster)) +
  facet_wrap(~channel) +
  geom_raster() +
  theme(legend.position = "top")
```

---
title             : "Instant effects of semantic information on visual perception"
shorttitle        : "Semantically informed perception"

author: 
  - name          : "Alexander Enge"
    affiliation   : "1,2"
    corresponding : "yes"
    address       : "Rudower Chaussee 18, 12489 Berlin, Germany"
    email         : "alexander.enge@hu-berlin.de"
  - name          : "Franziska Süß"
    affiliation   : "3"
  - name          : "Rasha Abdel Rahman"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Humboldt-Universität zu Berlin"
  - id            : "2"
    institution   : "Max Planck Institute for Human Cognitive and Brain Sciences"
  - id            : "3"
    institution   : "Fachhochschule des Mittelstands"

authornote: |
  \addORCIDlink{Alexander Enge}{0000-0003-0100-2297}
  
  \addORCIDlink{Rasha Abdel Rahman}{0000-0002-8438-1570}
  
  The authors declare no competing financial interests.
  The preprocessed data and analaysis code for this study are openly available at https://osf.io/uksbc/.

abstract: |
  Does our perception of an object change as soon as we discover what function it serves?
  This question matters not only when we encounter novel tools or gadgets in our everyday lives, but also pertains to the long-standing debate around the (im)penetrability of perception by higher cognitive capacities.
  Here, we showed human participants (*n* = 48) pictures of unfamiliar objects either together with words matching their function, leading to semantically informed perception, or together with non-matching words, resulting in naive perception.
  We measured event-related potentials (ERPs) to investigate at which stages in the visual processing hierarchy these two types of object perception differed from one another.
  We found that semantically informed as compared to naive perception was associated with larger amplitudes in the N170 component (150--200 ms) and reduced amplitudes in the N400 component (400--700 ms).
  When the same objects were presented once more without any information, the N400 effect persisted and we also observed enlarged amplitudes in the P1 component (100--150 ms) in response to objects for which semantically informed perception had taken place.
  Consistent with previous work, our results suggest that obtaining semantic information about previously unfamiliar objects alters aspects of their lower-level visual perception (P1 component), higher-level visual perception (N170 component), and semantic processing (N400 component).
  We demonstrate for the first time that these effects are instantaneous in the sense that they can be observed within the same trial as well as one trial after the information has been given, providing strong evidence that cognition affects perception.

keywords          : "objects, semantic knowledge, visual perception, event-related potentials"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

bibliography      : ["manuscript_files/references.bib", "manuscript_files/r-references.bib"]
csl               : "manuscript_files/apa.csl"

documentclass     : "apa7"
classoption       : "man,donotrepeattitle"
fontsize          : "10pt"

editor_options: 
  chunk_output_type: console
  
output:
  papaja::apa6_pdf:
    latex_engine: xelatex

header-includes:
  - \geometry{a4paper}
  - \fancyheadoffset[R,L]{0pt}
  - \raggedbottom
  - \usepackage[all]{nowidow}
  - \usepackage[bottom]{footmisc}
  - \interfootnotelinepenalty=10000
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \usepackage{makecell}
  - \renewcommand{\cellset}{\renewcommand{\arraystretch}{0.7}}
  - \captionsetup{font={stretch=1}, belowskip=15pt}
---

```{r, setup, include=FALSE}
# Load R packages
library(buildmer)
library(emmeans)
library(here)
library(knitr)
library(lme4)
library(lmerTest)
library(MASS)
library(papaja)
library(cowplot)
library(parallel)
library(reticulate)
library(tidyverse)
library(magrittr)

# Set global options
options(readr.show_col_types = FALSE)
opts_chunk$set(fig.width = 12, include = FALSE, out.width = "100%")

# Specify directory paths
data_dir <- here("data")
files_dir <- here("manuscript_files")
output_dir <- here("output")

# Load custom helper functions for creating figures and tables
source(here(files_dir, "helper_functions.R"))

# Re-run steps that take a long time to compute?
run <- list(
  eeg_processing = FALSE,
  mixed_models = FALSE
)

# Write R packages to bibliography
r_refs(here(files_dir, "r-references.bib"), append = FALSE)
```

# Introduction

Does our perception of an object change as soon as we discover what function it serves?
This question speaks not only to our everyday experiences, where we encounter novel tools and gadgets in our dynamic working and private environments.
It also pertains to the long-standing debate about the cognitive (im)penetrability [@pylyshyn1999] of perception by higher-level capacities such as semantic knowledge or language.
According to one view, these cognitive capacities kick in only after the retinal input has been processed by a specialized module for visual perception [@firestone2016; @fodor1983].
This module is encapsulated from higher-level inputs and therefore processes the visual information in a purely feedforward fashion, progressing from lower areas with small receptive field sizes to areas representing increasingly complex shapes and, eventually, whole objects [@dicarlo2012].
This architecture has been mirrored in classical [e.g., @marr1982] and contemporary [e.g., @krizhevsky2012] computer vision models, allowing them to achieve human-level perfomance in predicting object category labels from images [for review, see @lindsay2020].

The cognitive impenetrability hypothesis is challenged by the alternative view that perception dynamically interacts with different aspects of cognition from early on [@churchland1994; @lupyan2015].
This view is supported by a variety of theoretical, behavioral, and neurophysiological accounts.
On the theoretical level, the reverse hierarchy theory [@ahissar2004; @hochstein2002] posits that conscious processing initally occurs at the level of whole objects or object categories.
Only after this high-level interpretation has been obtained, more fine grained visual details---if relevant for the current task---are being accessed via top-down connections.
Along similar lines, Bayesian inference and predictive coding theories [e.g., @clark2013; @panichello2013; @yuille2006] propose bi-directional loops of predictions and prediction errors between visual and non-visual processes.

On the behavioral level, these ideas are supported by psychological studies that showed differences in ratings, detection rates, or reaction times for visual stimuli depending on their emotional [e.g., @phelps2016], motivational [e.g., @balcetis2010], linguistic [e.g., @boutonnet2015] or semantic [e.g., @gauthier2003] background.
However, some of these studies confounded tentantive high-level effects with low-level differences between conditions.
Furthermore, many studies were not able to discern between perceptual and post-perceptual (e.g., memory-related) effects [@firestone2016].

On the neurophysiological level, event-related potentials (ERPs) measured from the human EEG can mitigate most of these concerns:
Their excellent temporal resolution allows us to probe how early we can detect influences of high-level (e.g., semantic) information.
To this end, participants were typically trained to associate visual objects with different amounts of semantic information.
After training, an orthogonal task was used to compare the ERPs in response to semantically trained objects versus non-semantically trained objects.
This revealed differences not only in late ERP components associated with semantic processing (i.e., the N400 component) but also in the visual P1 component [@abdelrahman2008; @maier2019; @samaha2018; @weller2019].
The early peak of this component and its source in the occipital cortex [@abdelrahman2008] point to an immediate effect of semantic knowledge on visual perception.

Here we measured ERPs in response to unfamiliar objects instantly while participants gained an semantic understanding of their function.
To this end, we presented half of the objects together with matching keywords, thus allowing participants to understand what kind of object they were viweing, or together with non-matching keywords, thus keeping the perception of the object semantically naive.
We then presented the same objects again to test for downstream effects of semantic information as in previous studies.
We examined the influence of semantic information on ERPs associated with lower-level visual perception (P1 component), higher-level visual perception (N170 component), and semantic processing (N400 component).

# Materials and Methods

## Participants

Participants were 48 German native speakers (31 female, 17 male) with a mean age of 23.5 years (range 18--32) and no history of psychological disorder or treatment.
No a priori power analysis was carried out and the sample size was chosen in line with previous EEG studies on object processing in our lab.
All participants were right-handed according to the Edinburgh inventory [@oldfield1971] and reported normal or corrected-to-normal vision.
They provided written informed consent before starting the experiment and received a compensation of €8 per hour for participating.

## Materials

Stimuli consisted of 240 grayscale photographs of real-world objects.
Of these, 120 were well-known everyday objects (e.g., a bicycle, a toothbrush) and served as filler stimuli of no interest.
The other 120 were rare objects presumed to be unfamiliar to the majority of participants (e.g., a galvanometer, an udu drum; see Appendix A).
All stimuli were presented on a light blue background with a size of 207 × 207 pixels on a 19-inch LCD monitor with a resolution of 1,280 × 1,024 pixels and a refresh rate of 75 Hz.
At a standardized viewing distance of 90 cm, the images subtended approximately 3.9 degrees of participants' horizontal and vertical visual angle.

For each unfamiliar object, a pair of German keywords---a noun and a verb---was selected, describing the typical function or use of the object in a way that could typically be related to its visual features and their configuration (e.g., voltage, measuring; clay pot, drumming).
As our central experimental manipulation, the presentation of half of the objects was preceded by keywords that correctly matched their respective function, whereas the presentation of the other half of the objects was preceded by non-matching keywords belonging to one of the other objects.
The matching keywords were expected to induce semantically informed perception (i.e., participants suddenly understanding what kind of object they were viewing), whereas the non-matching keywords were expected to prevent such an understanding and keep the perception of the object semantically naive.
All participants saw each unfamiliar object with only one type of keywords (matching or non-matching).
This assignment of keywords to objects was counterbalanced across participants so that each object was presented with matching keywords (leading to semantically informed perception) and non-matching keywords (leading to naive perception) to an equal number of participants.
The experiment was programmed and displayed using Presentation® software (Neurobehavioral Systems, Inc., Berkeley, CA, www.neurobs.com).

## Experimental Design

The experiment consisted of three parts (see Figure \@ref(fig:fig1)A).
In the *pre-insight* part, after written informed consent had been obtained and the EEG had been prepared, all 240 familiar and unfamiliar objects were presented once in random order and without any keywords.
Each trial consisted of a fixation cross presented in the middle of the screen for 0.5 s, followed by the presentation of the object until participants made a response or until a time out after 3 s.
The inter-trial interval was 0.5 s and participants took a self-timed break after each block of 60 objects.
The task, which was kept the same across all three parts, was to classify each object using one of four response alternatives: (a) "I know what this is or have a strong assumption," (b) "I have an assumption what this is," (c) "I have rather no assumption what this is," or (d) "I don't know what this is and have no assumption." Participants were asked to respond as quickly and as accurately as possible by pressing one out of four buttons with the index or middle finger of their left or right hand, respectively.
The mapping of the rating scale to the four buttons (left to right or right to left) was counterbalanced across participants.

```{r, eeg_processing, eval=run$eeg_processing}
# Process behavioral data
c(
  list.files(here(data_dir, "rt/exp1"), "*.txt", full.names = TRUE),
  list.files(here(data_dir, "rt/exp2"), "*.txt", full.names = TRUE)
) %>%
  # Process each log file
  map(function(log_file) {

    # Read the file
    read_tsv(
      log_file,
      col_types = cols(),
      locale = locale(encoding = "latin1")
    ) %>%
      # Recode some columns
      transmute(

        # Phase of the experiment
        phase = factor(Wdh,
          levels = c(211, 212, 213),
          labels = c("Pre-insight", "Insight", "Post-insight")
        ),

        # Keyword conditions
        keywords = factor(Bed,
          levels = c("richtig", "falsch"),
          labels = c("Match", "Non-match")
        ),

        # Behavioral responses
        # 201: "I know what this is or have a strong assumption"  -> Recode as 4
        # 202: "I have an assumption what this is"                -> Recode as 3
        # 203: "I have rather no assumption what this is"         -> Recode as 2
        # 204: "I don't know what this is and have no assumption" -> Recode as 1
        response = 5 - (Tastencode - 200),

        # Reaction times
        rt = RT,

        # Item IDs
        item_id = factor(StimID)
      ) %>%
      # Remove filler items
      drop_na(keywords) -> log

    # Assign items to conditions based on manipulation (matching vs. non-
    # matching keywords) and the responses of the participants
    items_per_condition <- with(log, {
      list(

        # Informed condition: Matching keywords and positive response
        "Informed" = item_id[
          phase == "Insight" & keywords == "Match" & response %in% c(3, 4)
        ],

        # Naive condition: Non-matching keywords and negative response
        "Naive" = item_id[
          phase == "Insight" & keywords == "Non-match" & response %in% c(1, 2)
        ],

        # Exclude: Matching keywords and negative response
        "Exclude_informed" = item_id[
          phase == "Insight" & keywords == "Match" & response %in% c(1, 2)
        ],

        # Exclude: Non-matching keywords and positive response
        "Exclude_naive" = item_id[
          phase == "Insight" & keywords == "Non-match" & response %in% c(3, 4)
        ],

        # Exclude: Positive response *before* any info was presented
        "Exclude_known" = item_id[phase == "Pre-insight" & response == 4]
      )
    })

    # Assign these conditions to the trials of all three phases
    log$condition <- NA
    for (condition in names(items_per_condition)) {
      item_id <- items_per_condition[[condition]]
      log$condition[log$item_id %in% item_id] <- condition
    }

    # Sort columns
    log %>%
      select(item_id, phase, condition, response, rt) %>%
      return()
  }) -> logs

# Import EEG pipeline from Python
pipeline <- import("pipeline")

# Get list of BrainVision EEG header files
vhdr_files <- c(
  list.files(here(data_dir, "eeg/exp1"), "*.vhdr", full.names = TRUE),
  list.files(here(data_dir, "eeg/exp2"), "*.vhdr", full.names = TRUE)
)

# Process EEG data
res <- pipeline$group_pipeline(
  vhdr_files = vhdr_files,
  log_files = logs,
  output_dir = output_dir,
  report_dir = NULL,
  downsample_sfreq = 125,
  bad_channels = "auto",
  ocular_correction = "auto",
  highpass_freq = 0.1,
  lowpass_freq = 40,
  triggers = c(221, 222),
  epochs_tmin = -0.5,
  epochs_tmax = 1.5,
  baseline_tmin = -0.2,
  baseline_tmax = 0.0,
  reject_peak_to_peak = 200,
  components = list(
    "name" = list("P1", "N170", "N400"),
    "tmin" = list(0.1, 0.15, 0.4),
    "tmax" = list(0.15, 0.2, 0.7),
    "roi" = list(
      c("PO3", "PO4", "POz", "O1", "O2", "Oz"),
      c("P7", "P8", "PO7", "PO8", "PO9", "PO10"),
      c("C1", "C2", "Cz", "CP1", "CP2", "CPz")
    )
  ),
  average_by = c("phase/condition"),
  perform_tfr = TRUE,
  tfr_subtract_evoked = FALSE,
  tfr_freqs = seq(4, 40, by = 1.0),
  tfr_cycles = seq(2, 20, by = 0.5),
  tfr_baseline_tmin = -0.45,
  tfr_baseline_tmax = -0.05,
  tfr_baseline_mode = "percent",
  perm_contrasts = list(
    c("Pre-insight/Informed", "Pre-insight/Naive"),
    c("Insight/Informed", "Insight/Naive"),
    c("Post-insight/Informed", "Post-insight/Naive")
  ),
  perm_tmin = 0.0,
  perm_tmax = 1.0,
  n_jobs = 1
)
```

```{r, mixed_models, eval=run$mixed_models}
# Read single trial data
read_csv(trials_file, col_types = cols(
  participant_id = col_factor(),
  item_id = col_factor(),
  phase = col_factor(),
  condition = col_factor()
)) %>%
  # Mark unrealistically short RTs
  mutate(rt = ifelse(rt < 200, NA, rt)) -> trials

# Filter relevant conditions for the main analysis
trials %>%
  mutate(condition = factor(condition, levels = c("Informed", "Naive"))) %>%
  drop_na() -> trials_main

# Contrast coding for phase
cbind(
  c("Pre-insight" = -1, "Insight" = 1, "Post-insight" = 0),
  c("Pre-insight" = 0, "Insight" = -1, "Post-insight" = 1)
) %>%
  t() %>%
  ginv() -> contrasts(trials_main$phase)

# Contrast coding for condition
cbind(c("Informed" = 1, "Naive" = -1)) %>%
  t() %>%
  ginv() -> contrasts(trials_main$condition)

# Construct model formula
form <- ~ phase * condition +
  (phase * condition | participant_id) +
  (phase * condition | item_id)

# Fit linear mixed models
deps <- c("P1", "N170", "N400")
map(deps, function(dep, model_formula = form, model_data = trials_main) {

  # Modify the formula such that fixed effects are always retained
  model_formula %<>%
    tabulate.formula() %>%
    mutate(block = replace(block, is.na(grouping), "fixed"))

  # Start a CPU cluster so that multiple models can be fitted in parallel
  n_cores <- detectCores()
  cl <- makeCluster(n_cores)

  # Find the best fitting model (see Matuschek et al., 2017, *JML*)
  build <- buildmer(
    buildmerControl = buildmerControl(
      formula = model_formula,
      data = model_data,
      args = list("control" = lmerControl(
        optimizer = "bobyqa", optCtrl = list(maxfun = 1e6)
      )),
      direction = c("backward", "backward"),
      cl = cl,
      elim = LRTalpha(.20),
      calc.anova = TRUE,
      ddf = "Satterthwaite",
      dep = dep
    )
  )
  stopCluster(cl)

  # Compute marginal means and contrasts
  emm_options(lmer.df = "Satterthwaite", lmerTest.limit = Inf)
  specs <- pairwise ~ condition | phase
  em <- emmeans(build@model, specs, infer = TRUE)

  # Create list of returns
  list(
    "model" = build@model,
    "summary" = build@summary,
    "anova" = build@anova,
    "means" = em$emmeans,
    "contrasts" = em$contrasts
  ) %>%
    return()
}) %>%
  # Add ERP component names
  set_names(deps) -> models

# Save models
saveRDS(models, models_file)
```

```{r, fig1, include=TRUE, fig.height=12, fig.cap="(ref:figure-1-caption)"}
# Read pipeline configuration
config <- jsonlite::read_json(here(output_dir, "config.json"))

# Read by-participant ERP averages and channel locations
evokeds <- read_csv(here(output_dir, "ave.csv"))

# Read channel locations
channel_locations <- read_csv(here(output_dir, "channel_locations.csv"))

# Read fitted models
models <- readRDS(here(output_dir, "models.RDS"))

# Create Figure 1
plot_fig1(files_dir, evokeds, config, channel_locations, models)
```

```{r, table_1, include=TRUE}
# View model summaries
walk(models, function(model) {
  print(formula(model$model))
  cat("\n")
  print(model$anova)
  cat("\n")
  print(model$contrasts)
  cat("\n\n")
})
```

```{r, clusters, include=TRUE, fig.height=8}
# Read results of cluster-based permutation tests for ERPs
read_csv(here(output_dir, "clusters.csv"), col_types = cols(
  contrast = col_factor(levels = c(
    "Pre-insight/Informed - Pre-insight/Naive",
    "Insight/Informed - Insight/Naive",
    "Post-insight/Informed - Post-insight/Naive"
  ))
)) -> clusters

# Plot thresholded cluster images
clusters %>%
  filter(p_val < .05) %>%
  ggplot(aes(x = time, y = channel, fill = cluster)) +
  facet_grid(~contrast, drop = FALSE) +
  geom_raster() +
  theme(legend.position = "top")
```

```{r, tfr, include=TRUE, fig.height=12}
# Read results of cluster-based permutation tests for TFR
read_csv(here(output_dir, "tfr_clusters.csv"), col_types = cols(
  contrast = col_factor(levels = c(
    "Pre-insight/Informed - Pre-insight/Naive",
    "Insight/Informed - Insight/Naive",
    "Post-insight/Informed - Post-insight/Naive"
  ))
)) -> tfr_clusters

# Plot thresholded cluster images for the insight phase
tfr_clusters %>%
  filter(contrast == "Insight/Informed - Insight/Naive" & p_val < .05) %>%
  ggplot(aes(x = time, y = freq, fill = cluster)) +
  facet_wrap(~channel) +
  geom_raster() +
  theme(legend.position = "top")

# Plot thresholded cluster images for the post-insight phase
tfr_clusters %>%
  filter(
    contrast == "Post-insight/Informed - Post-insight/Naive" & p_val < .05
  ) %>%
  ggplot(aes(x = time, y = freq, fill = cluster)) +
  facet_wrap(~channel) +
  geom_raster() +
  theme(legend.position = "top")
```

```{r}
# Read grand-averaged power
tfr_grand_ave <- read_csv(here(output_dir, "tfr_grand_ave.csv")) %>%
  mutate(
    phase = factor(phase, levels = c("Pre-insight", "Insight", "Post-insight")),
    condition = factor(condition, levels = c("Informed", "Naive"))
  ) %>%
  filter(!is.na(condition))

channels <- tfr_grand_ave %>%
  select(-average_by, -phase, -condition, -time, -freq) %>%
  colnames()

tfr_grand_ave_informed <- filter(tfr_grand_ave, condition == "Informed")
tfr_grand_ave_naive <- filter(tfr_grand_ave, condition == "Naive")
tfr_grand_ave_diff <- mutate(tfr_grand_ave_informed, condition = "Difference")
tfr_grand_ave_diff[channels] <- tfr_grand_ave_informed[channels] -
  tfr_grand_ave_naive[channels]

bind_rows(tfr_grand_ave, tfr_grand_ave_diff) %>%
  mutate(
    condition = factor(condition, levels = c("Informed", "Naive", "Difference"))
  ) %>%
  pivot_longer(
    cols = all_of(channels), names_to = "electrode", values_to = "amplitude"
  ) %>%
  select(-average_by, -electrode) %>%
  group_by(phase, condition, time, freq) %>%
  summarise(amplitude = mean(amplitude), .groups = "drop") %>%
  ggplot(aes(x = time, y = freq, fill = amplitude)) +
  facet_grid(phase ~ condition) +
  geom_raster() +
  scale_fill_viridis_c(option = "turbo", limits = c(-0.6, 0.6)) +
  coord_cartesian(expand = FALSE)

tmins <- seq(0.0, 1.2, 0.2)
fmins <- seq(4, 36, 4)

channel_locations <- read_csv(here(output_dir, "channel_locations.csv"))

map(c("Pre-insight", "Insight", "Post-insight"), function(this_phase) {
  map(tmins, function(tmin, tstep = 0.2) {
    tmax <- tmin + tstep
    map(fmins, function(fmin, fstep = 4) {
      fmax <- fmin + fstep
      tfr_grand_ave_diff %>%
        filter(time >= tmin & time < tmax & freq >= fmin & freq < fmax) %>%
        filter(phase == this_phase) %>%
        pivot_longer(
          cols = all_of(channels), names_to = "electrode", values_to = "power"
        ) %>%
        left_join(channel_locations, by = c("electrode" = "channel")) %>%
        group_by(electrode, x, y) %>%
        summarise(power = mean(power), .groups = "drop") %>%
        eegUtils::topoplot(
          quantity = "power",
          contour = FALSE,
          interp_limit = "skirt",
          scaling = 0.7
        ) +
        scale_fill_viridis_c(limits = c(-0.3, 0.3), option = "turbo") +
        theme(legend.position = "none")
    }) %>%
      plot_grid(plotlist = ., nrow = 1)
  }) %>%
    plot_grid(plotlist = ., nrow = length(tmins))
})
```

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

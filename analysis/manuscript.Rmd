---
title             : |
  | Event-Related Potentials of the Semantically Informed
  | Perception of Unfamiliar Objects
shorttitle        : "Semantic Knowledge and Unfamiliar Objects"

author: 
  - name          : "Alexander Enge"
    affiliation   : "1,2"
  - name          : "Franziska Süß"
    affiliation   : "3"
  - name          : "Rasha Abdel Rahman"
    affiliation   : "1"
    corresponding : yes
    address       : "Rudower Chaussee 18, 12489 Berlin"
    email         : "rasha.abdel.rahman@hu-berlin.de"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Humboldt-Universität zu Berlin"
  - id            : "2"
    institution   : "Research Group Learning in Early Childhood, Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany"
  - id            : "3"
    institution   : "Fachhochschule des Mittelstands, Bamberg, Germany"

authornote: |
  \addORCIDlink{Alexander Enge}{0000-0003-0100-2297}
  
  \addORCIDlink{Rasha Abdel Rahman}{0000-0002-8438-1570}

  The preprocessed data and code for this study are openly available at https://osf.io/uksbc/.
  
  We have no conflict of interest to disclose.

abstract: |
  Does our perception of an object change as soon as we discover what it is for? This question is relevant not only for our everyday lives, where we may encounter novel tools and gadgets as parts of our dynamic working and private environments; it pertains to the long-standing debate around the (im)penetrability of perception by "higher" cognitive capacities. In this experiment, we showed participants (*n* = 24) pictures of 120 unfamiliar objects either together with valid information about their function---leading to semantically informed perception---or together with invalid information---resulting in naive perception. We measured event-related potentials (ERPs) to investigate at which stages in the visual processing hierarchy these two types of perceiving objects differed from one another. We found that semantically informed as compared to naive perception was associated with larger amplitudes in the N170 component and reduced amplitudes in the N400 component. When the same objects were presented once more (without any information), the N400 effect persisted and we now also observed enlarged amplitudes in the P1 component in response to objects for which semantically informed perception had taken place. We replicated these novel findings in an independent sample (*n* = 24). Consistent with previous work, they suggest that obtaining semantic information about previously unfamiliar objects alters aspects of their lower-level visual perception (P1 component), higher-level visual perception (e.g., holistic perception; N170 component), and semantic processing (N400 component).
  
keywords          : "objects, semantic knowledge, visual perception, event-related potentials"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

bibliography      : "manuscript_files/r-references.bib"
csl               : "manuscript_files/apa.csl"
zotero            : "aha"
documentclass     : "apa7"
classoption       : "man"
output            :
  papaja::apa6_pdf:
    latex_engine  : "xelatex"

header-includes:
  - \geometry{a4paper}
  - \fancyheadoffset[R,L]{0pt}
  - \raggedbottom
  - \usepackage[all]{nowidow}
  - \usepackage[bottom]{footmisc}
  - \interfootnotelinepenalty=10000
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \captionsetup[table]{font={stretch=1.5}}
  - \captionsetup[figure]{font={stretch=1.5}}
---

```{r setup, include=FALSE}
# Load packages
library(papaja)
library(reticulate)
library(tidyverse)
library(magrittr)
library(cowplot)

# Set global chunk options
knitr::opts_chunk$set(include = FALSE, fig.align = "center", out.width = "100%", fig.width = 10)

# Link to the bibliography file
r_refs("analysis/manuscript_files/r-references.bib")

# Create an export folder for results
dir.create("analysis/export", showWarnings = FALSE)

# Do you want to re-run the EEG preprocessing? Note that this can only be used with the raw EEG data, which are
# available from the corresponding author (rasha.abdel.rahman@hu-berlin.de) upon request.
rerun_preproc <- FALSE

# Do you want to re-run the linear mixed-effect models? This can be done even if only the preprocessed single-trial
# data are available (from GitHub and/or the OSF). Note, however, that the model fitting may take more than 24 hours.
rerun_models <- FALSE
```

How does our perception of an object change as soon as we discover what it is for? In this study, we presented participants with unfamiliar objects which were preceded either by matching information about their function, leading to semantically informed perception, or by non-matching information, leading to naive perception. Semantically informed perception of objects was expected to differ from naive perception at certain stages in the processing hierarchy and we aimed to identify these stages by capitalizing on the high temporal resolution of event-related brain potentials (ERPs).

The claim that higher-level cognitive capacities such as knowledge or language can modulate lower-level perceptual processes has been the source of a long-running debate. This debate revolves around the question of whether or not our perception of the (visual) world is "cognitively impenetrable" or, in other words, determined solely by the visual input reaching our retina. According to one view, higher-level mental states and functions are not able to penetrate perception and kick in only after lower-level perceptual analysis of the visual input has taken pace [e.g., @firestone2016; @fodor1983; @fodor1984; @pylyshyn1999]. Visual perception itself is treated as an encapsulated module that processes the retinal input in a feed-forward fashion, progressing from lower areas with smaller receptive field sizes to areas representing increasingly complex shapes and, eventually, whole objects [@dicarlo2012; @goodale1992; @marr1982].

This position is challenged by the alternative view that perceptual processing dynamically interacts with other aspects of cognition from early on [e.g., @churchland1988; @churchland1994; @lupyan2015; @teufel2017]. The myriad feedback connections from areas higher up the visual hierarchy (e.g., medial- and inferotemporal areas MT and IT) to early visual areas [e.g., visual areas V1 and V2\; @bullier2001; @gilbert2013]---as well as behavioral evidence---have inspired theories of visual perception that emphasize the top-down influence of cognitive processes. For instance, the reverse hierarchy theory [@ahissar2004; @hochstein2002] assumes that conscious visual perception occurs initially at the level of whole objects or object categories. Only after this high-level interpretation of the stimulus has been obtained, its more for fine grained visual details---if relevant for the current task---are being accessed from lower-level areas via top-down connections. Along similar lines, the label feedback hypothesis [@lupyan2012] posits that the activation of an object's name (a high level property) transiently warps perceptual space so that its diagnostic visual features are being processed preferentially. Finally, an active role of top-down influences is also promoted by theories of vision falling under the umbrella of Bayesian inference and predictive coding [e.g., @lupyan2015; @panichello2013; @yuille2006].

Empirical findings from experimental psychology have recently added additional support to the view that perception can indeed be penetrated by other aspects of cognition [for review, see @collins2014; @vetter2014]. These aspects of cognition may include transient states such as emotions [e.g., @bocanegra2009; @phelps2016] or intentions [e.g., @balcetis2010; @cole2012] as well as learned capacities such as language [e.g., @boutonnet2015; @maier2018; @mo2011] or declarative knowledge about faces [e.g., @abdelrahman2011; @suess2015; @eiserbeck2020] and objects [e.g., @abdelrahman2008; @lin1997; @weller2019]. Despite this wealth of empirical evidence, critics of the idea that cognitive functions can penetrate perception have remained skeptical and pointed out important methodological shortcomings in large parts of this literature [@firestone2016; @machery2015]. They have argued, for example, that most behavioral paradigms have not been able to discern between perceptual and postperceptual (e.g., memory-related) effects. Furthermore, cognitive influences in some studies have been confounded with differences in the lower-level visual input. We will revisit these and other potential pitfalls with regards to the present study in the Discussion section once we have laid out our methodology and results.

In general, however, it is important to note that one productive way to circumvent some of these concerns is to use neurophysiological recording methods such as ERPs [@luck2014]: Their high temporal resolution allows for a direct test of which processing stages---perceptual and/or postperceptual---are influenced by higher-level capacities such as semantic knowledge. This is made possible by the fact that when participants are presented with visual stimuli, their averaged brain responses show characteristic deflections (ERP components), each of them with its typical polarity (positive or negative), peak latency, spatial scalp distribution, and functional role(s).

The visual P1 component of the ERP refers to a positive deflection peaking as early as 100--150 ms after stimulus onset at occipital channels. It is generated by multiple sources in the extrastriatal cortices of the middle occipital and fusiform gyri [@dirusso2001; @foxe2002; @mangun1995]. The P1 is thought to reflect the processing of low-level visual characteristics of the stimulus [e.g., size, luminance, and contrast\; @dirusso2001; @luck2014; @johannes1995], which is why it being modulated by higher-level cognitive capacities would challenge a purely bottom-up, encapsulated view of visual perception. It is well-established that the P1 is enhanced when participants pay attention to the spatial location of a stimulus [@luck2000; @mangun1995; @mangun1991] but there is also more recent evidence for higher-level cognitive influences that go beyond spatial attention (see below).

The visual N170 (or N1) component refers to the negative deflection following the P1 and is maximal around 150--200 ms after stimulus onset at occipito-temporal channels. Just as the P1, it is influenced by visual parameters of the stimulus [@johannes1995] and by selective attention, especially when participants are required to discriminate between different visual stimuli [@vogel2000; @mangun1991]. The N170 is enlarged (i.e., more negative) in response to faces [@bentin1996; @rossion2011] and other visual stimuli for which one happens to be an expert [e.g., birds, dogs, or cars\; @gauthier2003a; @tanaka2001]. This category selectivity seems to reflect both the processing of individual visual features of the respective objects (including faces) as well as the holistic configuration of these features [@eimer2011; @jacques2010; @rossion1999; @sagiv2001]. Thus, a modulation of the N170 by cognitive capacities such as knowledge would suggest these capacities having an influence on higher-level, holistic perception of the visual stimulus.

Finally, the N400 component refers to more negative ERP amplitudes when stimuli require more as compared to less resources for semantic processing or integration [@kutas2011; @lau2008; @rabovsky2018]. It begins approximately 300 ms after stimulus onset and is most pronounced at centro-parietal channels. A modulation of the N400 component by cognitive capacities such as knowledge can be taken as evidence for a post-perceptual influence on stimulus processing.

Previous studies have focused on these and other ERP components to investigate which processing stages are indeed influenced by knowledge about visual objects [@abdelrahman2008; @gratton2009; @maier2014; @maier2019; @rossion2002; @rossion2004; @tanaka2001; @samaha2018; @weller2019]. Together, they suggest that not only higher-level ERPs like the semantic N400 component are influenced by acquiring information about objects, but also earlier components that typically reflect bottom-up visual processing. This was the case, for instance, when participants were presented with a range of unfamiliar objects, receiving in-depth verbal descriptions about their function for half of the objects and irrelevant verbal information (i.e., cooking recipes) for the other half of the objects [@abdelrahman2008, Experiment 1]. After this learning phase, the same objects were presented in three different ERP tasks which did not require explicit access to any of the learned semantic information. In all three tasks, ERP amplitudes in response to objects for which in-depth knowledge had been acquired differed from those in response to objects for which this had not been the case. Crucially, these differences occurred not just in the N400 component, indexing a modulation of post-perceptual semantic processing, but also in the P1 component, suggesting a top-down modulation of lower-level perceptual processing. Interestingly, the ERPs in the in-depth knowledge condition even were qualitatively similar to those for untrained but well-known objects. The modulation of the P1 component has recently been replicated when the same objects were presented under circumstances of limited attentional resources in an attentional blink paradigm [@weller2019]. Here, the (neurophysiological) differences in P1 amplitudes between objects with in-depth versus minimal knowledge were correlated with (behavioral) differences in their detection rate during the attentional blink. This can be taken as further evidence for an influence of semantic knowledge on the early visual perception of unfamiliar objects.

A complementary approach has been taken in a recent study investigating the ERPs in response to familiar (rather than unfamiliar) objects which were rendered difficult to recognize by converting them to two-tone, "Mooney" images [@samaha2018, Experiment 4]. In this experiment, participants were trained with meaningful verbal cues, telling them what kind of objects they should look for in the images, or with a non-meaningful perceptual task, familiarizing them with the images but not with their semantic content. When the EEG was measured in a delayed matching task later on, the two types of training led to differential effects, again suggesting a modulation of lower-level visual perception: Over left posterior electrodes, P1 amplitudes were larger and alpha power was higher for meaning-trained images than for perceptually-trained images. As before, these effects of perceiving objects in a semantically informed way seem to be behaviorally relevant, as participants tended respond faster and more accurately to meaning-trained as compared to perceptually-trained images in the matching task. This alternative approach further corroborates an early influence of semantic information on the visual perception of objects.

All the previously mentioned studies have in common that they investigated the effects of semantic knowledge on the visual perception of objects only *after* an extensive learning phase had taken place [@abdelrahman2008; @gauthier2003; @maier2014; @maier2019; @rossion2002; @rossion2004; @samaha2018; @weller2019]. Participants performed one or multiple training sessions during which they encountered each object together with its respective label or description multiple times. The EEG was usually not measured---or at least not analyzed and reported---during these training sessions, which sometimes took place on a separate day [@abdelrahman2008; @maier2014; @maier2019] or were spread out across multiple days or weeks [@rossion2002; @rossion2004]. While experimental designs with such substantive training maximize the chances of detecting even subtle top-down effects of the knowledge acquired, they leave at least three conceptual questions unresolved.

The first question is if we can detect any electrophysiological correlates of *semantic insight*, that is, the critical presentation during which an understanding of the previously unfamiliar object is happening (instead of asking if this understanding leads to differential effects in an orthogonal task later on). The second question is how much learning is actually necessary before reliable top-down effects of knowledge on ERP amplitudes can be obtained: Does it actually take dozens of repetitions per object or may a single exposure to the object together with the relevant semantic information be enough? The third and closely related question is about the nature of the effects of knowledge on perception: Are they reflecting genuine top-down effects of the semantic system altering perception online (while the object is being perceived), or do they merely reflect the (re-)activation of stored representations of the objects which have been altered over the course of the learning process? By measuring the ERPs before, during, and after participants received semantic hints about unfamiliar objects, we were able to provide tentative answers to all three of these questions.

In the two experiments subsequently reported, participants were presented with real-word objects that were presumed to be unfamiliar to most of them. They first viewed each of these objects without any semantic information. This first part served as a naive baseline to rule out that ERPs would differ in response to the objects based solely on low-level visual differences. Next, participants viewed each unfamiliar object for a second time, now preceded by verbal keywords. These keywords could either be matching the typical function of the object, thus making it possible for participants to understand what kind of object they were viewing, or they could be non-matching by describing the function of a different object, thus keeping the perception of the object semantically naive. During this second part, we were able measure the online influence of semantic insight on object-evoked ERPs as it happened. Finally, the objects were presented for a third time (without keywords, as in the first part) to investigate downstream effects of having acquired semantic knowledge about them---thereby mimicking previous studies [e.g., @abdelrahman2008; @samaha2018; @weller2019]. In all three parts, we examined the influence of semantic information on ERPs associated with lower-level visual perception (P1 component, 100--150 ms), higher-level visual perception (N170 component, 150--200 ms), and semantic processing (N400 component, 400--700 ms).

# Experiment 1

```{r exp1-preparation}
# Define function for reading and formatting the behavioral data
read_behav <- function(txt_fname) {
  # Read the log file
  dat <- suppressWarnings(read_tsv(txt_fname, col_types = cols()))
  # Rename columns for participant and item IDs
  dat %<>% rename(participant = VPNummer, item = StimID) %>%
    # Add a column for the part of the experiment
    mutate(part = case_when(
      Wdh %in% c(211, 231) ~ "I",
      Wdh %in% c(212, 232) ~ "II",
      Wdh %in% c(213, 233) ~ "III",
      Wdh == 234 ~ "IV"
    ) %>% as_factor()) %>%
    # Remove filler stimuli (i.e.,well-known objects)
    filter(bek_unbek != "bekannt")
  # Assign stimuli to conditions based on keywords and button presses
  excl_known <- dat %>% filter(part == "I" & Tastencode %in% c(201, 251)) %>% pull(item)
  excl_informed <- dat %>% filter(part == "II" & grepl("richtig", Bed) & !Tastencode %in% c(201, 202, 251, 252)) %>% pull(item)
  excl_naive <- dat %>% filter(part == "II" & grepl("falsch", Bed) & !Tastencode %in% c(203, 204, 253, 254)) %>% pull(item)
  informed <- dat %>% filter(part == "II" & grepl("richtig", Bed) & Tastencode %in% c(201, 202, 251, 252)) %>% pull(item)
  naive <- dat %>% filter(part == "II" & grepl("falsch", Bed) & Tastencode %in% c(203, 204, 253, 254)) %>% pull(item)
  # Create a new condition for the experimental condition
  dat %<>% mutate(condition = case_when(
    item %in% excl_known ~ "Excl_known",
    item %in% excl_informed ~ "Excl_informed",
    item %in% excl_naive ~ "Excl_naive",
    item %in% informed ~ "Informed",
    item %in% naive ~ "Naive"
  ) %>% factor(levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")))
  # In Experiment 3, we need an additional column for position (upright vs. inverted)
  if ("richtig_inv" %in% dat$Bed) {
    dat %<>% mutate(position = as_factor(ifelse(grepl("inv", Bed), "Inverted", "Upright"))) %>%
      # Return only relevant columns
      select(part, position, condition, participant, item, RT)
  } else {
    # Return only relevant columns
    dat %<>% select(part, condition, participant, item, RT)
  }
  return(dat)
}

# Read behavioral data for Experiment 1
data_exp1 <- list.files("data/exp1/RT", pattern = ".txt", full.names = TRUE) %>% map(read_behav)

# Check average number of stimuli per condition
(stims_exp1 <- map(data_exp1, function(x) {table(x$condition) / 3}) %>% bind_rows() %>% colMeans())
```

```{python exp1-preprocessing}
# Import libraries
import mne
import glob
import os

montage = mne.channels.make_standard_montage("easycap-M1")
montage._get_ch_pos()

# Set parameters for EEG preprocessing
preproc_params = dict(
    n_components=15, method="fastica",         # ICA parameters
    l_freq=0.1, h_freq=30,                     # Filter edges
    event_id={"match": 221, "mismatch": 222},  # EEG triggers
    tmin=-0.5, tmax=1.498,                     # Length of epochs
    baseline=(-0.2, 0),                        # Baseline correction
    reject=dict(eeg=200)                       # Rejection threshold
)

# # Debug
# locals().update(preproc_params)
# vhdr_fname = 'data/exp1/EEG/Vp0001.vhdr'

# Define function for preprocessing EEG data
def preproc(vhdr_fname, metadata, n_components, method, l_freq, h_freq, event_id, tmin, tmax, baseline, reject):
    # Read EEG data in microvolts
    raw = mne.io.read_raw_brainvision(vhdr_fname, scale=1e6, preload=True)
    # Create virtual EOG channels
    raw = mne.set_bipolar_reference(raw, "Auge_u", "Fp1", ch_name="VEOG", drop_refs=False)
    raw = mne.set_bipolar_reference(raw, "F9", "F10", ch_name="HEOG", drop_refs=False)
    raw.set_channel_types(mapping={"VEOG": "eog", "HEOG": "eog"})
    # Add EasyCap electrode layout, removing any excessive channels
    montage = mne.channels.make_standard_montage("easycap-M1")
    raw.drop_channels(list(set(raw.ch_names) - set(montage.ch_names) - set(["VEOG", "HEOG"])))
    raw.set_montage(montage=montage)
    # # Interpolate bad channels
    # if vhdr_fname=='data/exp3/EEG/VP05.vhdr':
    #   raw.info['bads'].extend(['AF8', 'TP9'])
    #   raw = raw.interpolate_bads()
    # Re-reference to common average
    raw, _ = mne.set_eeg_reference(raw, "average")
    # Run ICA on a copy of the data
    filt_raw = raw.copy()
    filt_raw.load_data().filter(l_freq=1, h_freq=None)
    ica = mne.preprocessing.ICA(n_components=n_components, random_state=12345, method=method)
    ica.fit(filt_raw)
    # Remove bad components based on correlations with EOG
    eog_indices, eog_scores = ica.find_bads_eog(raw)
    ica.exclude = eog_indices
    raw = ica.apply(raw)
    # Apply band-pass filter
    raw = raw.filter(l_freq=l_freq, h_freq=h_freq)
    # Epoching including baseline correction
    events, _ = mne.events_from_annotations(raw, verbose=False)
    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=baseline, preload=True)
    # Add behavioral data
    epochs.metadata = metadata
    # Reject bad epochs
    epochs = epochs.drop_bad(reject=reject)
    return epochs


# List raw EEG filenames
fnames_exp1 = sorted(glob.glob("data/exp1/EEG/*.vhdr"))

# Import behavioral data from R
metadata_exp1 = r.data_exp1

# Check if preprocessing was done already (delete the file to re-run)
if os.path.exists("analysis/export/exp1-epo.fif"):
    # Read preprocessed data from file
    epochs_exp1 = mne.read_epochs("analysis/export/exp1-epo.fif", preload=True)
else:
    # Preprocess EEG data for Experiment 1
    epochs_exp1 = [
        preproc(vhdr_fname=fname, metadata=meta, **preproc_params) for fname, meta in zip(fnames_exp1, metadata_exp1)
    ]
    # Combine epochs into a single data set
    epochs_exp1 = mne.concatenate_epochs(epochs_exp1)
    # Backup epochs to the export folder
    epochs_exp1.save("analysis/export/exp1-epo.fif")


# Define function to compute grand averages per condition
def compute_evokeds(epochs):
    evokeds = dict(); evokeds_dat = dict()
    # Each part becomes a dictionary of conditions
    for pt in ["I", "II", "III"]:
        evokeds[pt] = dict(); evokeds_dat[pt] = dict()
        # Each condition becomes a list of participants
        for cn in ["Informed", "Naive"]:
            evokeds[pt][cn] = list(); evokeds_dat[pt][cn] = list()
            # For every participant we average trials separately for all parts and conditions
            for vp in epochs.metadata["participant"].unique():
                query = 'participant == "' + vp + '" and part == "' + pt + '" and condition == "' + cn + '"'
                evokeds[pt][cn].append(epochs[query].average())
            # Compute grand averages across participants for this part and condition
            evokeds[pt][cn] = mne.grand_average(evokeds[pt][cn])
            # Export only the actual data for R
            evokeds_dat[pt][cn] = evokeds[pt][cn].data
    return (evokeds, evokeds_dat)


# Compute grand averages for Experiment 1
evokeds_exp1, evokeds_dat_exp1 = compute_evokeds(epochs=epochs_exp1)
```

```{python}
# Define function to compute grand averages per condition
def compute_evokeds(epochs):
    evokeds = dict(); evokeds_dat = dict()
    # Each part becomes a dictionary of conditions
    for pt in ["I", "II", "III"]:
        evokeds[pt] = dict(); evokeds_dat[pt] = dict()
        # Each part becomes a dictionary of conditions
        for pos in ["Inverted", "Upright"]:
            evokeds[pt][pos] = dict(); evokeds_dat[pt][pos] = dict()
            # Each condition becomes a list of participants
            for cn in ["Informed", "Naive"]:
                evokeds[pt][pos][cn] = list(); evokeds_dat[pt][pos][cn] = list()
                # For every participant we average trials separately for all parts and conditions
                for vp in epochs.metadata["participant"].unique():
                    query = 'participant == "' + vp + '" and part == "' + pt + '" and position == "' + pos + '" and condition == "' + cn + '"'
                    evokeds[pt][pos][cn].append(epochs[query].average())
                # Compute grand averages across participants for this part and condition
                evokeds[pt][pos][cn] = mne.grand_average(evokeds[pt][pos][cn])
                # Export only the actual data for R
                evokeds_dat[pt][pos][cn] = evokeds[pt][pos][cn].data
    return (evokeds, evokeds_dat)


# Compute grand averages for Experiment 1
evokeds_exp3, evokeds_dat_exp3 = compute_evokeds_3(epochs=epochs_exp3)
```

```{r}
evokeds_exp3 <- py$evokeds_exp3_anova

compute_erps <- function(evokeds, els, name, start, stop, roi) {
  map(evokeds, function(part){
    map(part, function(position) {
      map(position, function(condition) {
        map_dbl(condition, function(participant) {
          participant[which(els %in% roi), which(times %in% start:stop)] %>% mean()
        })
      }) %>% 
        bind_rows(.id = "condition") %>%
        mutate(participant = factor(1:24)) %>%
        pivot_longer(cols = c("Informed", "Naive"), names_to = "condition", values_to = "amplitude")
    }) %>% bind_rows(.id = "position")
  }) %>% bind_rows(.id = "part")
}
```

```{python exp1-tfr, eval=FALSE}
# TFR
import numpy as np
import matplotlib.pyplot as plt

# chans = mne.pick_channels(epochs_exp1.ch_names, include=["PO3", "PO4", "POz", "O1", "O2", "Oz", "P7", "P8", "PO7", "PO8", "PO9", "PO10"])
# 
# epochs = mne.channels.combine_channels(epochs_exp1, dict(roi=chans))
# epochs.metadata = epochs_exp1.metadata

# epochs.plot_psd(fmin=2., fmax=40., average=True, spatial_colors=False); plt.show()
# epochs.plot_psd_topomap(normalize=True); plt.show()

freqs = np.logspace(*np.log10([4, 30]), num=8)
n_cycles = freqs / 2.  # different number of cycle per frequency

epochs_informed = epochs_exp1["part == 'II' & condition == 'Informed'"]
epochs_naive = epochs_exp1["part == 'II' & condition == 'Naive'"]

power_informed = mne.time_frequency.tfr_morlet(epochs_informed, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True)
power_naive = mne.time_frequency.tfr_morlet(epochs_naive, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True)

freqs = np.arange(start=4, stop=30, step=1)
n_cycles = freqs / 2.  # different number of cycle per frequency

evokeds_informed = evokeds_exp1["I"]["Informed"]
evokeds_naive = evokeds_exp1["I"]["Naive"]

power_informed = mne.time_frequency.tfr_morlet(evokeds_informed, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True)
power_naive = mne.time_frequency.tfr_morlet(evokeds_naive, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True)

power_diff = power_informed - power_naive
power_diff.plot(baseline=(-0.3, 0), mode = "percent", tmin=-0.3, tmax = 0.7, vmin=-200, vmax=200); plt.show()

power_diff.plot_joint(baseline=(-0.5, 0), mode = "percent", vmin=-100, vmax=100, timefreqs=(0.175, 12)); plt.show()

#power_diff.plot_topomap(fmin=8, fmax=14, baseline=(-0.5, 0), mode="mean", vmin=-200, vmax=200, title="alpha"); plt.show()
#power_diff.plot_topomap(fmin=15, fmax=30, baseline=(-0.5, 0), mode="mean", vmin=-200, vmax=200, title="beta"); plt.show()
```

```{r exp1-analysis}
# Re-import behavioral data and re-factorize some columns
data_exp1 <- py$epochs_exp1$metadata %>% mutate(
  part = factor(part, levels = c("I", "II", "III")),
  condition = factor(condition, levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")),
  participant = factor(participant),
  item = factor(item)
)

# Check number of rejected epochs
rejected_exp1 <- data_exp1 %>% group_by(participant) %>% tally() %>% mutate(n = 360 - n) %>% pull(n)
mean(rejected_exp1)    # Mean
median(rejected_exp1)  # Median
range(rejected_exp1)   # Range

# Create vectors with sample time points (in ms) and electrodes
times <- seq(py$preproc_params$tmin * 1000, py$preproc_params$tmax * 1000, 1 / py$epochs_exp1$info["sfreq"] * 1000)
els <- py$epochs_exp1$ch_names

# Define ERP components of interest
comps <- tibble(
  name = c("P1", "N170", "N400"),
  start = c(100, 150, 400),
  stop = c(150, 200, 700),
  roi = list(
    c("PO3", "PO4", "POz", "O1", "O2", "Oz"),
    c("P7", "P8", "PO7", "PO8", "PO9", "PO10"),
    c("C1", "C2", "Cz", "CP1", "CP2", "CPz")
  )
)

# Define function to compute single-trial mean ERP amplitudes
compute_erps <- function(epochs, els, name, start, stop, roi) {
  erps <- epochs[, which(els %in% roi), which(times %in% start:stop)]
  amps <- apply(erps, 1, mean, na.rm = TRUE)
  return(amps)
}

# # Define function to compute single-trial ERP latencies
# compute_lats <- function(epochs, els, name, start, stop, roi) {
#   erps <- epochs[, which(els %in% roi), which(times %in% start:stop)]
#   samples <- apply(erps, MARGIN = c(1, 3), mean) %>% abs()
#   areas <- apply(samples, MARGIN = 1, sum)
#   cumss <- apply(samples, MARGIN = 1, cumsum) %>% asplit(MARGIN = 2)
#   lats <- map2_int(cumss, areas, function(cums, area){which(cums < area / 2) %>% max()})
#   lats <- start + lats * 2
#   return(lats)
# }

# Check if single-trial ERPs were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp1-erps.RDS")) {
  # Read data from file
  data_exp1 <- readRDS("analysis/export/exp1-erps.RDS")
} else {
  # Compute single-trial ERPs for Experiment 1
  data_exp1 <- pmap_dfc(comps, compute_erps, epochs = py$epochs_exp1$get_data(), els = els) %>%
    set_names(comps$name) %>%
    cbind(data_exp1, .) %T>%
    saveRDS("analysis/export/exp1-erps.RDS")
}

# Remove the epochs to free up memory
py_run_string("del epochs_exp1")
import("gc")$collect()

# Remove any trials excluded from conditions
data_exp1 %<>% filter(condition %in% c("Informed", "Naive")) %>% droplevels()

# Contrast coding for condition (informed-naive) and part (2-1, 3-1)
contrasts_condition <- t(cbind(c("Informed" = 1, "Naive" = -1)))
contrasts_part <- t(cbind(c("I" = -1, "II" = 1, "III" = 0), c("I" = 0, "II" = -1, "III" = 1)))
contrasts(data_exp1$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_exp1$part) <- MASS::ginv(contrasts_part)

# Set formula and parameters for linear mixed-effects regression models (LMMs)
form_exp12 <- buildmer::tabulate.formula(
  ~ part * condition + (part * condition | participant) + (part * condition | item)
) %>% mutate(block = replace(block, is.na(grouping), "fixed"))
ctrl_params <- lme4::lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e5))

# Set formula for follow-up contrasts
specs_exp12 <- pairwise ~ condition | part

# Define function to compute LMMs and follow-up contrasts for each component
compute_models <- function(dep, formula, data, control, specs) {
  require(lmerTest)
  require(buildmer)
  require(emmeans)
  build <- buildmer(
    control = control,
    REML = FALSE,
    buildmerControl = buildmerControl(
      formula = formula,
      data = data,
      direction = c("order", "backward"),
      elim = function(logp) exp(logp) >= .20,
      calc.anova = TRUE,
      ddf = "Satterthwaite",
      dep = dep
    )
  )
  emm_options(lmer.df = "Satterthwaite", lmerTest.limit = Inf)
  means <- emmeans(build@model, specs = specs, infer = TRUE, data = data)$emmeans %>% as.data.frame()
  conts <- emmeans(build@model, specs = specs, infer = TRUE, data = data)$contrasts %>% as.data.frame()
  mod <- list(model = build@model, anova = build@anova, summary = build@summary, means = means, contrasts = conts)
  return(mod)
}

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp1-stats.RDS")) {
  # Read models from file
  models_exp1 <- readRDS("analysis/export/exp1-stats.RDS")
} else {
  # Compute LMMs for Experiment 1
  models_exp1 <- map(
    comps$name,
    compute_models,
    formula = form_exp12,
    data = data_exp1,
    control = ctrl_params,
    specs = specs_exp12
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/exp1-stats.RDS")
}
```

## Method

### Participants

Participants for Experiment 1 were 24 German native speakers (13 female, 11 male) with a mean age of 24 years (range 18 to 31) and no history of psychological disorder or treatment. No a priori power analysis was carried out and the sample size was chosen in line with other EEG studies in our lab at the time of data collection. All participants were right-handed according to the Edinburgh inventory [@oldfield1971] and reported normal or corrected-to-normal vision. They gave written informed consent before starting the experiment and received a compensation of €8 per hour for participating.

### Materials

Stimuli for Experiments 1 and 2 consisted of 240 grayscale photographs of real-world objects, 120 of which were well-known everyday objects (e.g., a bicycle, a toothbrush), serving as filler stimuli of no interest, whereas the other 120 were rare objects presumed to be unfamiliar to the majority of participants (e.g., a galvanometer, an udu drum). A list of these unfamiliar objects can be found in the appendix. All stimuli were presented on a light blue background with a size of 207 × 207 pixels on a 19-inch LCD monitor with a resolution of 1,280 × 1,024 pixels and a refresh rate of 75 Hz. At a standardized viewing distance of 90 cm, the images of the objects subtended approximately 3.9 degrees of participants' horizontal and vertical visual angle.

For each unfamiliar object, a pair of keywords---a noun and a verb---was selected, describing the object's typical function or use in a way that could typically be related to its visual features and their configuration (e.g., current--measuring, pottery--drumming). As our central experimental manipulation, the presentation of each unfamiliar object was preceded by its correctly matching keywords for half of the objects, whereas the other half were preceded by non-matching keywords belonging to one of the other objects. The matching keywords were expected to induce semantically informed perception (i.e., participants suddenly understanding what kind of object they were viewing), whereas the non-matching keywords were expected hamper such an understanding and keep the perception of the object semantically naive. All participants saw each unfamiliar object with only one type of keywords (matching or non-matching). This assignment of keywords to objects was counterbalanced across participants so that each object would be presented with matching keywords (leading to semantically informed perception) and non-matching keywords (leading to naive perception) to an equal number of participants. The experiment was programmed and displayed using Presentation® software (Neurobehavioral Systems, Inc., Berkeley, CA, www.neurobs.com).

### Procedure

Each experimental session consisted of three parts (see Figure \@ref(fig:exp1-plot)A). In the *pre-insight* part, after written informed consent had been obtained and the EEG had been prepared, all 240 familiar and unfamiliar objects were presented once in random order and without any keywords. Each trial consisted of a fixation cross presented in the middle of the screen for 0.5 s, followed by the presentation of the object until participants made a response or until a time out after 3 s. The inter-trial interval until the presentation of the next fixation cross was 0.5 s and participants took a self-timed break after each block of 60 objects. The task, which was kept the same across all three parts, was to classify each object using one of four response alternatives: (a) "I know what this is or have a strong assumption," (b) "I have an assumption what this is," (c) "I have rather no assumption what this is," or (d) "I don't know what this is and have no assumption." Participants were asked to respond as quickly and as accurately as possible by pressing one out of four buttons with the index or middle finger of their left or right hand, respectively. The mapping of the rating scale to the four buttons (left to right or right to left) was counterbalanced across participants.

(ref:figure-1-caption) Procedure and Results of Experiment 1\smallskip

<!--

The papaja package does not yet deal with the placement of figure captions above rather than below the figures. Therefore, we manually need to post-process the .tex file, moving the caption{} above includegraphics{} and copying the figure note manually below includegraphics{}:

\bigskip\small\emph{Note.} 
(A) In the pre-insight part, participants were presented with 120 unfamiliar objects and indicated whether they knew what kind of object they were viewing. In the insight part, half of these objects were presented with matching keywords (in purple color for illustration), leading to semantically informed perception, and the other half with non-matching keywords (in petrol color for illustration), leading to naive perception. In the post-insight part, the same objects were presented again without the keywords. (B) ERP waveforms and scalp topographies are shown for objects with semantically informed versus naive perception within the three different parts. Semantically informed perception was associated with significantly more negative amplitudes in the N170 component in in the insight part, significantly less negative amplitudes in the N400 component in the insight and post-insight parts, and significantly more positive amplitudes in the P1 component in the post-insight part. Ampl. = amplitude.\newline
*\emph{p} \textless{} .05. **\emph{p} \textless{} .01. ***\emph{p} \textless{} .001.

-->

```{r exp1-plot, include=TRUE, fig.height=11, fig.cap = "(ref:figure-1-caption)"}
# Locate standard montage file from MNE Python
fname_montage <- list.files(
  path = reticulate::py_discover_config()$python %>% str_remove("bin/.*"),
  pattern = "easycap-M1.txt", full.names = TRUE, recursive = TRUE
)[1]

# Retrieve locations for the relevant electrodes from this montage
montage <- py$evokeds_exp1$I$Informed$ch_names %>%
  as_tibble() %>%
  rename(electrode = value) %>%
  left_join(
    read_tsv(fname_montage, col_types = cols()) %>%
      rename(electrode = Site),
    by = "electrode"
  ) %>%
  mutate(
    x = eegUtils:::deg2rad(Theta) * cos(eegUtils:::deg2rad(Phi)),
    y = eegUtils:::deg2rad(Theta) * sin(eegUtils:::deg2rad(Phi))
  )

# Trial structure
plot_trials <- function(){
  require(magick)
  color_informed <- viridisLite::viridis(1, begin = 0.1)
  color_naive <- viridisLite::viridis(1, begin = 0.5)
  ggplot() +
    coord_cartesian(xlim = c(-100, 100), ylim = c(0, 100), expand = FALSE) +
    ## PART II ##
    # Title
    annotate("text", x = 0, y = 97.2, label = "Insight part", size = 14 / .pt, family = "Helvetica", fontface = "bold") +
    # Arrow
    annotate("segment", x = -30, xend = 24, y = 79, yend = 85, arrow = arrow(length = unit(0.1, "inches"))) +
    # Squares
    annotate("rect", xmin = -37, xmax = -17, ymin = 55, ymax = 75, color = "black", fill = "white") +
    annotate("rect", xmin = -19, xmax = 1, ymin = 45, ymax = 65, color = color_informed, fill = "white") +
    annotate("rect", xmin = -19, xmax = 1, ymin = 69, ymax = 89, color = color_naive, fill = "white") +
    annotate("rect", xmin = -1, xmax = 19, ymin = 59, ymax = 79, color = "black", fill = "white") +
    annotate("rect", xmin = 17, xmax = 37, ymin = 61, ymax = 81, color = "black", fill = "white") +
    annotate("text", x = -27, y = 65.3, label = "+", size = 20 / .pt, family = "Helvetica") +
    annotate("text", x = -9, y = 55, label = "Potato\nmashing", size = 10 / .pt, family = "Helvetica", color = color_informed) +
    annotate("text", x = -9, y = 79, label = "Message\nmorsing", size = 10 / .pt, family = "Helvetica", color = color_naive) +
    annotate("text", x = 9, y = 67, label = "*", size = 30 / .pt, family = "Helvetica") +
    draw_image("analysis/manuscript_files/potato_masher.png", x = 18, y = 62, width = 18, height = 18) +
    # Timings
    annotate("text", x = -9, y = 67.3, label = "or", size = 10 / .pt, family = "Helvetica") +
    annotate("text", x = -27, y = 52, label = "0.5 s", size = 10 / .pt, family = "Helvetica") +
    annotate("text", x = -9, y = 42, label = "2.5 s", size = 10 / .pt, family = "Helvetica") +
    annotate("text", x = 9, y = 56, label = "0.5 s", size = 10 / .pt, family = "Helvetica") +
    annotate("text", x = 27, y = 56.3, label = "3 s or\nresponse", size = 10 / .pt, family = "Helvetica", lineheight = 1) +
    # Response options
    annotate("segment", x = 27, y = 39, xend = 27, yend = 51.5) +
    annotate("segment", x = 0, y = 39, xend = 27, yend = 39) +
    annotate("segment", x = 0, y = 35, xend = 0, yend = 39) +
    annotate("segment", x = -32, y = 35, xend = 32, yend = 35) +
    annotate("segment", x = -32, y = 30.5, xend = -32, yend = 35) +
    annotate("segment", x = -12, y = 32, xend = -12, yend = 35) +
    annotate("segment", x = 12, y = 32, xend = 12, yend = 35) +
    annotate("segment", x = 32, y = 32, xend = 32, yend = 35) +
    annotate("text", x = -32, y = 26, label = "A. \"I know\nwhat this is\"", size = 10 / .pt, family = "Helvetica", lineheight = 1) +
    annotate("text", x = -12, y = 26, label = "B. \"I have an\nassumption\nwhat this is\"", size = 10 / .pt, family = "Helvetica", lineheight = 1) +
    annotate("text", x = 12, y = 26, label = "C. \"I have rather\nno assumption\nwhat this is\"", size = 10 / .pt, family = "Helvetica", lineheight = 1) +
    annotate("text", x = 32, y = 26, label = "D. \"I don\'t\n know what\nthis is\"", size = 10 / .pt, family = "Helvetica", lineheight = 1) +
    # Conditions
    annotate("segment", x = -32, y = 17, xend = -32, yend = 22, color = color_informed) +
    annotate("segment", x = -12, y = 17, xend = -12, yend = 20, color = color_informed) +
    annotate("segment", x = 12, y = 17, xend = 12, yend = 20, color = color_naive) +
    annotate("segment", x = 32, y = 17, xend = 32, yend = 20, color = color_naive) +
    annotate("segment", x = -32, y = 17, xend = -12, yend = 17, color = color_informed) +
    annotate("segment", x = 12, y = 17, xend = 32, yend = 17, color = color_naive) +
    annotate("segment", x = -22, y = 15, xend = -22, yend = 17, color = color_informed) +
    annotate("segment", x = 22, y = 15, xend = 22, yend = 17, color = color_naive) +
    annotate("text", x = -22, y = 13, label = "Informed condition", size = 10 / .pt, family = "Helvetica", fontface = "bold", color = color_informed) +
    annotate("text", x = 22, y = 13, label = "Naive condition", size = 10 / .pt, family = "Helvetica", fontface = "bold", color = color_naive) +
    annotate("text", x = -22, y = 7.9, label = "Matching keywords\nand response A or B", size = 10 / .pt, family = "Helvetica", color = color_informed, lineheight = 1) +
    annotate("text", x = 22, y = 7.9, label = "Non-matching keywords\nand response C or D", size = 10 / .pt, family = "Helvetica", color = color_naive, lineheight = 1) +
    ## PART I ##
    # Title
    annotate("text", x = -72, y = 56, label = "Pre-insight part", size = 14 / .pt, family = "Helvetica", fontface = "bold") +
    # Arrow
    annotate("segment", x = -84, xend = -66, y = 46, yend = 48, arrow = arrow(length = unit(0.1, "inches"))) +
    # Squares
    annotate("rect", xmin = -91, xmax = -71, ymin = 22, ymax = 42, color = "black", fill = "white") +
    annotate("rect", xmin = -53, xmax = -73, ymin = 24, ymax = 44, color = "black", fill = "white") +
    annotate("text", x = -81, y = 32.3, label = "+", size = 20 / .pt, family = "Helvetica") +
    draw_image("analysis/manuscript_files/potato_masher.png", x = -72, y = 25, width = 18, height = 18) +
    # Timings
    annotate("text", x = -81, y = 19, label = "0.5 s", size = 10 / .pt, family = "Helvetica") +
    annotate("text", x = -63, y = 19.3, label = "3 s or\nresponse", size = 10 / .pt, family = "Helvetica", lineheight = 1) +
    # Conditions
    annotate("text", x = -72, y = 7.9, label = "Objects classified according\nto conditions in the insight part", size = 10 / .pt, family = "Helvetica", lineheight = 1) +
    # PART III
    # Title
    annotate("text", x = 72, y = 56, label = "Post-insight part", size = 14 / .pt, family = "Helvetica", fontface = "bold") +
    # Arrow
    annotate("segment", x = 60, xend = 78, y = 46, yend = 48, arrow = arrow(length = unit(0.1, "inches"))) +
    # Squares
    annotate("rect", xmin = 53, xmax = 73, ymin = 22, ymax = 42, color = "black", fill = "white") +
    annotate("rect", xmin = 71, xmax = 91, ymin = 24, ymax = 44, color = "black", fill = "white") +
    annotate("text", x = 63, y = 32.3, label = "+", size = 20 / .pt, family = "Helvetica") +
    draw_image("analysis/manuscript_files/potato_masher.png", x = 72, y = 25, width = 18, height = 18) +
    # Timings
    annotate("text", x = 63, y = 19, label = "0.5 s", size = 10 / .pt, family = "Helvetica") +
    annotate("text", x = 81, y = 19.3, label = "3 s or\nresponse", size = 10 / .pt, family = "Helvetica", lineheight = 1) +    
    # Conditions
    annotate("text", x = 72, y = 7.9, label = "Objects classified according\nto conditions in the insight part", size = 10 / .pt, family = "Helvetica", lineheight = 1) +    
    theme_void()
}

# Define function to plot ERP waveforms and topographies
plot_erps <- function(comps, evokeds, models, montage) {
  suppressWarnings(suppressMessages(
    pmap(comps, function(name, start, stop, roi) {
      # N400 gets a different scale than P1 and N170
      ymin <- ifelse(name == "N400", -3, -4)
      # Loop through parts
      parts <- map(c("I", "II", "III"), function(part) {
        # Create (conditions x time points) x electrodes tibble
        data_plot <- evokeds[[part]] %>%
          map(function(data) {
            data %>%
              t() %>%
              as_tibble(.name_repair = "unique") %>%
              set_colnames(montage$electrode) %>%
              mutate(.time = times, roi = rowMeans(select(., all_of(roi))))
          }) %>% bind_rows(.id = "condition")
        # Shade background depending on whether the effect is significant or not
        asterisks <- models[[name]]$contrasts %>%
          filter(part == !!part) %>%
          mutate(asterisks = case_when(p.value < .001 ~ "***", p.value < .01 ~ "**", p.value < .05 ~ "*")) %>%
          pull(asterisks)
        xasterisks <- case_when(str_length(asterisks) == 1 ~ start + 11, str_length(asterisks) == 3 ~ start + 15)
        if (is.na(asterisks)) {
          shade <- annotate("rect", xmin = start, xmax = stop, ymin = ymin + 0.1, ymax = ymin + 11.95, color = "black", fill = NA)
        } else {
          shade <- annotate("rect", xmin = start, xmax = stop, ymin = ymin + 0.1, ymax = ymin + 11.95, color = "black", fill = "gray90")
        }

        # # Prepare waveform
        # wave <- ggplot(data = data_plot, aes(x = .time, y = roi, color = condition)) +
        #   annotate("rect", xmin = start, xmax = stop, ymin = -Inf, ymax = ymin + 12, fill = "gray90")
        # # Shade significant area
        # star <- stars[[name]][[part]]
        # if (!is.na(star)) {
        #   wave <- wave + geom_ribbon(
        #     data = data_plot %>% filter(.time >= start & .time <= stop) %>%
        #       group_by(.time) %>% summarize(ymin = min(roi), ymax = max(roi)),
        #     aes(x = .time, xmin = start, xmax = stop, ymin = ymin, ymax = ymax),
        #     inherit.aes = FALSE, fill = viridisLite::viridis(n = 1, begin = 1)
        #   )
        # } +
        # annotate("text", label = star, x = start + (stop - start)/2, y = -3.2, size = 7, family = "Helvetica")}
        # Add waves and styling

        # Plot waveform
        wave <- ggplot(data = data_plot, aes(x = .time, y = roi, color = condition)) +
          shade +
          annotate("text", label = asterisks, x = xasterisks, y = ymin + 11, size = 6, hjust = 0, family = "Helvetica") +
          annotate("segment", x = -200, xend = 800, y = 0, yend = 0) +
          annotate("segment", x = 0, xend = 0, y = ymin, yend = ymin + 12) +
          annotate("segment", x = seq(-100, 700, 200), xend = seq(-100, 700, 200), y = -0.3, yend = 0) +
          annotate("segment", x = -12, xend = 0, y = seq(-2, 8, 4), yend = seq(-2, 8, 4)) +
          annotate("text", x = seq(-100, 700, 200), y = -0.9, label = seq(-100, 700, 200), size = 10 / .pt, family = "Helvetica") +
          annotate("text", x = -20, y = seq(-2, 8, 4), label = seq(-2, 8, 4), size = 10 / .pt, family = "Helvetica", hjust = 1) +
          annotate("text", x = 400, y = ymin + 0.8, label = "Time (ms)", size = 10 / .pt, family = "Helvetica", lineheight = 0.9) +
          annotate("text", x = -100, y = 4, label = paste(name, "ampl.\n(µV)"), size = 10 / .pt, family = "Helvetica", angle = 90, lineheight = 0.9) +
          geom_line() +
          scale_color_viridis_d(begin = 0.1, end = 0.5) +
          coord_cartesian(xlim = c(-200, 800), ylim = c(ymin, ymin + 14), expand = FALSE) +
          theme_void() +
          theme(legend.position = "none")
        # Plot topography
        topo <- data_plot %>%
          filter(.time >= start & .time <= stop) %>%
          group_by(condition) %>%
          summarise(across(montage$electrode, mean)) %>%
          pivot_longer(-condition) %>%
          pivot_wider(names_from = condition) %>%
          transmute(amplitude = Informed - Naive) %>%
          bind_cols(montage) %>%
          eegUtils::topoplot(limits = c(-1, 1), palette = "viridis", contour = FALSE, highlights = roi, scaling = 0.1) +
          theme(legend.position = "none")
        topo$layers[[3]]$aes_params$size <- topo$layers[[4]]$aes_params$size <- topo$layers[[5]]$aes_params$size <- 0.6
        # Combine waveform and topography
        wave + draw_plot(topo, width = 320, height = 9, x = 500, y = ymin + 6.4)
      })
      # Combine plots for the different parts
      plot_grid(parts[[1]], NULL, parts[[2]], NULL, parts[[3]], nrow = 1, rel_widths = c(10, 1, 10, 1, 10))
      # Combine plots for the different components
    }) %>% plot_grid(plotlist = ., nrow = 3)
  ))
}

# Create a combined legend and colorbar
plot_legends_erps <- function(direction) {
  if (direction == "vertical") {
    mar <- margin(l = 12, r = 10, b = 6, t = 7); vjust <- 3.5
  }
  else {
    mar <- margin(l = 8, r = 12, b = 4, t = 6); vjust <- 0.7
  }
  get_legend(
    tibble(amplitude = c(-1, 1), condition = c("Informed", "Naive")) %>%
      ggplot(aes(x = 0, y = 0, color = condition, fill = amplitude)) +
      geom_line() +
      geom_raster() +
      labs(color = "Conditions", fill = "Informed - naive (µV)") +
      scale_color_viridis_d(begin = 0.1, end = 0.5, guide = guide_legend(direction = "vertical", title.position = "top")) +
      scale_fill_viridis_c(guide = guide_colorbar(direction = "horizontal", ticks = FALSE, title.position = "top", title.vjust = vjust)) +
      theme(
        legend.direction = direction,
        legend.box = direction,
        legend.box.background = element_rect(colour = "black", size = 0.5),
        legend.box.margin = mar,
        legend.background = element_blank(),
        legend.title = element_text(family = "Helvetica", size = 10, face = "bold"),
        legend.text = element_text(family = "Helvetica", size = 10),
        legend.key = element_rect(fill = NA),
        legend.key.width = unit(0.7, "cm"),
        legend.key.height = unit(0.4, "cm"),
        legend.spacing.x = unit(0.3, "cm")
      )
  )
}

# Create plot for Experiment 1
plot_grid(
  plot_trials(),
  plot_erps(comps = comps, evokeds = py$evokeds_dat_exp1, models = models_exp1, montage = montage),
  nrow = 2, rel_heights = c(5, 6), labels = c("A", "B"), label_fontfamily = "Helvetica"
) + # Add some lines (to separate the parts) and the legend
  annotate(geom = "segment", x = 0.328, xend = 0.328, y = -Inf, yend = 0.53, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.672, y = -Inf, yend = 0.53, linetype = "dashed") +
  annotate(geom = "segment", x = 0.328, xend = 0.28, y = 0.53, yend = 0.58, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.72, y = 0.53, yend = 0.58, linetype = "dashed") +  
  annotate(geom = "segment", x = 0.28, xend = 0.28, y = 0.58, yend = Inf, linetype = "dashed") +
  annotate(geom = "segment", x = 0.72, xend = 0.72, y = 0.58, yend = Inf, linetype = "dashed") +
  draw_plot(plot_legends_erps(direction = "vertical"), x = 0.405, y = 0.422, width = 1, height = 1)
```

In the *insight* part, the 120 unfamiliar objects were presented for a second time, now preceded either by matching keywords (leading to semantically informed perception) or by non-matching keywords (leading to naive perception). Each trial consisted of a fixation cross presented for 0.5 s, followed by the presentation of the keywords for 2.5 s. Then, an asterisk was presented in the middle of the screen for another 0.5 s, followed by the presentation of the object until a response was made or until a time out after 3 s. The objects were presented in blocks of 30 trials so that within each block (a) there were 15 objects from each of the two experimental conditions and (b) objects were heterogeneous in terms of their shape, visual complexity, and functional category (e.g., medical devices, musical instruments).

Finally, in the *post-insight* part, the unfamiliar objects were presented for a third time with an identical trial structure as in the pre-insight part, that is, without any keywords. Note that the insight and post-insight parts were presented in an interleaved fashion so that after the presentation of one block of 30 objects in the insight part (with keywords), participants took a self-timed break and continued with the same block of 30 objects in the post-insight part (without keywords) before moving on to the next block consisting of 30 different objects. They continued like this until all four blocks were completed in both parts. In total, the experiment consisted of 480 trials (120 familiar objects in the pre-insight part and 120 unfamiliar objects in the pre-insight, insight, and post-insight parts). It took participants approximately 35 minutes to complete.

### EEG Recording and Preprocessing

The continuous EEG was recorded from 62 Ag/AgCl scalp electrodes placed according to the extended 10--20 system [@americanelectroencephalographicsociety1991] and referenced online to an external electrode placed on the left mastoid (M1). Two additional external electrodes were placed on the right mastoid (M2) and below the left eye (IO1), respectively. During the recording, electrode impedance was kept below 5 kΩ. An online band-pass filter with a high-pass time-constant of 10 s (0.016 Hz) and a low-pass cutoff frequency of 1000 Hz was applied before digitizing the signal at a sampling rate of 500 Hz.

Offline, the data were preprocessed using the MNE software [Version 0.21.0\; @gramfort2013] in Python [Version 3.8.5\; @vanrossum2009]. First, all scalp electrodes were re-referenced to the common average. Next, artifacts resulting from blinks and eye movements were removed using independent component analysis (ICA). The first 15 components were extracted by the FastICA algorithm [@hyvärinen1999] after temporarily low-pass filtering the data at 1 Hz. Any components showing substantive correlations with either of two virtual EOG channels (VEOG: IO1 minus Fp1, HEOG: F9 minus F10) were removed automatically using the *find\_bads\_eog* function. After artifact correction, a zero-phase, non-causal FIR filter with a lower pass-band edge at 0.1 Hz (transition bandwidth: 0.1 Hz) and an upper pass-band edge at 30 Hz (transition bandwidth: 7.5 Hz) was applied. Next, the continuous EEG was epoched into segments of 2,000 ms, starting 500 ms before the onset of the visual presentation of each unfamiliar object. The epochs were baseline-corrected by subtracting the average voltage during the 200 ms before stimulus onset. Epochs containing artifacts despite ICA, defined as peak-to-peak amplitudes exceeding 200 µV, were removed from further analysis. This led to the exclusion of an average of `r format(round(mean(rejected_exp1), 1), nsmall = 1)` trials per participant (= `r scales::percent(mean(rejected_exp1)/360, accuracy = 0.1)`; range `r min(rejected_exp1)` to `r max(rejected_exp1)` trials). Single-trial event-related potentials were computed as the mean amplitude across time windows and regions of interests (ROIs) defined a priori, namely 100--150 ms after object onset at electrodes PO3, PO4, POz, O1, O2, and Oz for the P1 component, 150--200 ms after object onset at electrodes P7, P8, PO7, PO8, PO9, and PO10 for the N170 component, and 400--700 ms after object onset at electrodes C1, C2, Cz, CP1, CP2, and CPz for the N400 component (see the scalp topographies in Figure \@ref(fig:exp1-plot)B).

### Statistical Analysis

First, because we were interested in the effects of knowledge on perceiving *unfamiliar* objects only, we excluded from all further analyses those objects which participants classified as being known to them in the pre-insight part (i.e., before any keywords were presented). This led to the exclusion of an average of `r format(round(stims_exp1["Excl_known"], 1), nsmall = 1)` objects per participant (= `r scales::percent(stims_exp1["Excl_known"]/120, accuracy = 0.1)` of all unfamiliar objects). Next, to clearly delineate semantically informed and naive perception, the assignment of all other objects to one of these two conditions for statistical analyses was co-determined by our experimental manipulation (matching versus non-matching keywords in the insight part) and the behavioral responses of the participants themselves (see Figure \@ref(fig:exp1-plot)A). Objects were assigned to the semantically informed condition only if they were presented with matching keywords *and* if participants indicated knowing what the object was or having an assumption. This was the case for an average of `r format(round(stims_exp1["Informed"], 1), nsmall = 1)` objects per participant (= `r scales::percent(stims_exp1["Informed"]/60, accuracy = 0.1)` of objects presented with matching keywords). Complementarily, objects were assigned to the naive condition only if they were presented with non-matching keywords *and* if participants indicated not knowing what the object was or having rather no assumption. This was the case for an average of `r format(round(stims_exp1["Naive"], 1), nsmall = 1)` objects per participant (= `r scales::percent(stims_exp1["Naive"]/60, accuracy = 0.1)` of objects presented with non-matching keywords). Although this assignment was based on the manipulation and responses in the second part---when insight was thought to occur---the same assignment was used to analyze the data from the other two parts. This allowed us to test, on the one hand, if the objects from both conditions differed in important aspects even before any keywords were presented (pre-insight part) and, on the other hand, if the semantic understanding acquired in the insight part had any lasting effects on a subsequent, third presentation of the objects (post-insight part).

The event-related potentials in response to objects from both conditions and all three parts were analyzed on the single trial level using linear mixed-effects regression models [@baayen2008; @frömer2018]. For the purpose of the present study, these models have at least two desirable properties compared to traditional approaches such as analyses of variance (ANOVAs) performed on by-participant grand averages. First, they can account not only for the non-independence of data points coming from the same participant, but also simultaneously for the non-independence of data points coming from the same item. In contrast, the neglect of the item as a random variable in ANOVAs leads to anti-conservative test statistics and strictly does not allow for inferences beyond the stimulus set under study [@bürki2018; @judd2012]. Second, mixed-effects models can flexibly deal with unbalanced designs in which the number of trials differs across (combinations of) conditions. Such a situation is inevitable in designs where the assignment of trials to conditions is co-determined by the experimental manipulation and the responses of the participants rather than by the experimental manipulation alone [e.g., @fröber2017].

Three separate models were computed predicting P1, N170, and N400 mean amplitudes, respectively. All models included three fixed effects: (a) the part of the experiment, coded as a repeated contrast (i.e., subtracting the first from the second part and the second from the third part, the intercept being the grand mean across all three parts), (b) the condition of the object, coded as a scaled sum contrast (i.e., subtracting the naive condition from the semantically informed condition, the intercept being the grand mean across both conditions), and (c) the two-way interaction of part and condition. For details on these and other contrast coding schemes in linear (mixed-effects) models, please refer to @schad2020. To determine the random effects structure, we always started with a maximal model containing by-participant and by-item random intercepts and random slopes for all fixed effects [@barr2013]. We then performed a model selection algorithm as proposed by @matuschek2017 in order to increase statistical power and avoid overparameterization: Iteratively, each random effect was removed and the resulting, more parsimonious model was compared to the previous, more complex model by means of a likelihood ratio test. Only if the parsimonious model explained the data equally well as the complex model [determined by *p* \> .20\; @matuschek2017] did we leave the random effect out, otherwise it was kept in the final model. All models were computed in R [Version `r as.character(packageVersion("base"))`\; @R-base] using the lme4 package [Version `r as.character(packageVersion("lme4"))`\; @R-lme4]. The optimizer function *bobyqa* with 20,000 iterations was used for maximum likelihood estimation. The model selection algorithm via likelihood ratio tests was performed using the buildmer package [Version `r as.character(packageVersion("buildmer"))`\; @R-buildmer]. Finally, to answer our research question of whether or not semantically informed perception had an influence on the ERP components within each part, planned follow-up comparisons were calculated, contrasting the informed against the naive condition within the pre-insight, insight, and post-insight parts. This was achieved using the emmeans package [Version `r as.character(packageVersion("emmeans"))`\; @R-emmeans]. All *p*-values were computed by approximating the relevant denominator degrees of freedom using Satterthwaite's method as implemented in the lmerTest package [Version `r as.character(packageVersion("lmerTest"))`\; @R-lmerTest].

The materials, single trial behavioral and ERP data, and all code for data analysis can be accessed via the Open Science Framework (<https://osf.io/uksbc/>). Unfortunately, lack of informed consent prevents us from sharing the raw EEG data.

## Results

Single-trial ERPs were analyzed in response to unfamiliar objects before (pre-insight part), while (insight part), and after (post-insight part) participants obtained relevant semantic information about their function. In the insight part, half of the objects were preceded by matching keywords, fostering semantically informed perception. The other half were preceded by non-matching keywords, keeping the perception of the object semantically naive. The objects were analyzed according to this manipulation in combination with participants' self-report in the insight part (see Figure \@ref(fig:exp1-plot)A), thereby making sure that semantically informed and naive perception did indeed occur. The analysis focused on differences between these two conditions in the P1 component (100--150 ms) as an index of lower-level visual perception, the N170 component (150--200 ms) as index of higher-level visual perception, and the N400 component (400--700 ms) as an index of semantic processing.

```{r exp1-table, include=TRUE, results="asis"}
# Define function to print ANOVA-style table
create_table <- function(models, stub_anova, stub_contrasts, caption, note) {
  anov <- map(models, function(model) {
    data.frame(
      "f" = format(round(model$anova$`F value`, 2), trim = FALSE, nsmall = 2),
      "df" = paste0("(", model$anova$NumDF, ", ", format(round(model$anova$DenDF, 1), trim = TRUE, nsmall = 1), ")"),
      "p" = format(round(model$anova$`Pr(>F)`, 3), trim = FALSE, nsmall = 3)
    ) %>% mutate(p = ifelse(p == "0.000", "< .001", substr(p, 2, nchar(p)))) %>% set_rownames(c(stub_anova))
  })
  anov_print <- anov %>%
    map(unite, col = fdf, f, df, sep = " ") %>%
    map(add_row, fdf = "\\textit{F} (\\textit{df})", p = "\\textit{p}", .before = 1) %>%
    bind_cols() %>%
    set_rownames(c("\\textit{Fixed effects}", stub_anova))
  anov %<>% 
    bind_cols() %>% 
    set_colnames(paste(rep(names(models), each = length(names(models))), c("f", "df", "p"), sep = "_"))
  conts <- map(models, function(model) {
    data.frame(
      "est" = format(round(model$contrasts$estimate, 2), trim = FALSE, nsmall = 2),
      "ci" = paste0(
        "[", format(round(model$contrasts$lower.CL, 2), trim = TRUE, nsmall = 2), ", ",
        format(round(model$contrasts$upper.CL, 2), trim = TRUE, nsmall = 2), "]"
      ),
      "p" = format(round(model$contrasts$`p.value`, 3), trim = FALSE, nsmall = 3)
    ) %>% mutate(p = ifelse(p == "0.000", "< .001", substr(p, 2, nchar(p)))) %>% set_rownames(stub_contrasts)
  })
  conts_print <- conts %>%
    map(unite, col = estci, est, ci, sep = " ") %>%
    map(add_row, estci = "Est. [95% CI]", p = "\\textit{p}", .before = 1) %>%
    bind_cols() %>%
    set_rownames(c("\\textit{Informed $-$  naive}", stub_contrasts))
  conts %<>% 
    bind_cols() %>% 
    set_colnames(paste(rep(names(models), each = length(names(models))), c("est", "ci", "p"), sep = "_"))
  cnames <- c("\\textit{Fixed effects}", as.character(anov_print[1, ]))
  list(anov_print[2:nrow(anov_print), ], conts_print) %>%
    map(set_colnames, paste0("V", 1:ncol(anov_print))) %>%
    apa_table(
      col.names = cnames, col_spanners = list("\\textbf{P1}" = 2:3, "\\textbf{N170}" = 4:5, "\\textbf{N400}" = 6:7),
      midrules = nrow(anov_print), font_size = "footnotesize", align = "lcccccc", escape = FALSE,
      caption = caption, note = note
    ) %>% cat()
  return(list("anov" = anov, "conts" = conts))
}

# Create table of models for Experiment 1
table_exp1 <- create_table(
  models = models_exp1,
  stub_anova = c("Part", "Condition", "Pt. × con."),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption = "Results of Linear Mixed-Effects Regression Models for Experiment 1\\smallskip",
  note = "Pt. = part, con. = condition, est. = estimate, CI = confidence interval."
)
```

Averaged across conditions, P1, N170, and N400 amplitudes differed as a function of the part of the experiment, all *F*s \> `r table_exp1$anov["Part", "P1_f"]`, all *p*s `r table_exp1$anov["Part", "P1_p"]` (see Table \@ref(tab:exp1-table)). In addition, N400 amplitudes differed between the informed and the naive condition averaged across the three parts of the experiment, *F*`r table_exp1$anov["Condition", "N400_df"]` = `r table_exp1$anov["Condition", "N400_f"]`, *p* = `r table_exp1$anov["Condition", "N400_p"]`. Crucially, the part × condition interaction was significant in the N170 component, *F*`r table_exp1$anov["Pt. × con.", "N170_df"]` = `r table_exp1$anov["Pt. × con.", "N170_f"]`, *p* = `r table_exp1$anov["Pt. × con.", "N170_p"]`, and in the N400 component, *F*`r table_exp1$anov["Pt. × con.", "N400_df"]` = `r table_exp1$anov["Pt. × con.", "N400_f"]`, *p* `r table_exp1$anov["Pt. × con.", "N400_p"]`, while also being marginally significant in the P1 component, *F*`r table_exp1$anov["Pt. × con.", "P1_df"]` = `r table_exp1$anov["Pt. × con.", "P1_f"]`, *p* = `r table_exp1$anov["Pt. × con.", "P1_p"]`. To answer our main research question, we decomposed these interactions into the differences between the semantically informed condition and the naive condition within the three different parts of the experiment.

### ERPs Before Insight Was Occurring

In the pre-insight part, when objects were unfamiliar to participants and presented without keywords, no differences emerged between the semantically informed and the naive condition in the P1, N170, or N400 component, all *p*s \> `r table_exp1$conts["Pre-insight part", "N400_p"]` (see Table \@ref(tab:exp1-table) and Figure \@ref(fig:exp1-plot)B). On the one hand, this was to be expected given that the critical presentation of the keywords (leading to semantically informed vs. naive perception) had not yet taken place. On the other hand, the absence of reliable differences in this part can be taken as evidence---with the usual caveats when interpreting null effects---that any subsequent effect of the semantic information in the other two parts cannot be accounted for by visual differences between the objects in the two conditions. Although the presentation of matching or non-matching keywords for each object was counterbalanced across participants, the fact that different numbers of objects were assigned to the two conditions based on participants' self report would have made it possible for such visual differences to emerge as a confounding factor. If they did, however, one would expect to detect these differences even before any keywords were presented, which we have now seen was not the case.

### ERPs While Insight Was Occurring

In the insight part, half of the unfamiliar objects were presented with matching keywords (for forming the semantically informed condition) and the other half were presented with non-matching keywords (for forming the naive condition). When semantic information informed the perception of the object, the amplitude of the N170 component was significantly enlarged (i.e., more negative), *b* = `r table_exp1$conts["Insight part", "N170_est"]` µV, *p* = `r table_exp1$conts["Insight part", "N170_p"]`, and the amplitude of the N400 component was significantly reduced (i.e., less negative), *b* = `r table_exp1$conts["Insight part", "N400_est"]` µV, *p* `r table_exp1$conts["Insight part", "N400_p"]`, compared to when the object was viewed naively without relevant semantic information. As in the pre-insight part, there were no reliable differences in the P1 component, *p* = `r table_exp1$conts["Insight part", "P1_p"]`.

### ERPs After Insight Had Occurred

In the post-insight part, the unfamiliar objects were presented for a third time, again without the keywords (as in the pre-insight part), to test whether the semantic information had any lasting effects on the processing of the objects. As in the insight part, the N400 component remained significantly reduced during semantically informed as compared to naive perception, *b* = `r table_exp1$conts["Post-insight part", "N400_est"]` µV, *p* `r table_exp1$conts["Post-insight part", "N400_p"]`, whereas the effect in the N170 component did not reoccur, *p* = `r table_exp1$conts["Post-insight part", "N170_p"]`. Instead, we now observed an even earlier modulation in the P1 component which was significantly enlarged (i.e., more positive) in response to objects for which semantically informed perception had taken place, *b* = `r table_exp1$conts["Post-insight part", "P1_est"]` µV, *p* = `r table_exp1$conts["Post-insight part", "P1_p"]`.

## Discussion

In Experiment 1, we measured event-related brain potentials from participants viewing unfamiliar objects before (pre-insight part), while (insight part) and after (post-insight part) they were able to understand what kind of object they saw. To induce this semantically informed perception, half of the objects in the insight part were preceded by matching verbal keywords about the object's typical function or use, whereas the other half were preceded by non-matching keywords, serving as a naive baseline condition.

In the insight part, we found that semantically informed perception was associated with enlarged amplitudes in the N170 component (150--200 ms after object onset) and reduced amplitudes in the N400 component (400--700 ms). When the same objects were presented once more in the post-insight part, the reduction of the N400 component reoccurred and we also observed a modulation of the P1 component (100--150 ms), which was significantly larger for objects for which semantically informed perception had taken place.

Because of the novelty of our experimental paradigm and the findings of Experiment 1, we ran a replication study to assess the robustness of these effects in another samples of participants.

# Experiment 2

## Method

```{r exp2-preparation}
# Read behavioral data for Experiment 2
data_exp2 <- list.files("data/exp2/RT", pattern = ".txt", full.names = TRUE) %>% map(read_behav)

# Check average number of stimuli per condition
(stims_exp2 <- map(data_exp2, function(x) {table(x$condition) / 3}) %>% bind_rows() %>% colMeans())
```

```{python exp2-preprocessing}
# List raw EEG filenames
fnames_exp2 = sorted(glob.glob("data/exp2/EEG/*.vhdr"))

# Import behavioral data from R
metadata_exp2 = r.data_exp2

# Check if preprocessing was done already (delete the file to re-run)
if os.path.exists("analysis/export/exp2-epo.fif"):
    # Read preprocessed data from file
    epochs_exp2 = mne.read_epochs("analysis/export/exp2-epo.fif", preload=True)
else:
    # Preprocess EEG data for Experiment 2
    epochs_exp2 = [
        preproc(vhdr_fname=fname, metadata=meta, **preproc_params) for fname, meta in zip(fnames_exp2, metadata_exp2)
    ]
    # Combine epochs into a single data set
    epochs_exp2 = mne.concatenate_epochs(epochs_exp2)
    # Backup epochs to the export folder
    epochs_exp2.save("analysis/export/exp2-epo.fif")


# Compute grand averages for Experiment 2
evokeds_exp2, evokeds_dat_exp2 = compute_evokeds(epochs=epochs_exp2)
```

```{r exp2-analysis}
# Re-import behavioral data and re-factorize some columns
data_exp2 <- py$epochs_exp2$metadata %>% mutate(
  part = factor(part, levels = c("I", "II", "III")),
  condition = factor(condition, levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")),
  participant = factor(participant),
  item = factor(item)
)

# Check number of rejected epochs
rejected_exp2 <- data_exp2 %>% group_by(participant) %>% tally() %>% mutate(n = 360 - n) %>% pull(n)
mean(rejected_exp2)    # Mean
median(rejected_exp2)  # Median
range(rejected_exp2)   # Range

# Check if single-trial ERPs were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp2-erps.RDS")) {
  # Read data from file
  data_exp2 <- readRDS("analysis/export/exp2-erps.RDS")
} else {
  # Compute single-trial ERPs for Experiment 2
  data_exp2 <- pmap_dfc(comps, compute_erps, epochs = py$epochs_exp2$get_data(), els = els) %>%
    set_names(comps$name) %>%
    cbind(data_exp2, .) %T>%
    saveRDS("analysis/export/exp2-erps.RDS")
}

# Remove the epochs to free up memory
py_run_string("del epochs_exp2")
import("gc")$collect()

# Remove any trials excluded from conditions
data_exp2 %<>% filter(condition %in% c("Informed", "Naive")) %>% droplevels()

# Contrast coding for condition (informed-naive) and part (2-1, 3-1)
contrasts(data_exp2$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_exp2$part) <- MASS::ginv(contrasts_part)

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp2-stats.RDS")) {
  # Read models from file
  models_exp2 <- readRDS("analysis/export/exp2-stats.RDS")
} else {
  # Compute LMMs for Experiment 2
  models_exp2 <- map(
    comps$name,
    compute_models,
    formula = form_exp12,
    data = data_exp2,
    control = ctrl_params,
    specs = specs_exp12
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/exp2-stats.RDS")
}
```

### Participants

Participants for Experiment 2 were 24 German native speakers (15 female, 9 male) with a mean age of 26 years (range 19 to 29 years) who had not participated in Experiment 1. They had no history of psychological disorders or treatment, were right-handed, and reported normal or corrected-to-normal vision. They gave written informed consent before starting the experiment and received a compensation of €8 per hour for participating.

### Materials, Procedure, and Analysis

All materials, procedures, EEG-related methods, and statistical analyses were identical to Experiment 1. An average of `r format(round(stims_exp2["Excl_known"], 1), nsmall = 1)` objects per participant (= `r scales::percent(stims_exp2["Excl_known"]/120, accuracy = 0.1)` of all unfamiliar objects) was classified as being known in the pre-insight part and excluded from all further analyses. Based on participants' responses in the insight part, an average of `r format(round(stims_exp2["Informed"], 1), nsmall = 1)` objects were assigned to the semantically informed condition (= `r scales::percent(stims_exp2["Informed"]/60, accuracy = 0.1)` of objects presented with matching keywords) and an average of `r format(round(stims_exp2["Naive"], 1), nsmall = 1)` objects were assigned to the naive condition (= `r scales::percent(stims_exp2["Naive"]/60, accuracy = 0.1)` of objects presented with non-matching keywords). Automatic rejection of EEG epochs containing artifacts led to the exclusion of `r format(round(mean(rejected_exp2), 1), nsmall = 1)` trials per participant (= `r scales::percent(mean(rejected_exp2)/360, accuracy = 0.1)`; range `r min(rejected_exp2)` to `r max(rejected_exp2)` trials).

## Results

```{r exp2-table, include=TRUE, results="asis"}
# Create table of models for Experiment 2
table_exp2 <- create_table(
  models_exp2,
  stub_anova = c("Part", "Condition", "Pt. × con."),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption = "Results of Linear Mixed-Effects Regression Models for Experiment 2\\smallskip",
  note = "Pt. = part, con. = condition, est. = estimate, CI = confidence interval."
)
```

As in Experiment 1, P1, N170, and N400 amplitudes differed between the three different parts of the experiments, all *F*s \> `r table_exp2$anov["Part", "N170_f"]`, all *p*s `r table_exp2$anov["Part", "N170_p"]` (see Table \@ref(tab:exp2-table)). Also as in Experiment 1, N400 amplitudes differed between the informed and the naive condition averaged across parts, *F*`r table_exp2$anov["Condition", "N400_df"]` = `r table_exp2$anov["Condition", "N400_f"]`, *p* `r table_exp2$anov["Condition", "N400_p"]`. The part × condition interaction was significant in the P1 component, *F*`r table_exp2$anov["Pt. × con.", "P1_df"]` = `r table_exp2$anov["Pt. × con.", "P1_f"]`, *p* = `r table_exp2$anov["Pt. × con.", "P1_p"]`, and in the N400 component, *F*`r table_exp2$anov["Pt. × con.", "N400_df"]` = `r table_exp2$anov["Pt. × con.", "N400_f"]`, *p* `r table_exp2$anov["Pt. × con.", "N400_p"]`, but not in the N170 component, *F*`r table_exp2$anov["Pt. × con.", "N170_df"]` = `r table_exp2$anov["Pt. × con.", "N170_f"]`, *p* = `r table_exp2$anov["Pt. × con.", "N170_p"]`.

(ref:figure-2-caption) Results of Experiment 2\smallskip

<!--

The papaja package does not yet deal with the placement of figure captions above rather than below the figures. Therefore, we manually need to post-process the .tex file, moving the caption{} above includegraphics{} and copying the figure note manually below includegraphics{}:

\bigskip\small\emph{Note.} 
ERP waveforms and scalp topographies are shown for objects for which participants experienced semantically informed versus naive perception within the pre-insight, insight, and post-insight parts of the experiment. In a direct replication of Experiment 1, the effect of semantic information on the N400 component in the insight and post-insight part and on the P1 component in post-insight part remained statistically significant, while the effect on the N170 component in the insight part remained only marginally significant (\emph{p} = .064). Ampl. = amplitude.\newline*\emph{p} \textless{} .05. **\emph{p} \textless{} .01. ***\emph{p} \textless{} .001.

-->

```{r exp2-plot, include=TRUE, fig.height=7.5, fig.cap = "(ref:figure-2-caption)"}
# Create headings for Parts I, II, and III
plot_headings <- function(spacing){
  plot_grid(
    ggplot() + annotate("text", label = "Pre-insight part", x = 0, y = 0, size = 14 / .pt, family = "Helvetica", fontface = "bold") + theme_void(), NULL,
    ggplot() + annotate("text", label = "Insight part", x = 0, y = 0, size = 14 / .pt, family = "Helvetica", fontface = "bold") + theme_void(), NULL,
    ggplot() + annotate("text", label = "Post-insight part", x = 0, y = 0, size = 14 / .pt, family = "Helvetica", fontface = "bold") + theme_void(),
    nrow = 1, rel_widths = spacing
  )
}

# Create plot for Experiment 2
plot_grid(
  #plot_bars(data = data_exp2, stars = stars_exp2, ymin = -2.5),
  plot_headings(spacing = c(10, 1, 10, 1, 10)),
  plot_erps(comps = comps, evokeds = py$evokeds_dat_exp2, models = models_exp2, montage = montage),
  plot_legends_erps(direction = "horizontal"),
  nrow = 3, rel_heights = c(0.3, 6, 1.2), labels = NULL
) + # Add some lines (to separate the parts)
  annotate(geom = "segment", x = 0.328, xend = 0.328, y = 0.15, yend = Inf, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.672, y = 0.15, yend = Inf, linetype = "dashed")
```

### ERPs Before Insight Was Occurring

As in Experiment 1, no differences between objects in the semantically informed and the naive condition emerged in the P1, N170, or N400 component, all *p*s \> `r table_exp2$conts["Pre-insight part", "P1_p"]` (see Table \@ref(tab:exp2-table) and Figure \@ref(fig:exp2-plot)).

### ERPs While Insight Was Occurring

As in Experiment 1, semantically informed as compared to naive perception (induced by matching vs. non-matching keywords) was associated with a (marginally) significant enhancement of the N170 component, *b* = `r table_exp2$conts["Insight part", "N170_est"]` µV, *p* = `r table_exp2$conts["Insight part", "N170_p"]`, and a significant reduction of the N400 component, *b* = `r table_exp1$conts["Insight part", "N400_est"]` µV, *p* `r table_exp1$conts["Insight part", "N400_p"]`.

### ERPs After Insight Had Occurred

As in Experiment 1, the presentation of the same unfamiliar objects for a third time (without keywords, as in the pre-insight part) led to significantly larger amplitudes in the P1 component in response to objects for which semantically informed perception had occurred, *b* = `r table_exp2$conts["Post-insight part", "P1_est"]` µV, *p* = `r table_exp2$conts["Post-insight part", "P1_p"]`. Also, N400 amplitudes in response to these objects remained significantly reduced, *b* = `r table_exp2$conts["Post-insight part", "N400_est"]` µV, *p* = `r table_exp2$conts["Post-insight part", "N400_p"]`.

### Joint Analysis of Experiments 1 and 2

In an attempt to maximize statistical power, we combined the ERP data sets from Experiments 1 and 2. This allowed us to determine (a) if the above effects---including the marginally significant ones---were reliable when tested in a larger sample, and (b) if there were significant differences in the ERP amplitudes between Experiments 1 and 2. Methods for statistical analysis were kept unchanged apart from the addition of a new factor denoting the experiment, coded as a scaled sum contrast [i.e., subtracting Experiment 1 from Experiment 2, the intercept being the grand mean across both experiments\; @schad2020]. This factor and its possible interactions with part, condition, and part × condition were included in the linear mixed-effects regression models as fixed effects and as potential by-item random slopes. They were not included as by-participant random slopes since different participants took part in Experiments 1 and 2. Note that, as above, random effects were eventually included only if their omission led to a significant decline in model fit [@matuschek2017; @R-buildmer].

```{r joint-analysis}
# Combine data from Experiments 1 and 2 (requires changing the participant IDs for Experiment 2)
data_joint <- data_exp2 %>%
  mutate(participant = fct_relabel(participant, ~ paste0(., "_2"))) %>%
  bind_rows(data_exp1, ., .id = "experiment") %>%
  mutate(experiment = as_factor(experiment))

# Contrast coding for experiment (2-1), condition (informed-naive), and part (2-1, 3-1)
t(contrasts_experiment <- t(cbind(c("1" = -1, "2" = 1))))
contrasts(data_joint$experiment) <- MASS::ginv(contrasts_experiment)
contrasts(data_joint$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_joint$part) <- MASS::ginv(contrasts_part)

# New formula for LMMs
form_joint <- buildmer::tabulate.formula(
  ~ part * condition * experiment + (part * condition | participant) + (part * condition * experiment | item)
) %>% mutate(block = replace(block, is.na(grouping), "fixed"))

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/joint-stats.RDS")) {
  # Read models from file
  models_joint <- readRDS("analysis/export/joint-stats.RDS")
} else {
  # Compute LMMs for Experiments 1 and 2 combined
  models_joint <- map(
    comps$name,
    compute_models,
    formula = form_joint,
    data = data_joint,
    control = ctrl_params,
    specs = specs_exp12
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/joint-stats.RDS")
}
```

```{r joint-table, include=TRUE, results="asis"}
# Create table of models for Experiments 1 and 2 combined
table_joint <- create_table(
  models_joint,
  stub_anova = c("Part", "Condition", "Experiment", "Pt. × con.", "Pt. × exp.", "Ins. × exp.", "Pt. × con. × exp."),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption = "Results of Linear Mixed-Effects Regression Models for Experiments 1 and 2 Combined\\smallskip",
  note = "Pt. = part, con. = condition, exp. = experiment, est. = estimate, CI = confidence interval."
)
```

As shown in Table \@ref(tab:joint-table), the main effect of the part of the experiment was significant in the P1, N170, and N400 component, all *F*s \> `r table_joint$anov["Part", "P1_f"]`, all *p*s `r table_joint$anov["Part", "P1_p"]`, as was the main effect of condition in the N400, *F*`r table_joint$anov["Condition", "N400_df"]` = `r table_joint$anov["Condition", "N400_f"]`, *p* `r table_joint$anov["Condition", "N400_p"]`. Furthermore, the part × condition interaction was now observed reliably in all three components, all *F*s \> `r table_joint$anov["Pt. × con.", "P1_f"]`, all *p*s \< `r table_joint$anov["Pt. × con.", "P1_p"]`. While there was a main effect of experiment in the N400, *F*`r table_joint$anov["Experiment", "N400_df"]` = `r table_joint$anov["Experiment", "N400_f"]`, *p* = `r table_joint$anov["Experiment", "N400_p"]`, the absence of any significant interactions of experiment with part or condition indicated that the effects of our experimental manipulations did not differ between Experiments 1 and 2.

Based on the part × condition interaction, we again computed follow-up comparisons between the semantically informed and the naive condition within each part, now collapsed across the data from both experiments (see Table \@ref(tab:joint-table) and Figure \@ref(fig:joint-plot)). This confirmed the absence of any reliable differences between the two conditions in the pre-insight part, all *p*s \> `r table_joint$conts["Pre-insight part", "N400_p"]`, the significant enhancement of the N170 component in the insight part, while the semantic information was obtained, *b* = `r table_joint$conts["Insight part", "N170_est"]` µV, *p* = `r table_joint$conts["Insight part", "N170_p"]`, the significant reduction of the N400 component in the insight part, while the semantic information was obtained, *b* = `r table_joint$conts["Insight part", "N400_est"]` µV, *p* `r table_joint$conts["Insight part", "N400_p"]`, and in the post-insight part, after the information had been obtained, *b* = `r table_joint$conts["Post-insight part", "N400_est"]` µV, *p* `r table_joint$conts["Post-insight part", "N400_p"]`, as well as the significant enhancement of the P1 component in the post-insight part, after the information had been obtained, *b* = `r table_joint$conts["Post-insight part", "P1_est"]` µV, *p* = `r table_joint$conts["Post-insight part", "P1_p"]`.

(ref:figure-3-caption) Measured and Modeled ERP Amplitudes for Experiments 1 and 2 Combined\smallskip

<!--

The papaja package does not yet deal with the placement of figure captions above rather than below the figures. Therefore, we manually need to post-process the .tex file, moving the caption{} above includegraphics{} and copying the figure note manually below includegraphics{}:

\bigskip\small\emph{Note.} 
The violins and box plots show the distributions of the by-participant grand averaged ERPs during semantically informed perception (in purple color) and naive perception (in petrol color), separately for the pre-insight part (before insight was occurring), the insight part (while insight was occurring), and the post-insight (after insight had occurred). The yellow dots show the corresponding means of these conditions as predicted by linear mixed-effects modelling (see main text and Table \ref{tab:joint-table}), together with their respective 95\% confidence interval. Ampl. = amplitude, CI = confidence interval, LMM = linear mixed-effects model.\newline*\emph{p} \textless{} .05. **\emph{p} \textless{} .01. ***\emph{p} \textless{} .001.

-->

```{r joint-plot, include=TRUE, fig.height=8, fig.cap = "(ref:figure-3-caption)"}
# Define two functions for split violin plots (kudos to https://stackoverflow.com/a/45614547)
GeomSplitViolin <- ggproto(
  "GeomSplitViolin", GeomViolin,
  draw_group = function(self, data, ..., draw_quantiles = NULL) {
    data <- transform(data, xminv = x - violinwidth * (x - xmin), xmaxv = x + violinwidth * (xmax - x))
    grp <- data[1, "group"]
    newdata <- plyr::arrange(transform(data, x = if (grp %% 2 == 1) xminv else xmaxv), if (grp %% 2 == 1) y else -y)
    newdata <- rbind(newdata[1, ], newdata, newdata[nrow(newdata), ], newdata[1, ])
    newdata[c(1, nrow(newdata) - 1, nrow(newdata)), "x"] <- round(newdata[1, "x"])
    if (length(draw_quantiles) > 0 & !scales::zero_range(range(data$y))) {
      stopifnot(all(draw_quantiles >= 0), all(draw_quantiles <= 1))
      quantiles <- ggplot2:::create_quantile_segment_frame(data, draw_quantiles)
      aesthetics <- data[rep(1, nrow(quantiles)), setdiff(names(data), c("x", "y")), drop = FALSE]
      aesthetics$alpha <- rep(1, nrow(quantiles))
      both <- cbind(quantiles, aesthetics)
      quantile_grob <- GeomPath$draw_panel(both, ...)
      ggplot2:::ggname("geom_split_violin", grid::grobTree(GeomPolygon$draw_panel(newdata, ...), quantile_grob))
    }
    else {
      ggplot2:::ggname("geom_split_violin", GeomPolygon$draw_panel(newdata, ...))
    }
  }
)
geom_split_violin <- function(mapping = NULL, data = NULL, stat = "ydensity", position = "identity", ...,
                              draw_quantiles = NULL, trim = TRUE, scale = "area", na.rm = FALSE,
                              show.legend = NA, inherit.aes = TRUE) {
  layer(
    data = data, mapping = mapping, stat = stat, geom = GeomSplitViolin,
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(trim = trim, scale = scale, draw_quantiles = draw_quantiles, na.rm = na.rm, ...)
  )
}

# Define our custom function for the violin plot
plot_violins <- function(data, models) {
  suppressWarnings(suppressMessages(
    parts <- map(c("I", "II", "III"), function(part) {
      map(c("P1", "N170", "N400"), function(comp) {
        modmeans <- models[[comp]]$means %>% filter(part == !!part) %>% rename(amplitude = emmean)
        ymin <- case_when(comp == "P1" ~ -5, comp == "N170" ~ -11, comp == "N400" ~ -11)
        asterisks <- models[[comp]]$contrasts %>%
          filter(part == !!part) %>%
          mutate(asterisks = case_when(p.value < .001 ~ "***", p.value < .01 ~ "**", p.value < .05 ~ "*")) %>%
          pull(asterisks)
        yastersisks <- case_when(
          comp == "P1" ~ 17.6,
          comp == "N170" ~ 10,
          comp == "N400" & part == "II" ~ 9.7,
          comp == "N400" & part == "III" ~ 8.9
        )
        data %>%
          filter(part == !!part) %>%
          rename(amplitude = !!comp) %>%
          group_by(participant, part, condition) %>%
          summarise(across(amplitude, mean)) %>%
          ggplot(aes(x = 0, y = amplitude, fill = condition)) +
          annotate("text", x = 0, y = yastersisks, label = asterisks, size = 6, family = "Helvetica") +
          geom_split_violin(trim = TRUE, color = "gray70") +
          geom_boxplot(position = position_dodge(width = 0.8), width = 0.3, color = "gray70") +
          geom_pointrange(
            data = modmeans,
            aes(ymin = lower.CL, ymax = upper.CL),
            position = position_dodge(width = 0.25),
            size = 0.4,
            color = viridisLite::viridis(n = 1, begin = 1)
          ) +
          coord_cartesian(ylim = c(ymin, ymin + 25), xlim = c(-0.6, 0.6)) +
          scale_x_continuous(breaks = c(-0.2, 0.2), labels = c("Informed", "Naive")) +
          scale_y_continuous(breaks = seq(ymin, ymin + 25, 5)) +
          ylab(paste(comp, "ampl. (µV)")) +
          scale_fill_viridis_d(begin = 0.1, end = 0.5) +
          theme_classic() +
          theme(
            legend.position = "none",
            axis.title.x = element_blank(),
            axis.title.y = element_text(size = 10, color = "black", family = "Helvetica"),
            axis.text = element_text(size = 10, color = "black", family = "Helvetica")
          )
      }) %>% plot_grid(plotlist = ., align = "v", nrow = 3)
    })
  ))
  plot_grid(parts[[1]], NULL, parts[[2]], NULL, parts[[3]], ncol = 5, rel_widths = c(10, 2, 10, 2, 10))
}

# Define function for the legend for the violin plot
plot_legends_violins <- function() {
  get_legend(
    tibble(Conditions = c("Informed", "Naive")) %>%
      ggplot(aes(x = 0, y = 0, color = "LMM mean\n± 95% CI", fill = Conditions)) +
      geom_raster() +
      geom_pointrange(aes(ymin = -1, ymax = 1)) +
      scale_fill_viridis_d(begin = 0.1, end = 0.5) +
      scale_color_viridis_d(begin = 1) +
      guides(
        fill = guide_legend(override.aes = list(color = NA)),
        color = guide_legend(title = NULL)
      ) +
      theme(
        legend.box.background = element_rect(colour = "black", size = 0.5),
        legend.box.margin = margin(l = 9, r = 9, b = 9, t = 7),
        legend.background = element_blank(),
        legend.spacing = unit(0.01, "inches"),
        legend.title = element_text(family = "Helvetica", size = 10, face = "bold"),
        legend.text = element_text(family = "Helvetica", size = 10),
        legend.key = element_rect(fill = "gray70")
      )
  )
}

# Create violin plot for Experiments 1 and 2 combined
plot_grid(
  plot_headings(spacing = c(10, 2, 10, 2, 10)),
  plot_violins(data = data_joint, models = models_joint),
  nrow = 2, rel_heights = c(0.3, 6.7)
) %>% plot_grid(
  plot_legends_violins(),
  nrow = 1, rel_widths = c(8.5, 1.5)
) + # Add some lines (to separate the parts)
  annotate(geom = "segment", x = 11/34*0.85, xend = 11/34*0.85, y = -Inf, yend = Inf, linetype = "dashed") +
  annotate(geom = "segment", x = 23/34*0.85, xend = 23/34*0.85, y = -Inf, yend = Inf, linetype = "dashed")
```

### Control Analysis

```{r control-analysis}
# Calculate average number of objects per condition in Experiments 1 and 2
stims_joint <- (stims_exp1 + stims_exp2) / 2

# Re-load saved ERPs from Experiments 1 and 2
data_joint_control <- bind_rows(
  readRDS("analysis/export/exp1-erps.RDS"),
  readRDS("analysis/export/exp2-erps.RDS") %>%
    mutate(participant = fct_relabel(participant, ~ paste0(., "_2"))),
  .id = "experiment"
) %>% mutate(experiment = as_factor(experiment))

# # Keep all three conditions
# data_joint_control %<>% filter(condition %in% c("Informed", "Naive", "Excl_informed")) %>% droplevels()
# contrasts_condition <- t(cbind(c("Informed" = 1, "Naive" = -1, "Excl_informed" = 0),
#                                c("Informed" = 1, "Naive" = 0, "Excl_informed" = -1)))
# contrasts(data_joint_control$condition) <- MASS::ginv(contrasts_condition)
# model_joint_control_N170 <- buildmer::buildmer(
#   N170 ~ part * condition * experiment + (part * condition | participant) + (part * condition * experiment | item),
#   REML = FALSE,
#   control = ctrl_params,
#   buildmerControl = buildmer::buildmerControl(
#     data = data_joint_control,
#     direction = c("order", "backward"),
#     elim = function(logp) exp(logp) >= .20,
#     calc.anova = TRUE,
#     ddf = "Satterthwaite"
#   )
# )
# emmeans::emm_options(lmer.df = "Satterthwaite", lmerTest.limit = Inf)
# emmeans::emmeans(model_joint_control_N170@model, pairwise ~ condition | part, infer = TRUE)$contrasts

# This time, we keep only the two conditions with matching keywords
data_joint_control %<>% filter(condition %in% c("Informed", "Excl_informed")) %>% droplevels()

# Contrast coding for condition (informed-no_informed) and part (2-1, 3-1)
contrasts(data_joint_control$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_joint_control$part) <- MASS::ginv(contrasts_part)

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/joint-control-stats.RDS")) {
  # Read models from file
  models_joint_control <- readRDS("analysis/export/joint-control-stats.RDS")
} else {
  # Re-compute LMMs for Experiments 1 and 2 combined
  models_joint_control <- map(
    comps$name,
    compute_models,
    formula = form_joint,
    data = data_joint_control,
    control = ctrl_params,
    specs = specs_exp12
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/joint-control-stats.RDS")
}

# Create a table but don't print
table_joint_control <- create_table(
  models = models_joint_control,
  stub_anova = c("Part", "Condition", "Experiment", "Pt. × con.", "Pt. × exp.", "Ins. × exp.", "Pt. × con. × exp."),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption = "Results of linear mixed-effects regression models with a different baseline",
  note = "Pt. = part, con. = condition, exp. = experiment, est. = estimate, CI = confidence interval."
)
```

One may raise concerns whether the modulation of the N170 component in the insight part genuinely reflects the semantically informed perception of the objects in the respective condition, or---as an alternative explanation---whether it may be driven by the objects in the other, semantically naive condition. Remember that these objects were preceded by non-matching keywords which were picked so that they could not be related to the visual features of the object and their configuration. Thus, the modulation of the ERP components in the insight part could be a mismatch response to those objects---reflecting, for example, the fact that the visual features of the object shown in Figure \@ref(fig:exp1-plot)A cannot be reconciled with the function of morsing messages. To preclude this alternative explanation, we repeated our analysis using a different baseline against which the objects in the semantically informed condition were contrasted. Instead of the naive condition (where objects were presented with non-matching keywords), we now used those objects which were preceded by matching keywords (as in the informed condition), but which were excluded from the main analysis because participants indicated behaviorally that they did not understand the object they were seeing. Across both experiments, this was the case for `r scales::percent(stims_joint["Excl_informed"]/60, accuracy = 0.1)` of objects presented with matching keywords as compared to `r scales::percent(stims_joint["Informed"]/60, accuracy = 0.1)` of objects which did indeed lead to semantically informed perception.[^1] Just as above, this control analysis revealed a robust N170 effect in the insight part, *b* = `r table_joint_control$conts["Insight part", "N170_est"]` µV, *p* `r table_joint_control$conts["Insight part", "N170_p"]`. Thus, this enhanced negativity seems to be a genuine marker of semantically informed perception, no matter if compared to objects presented with non-matching keywords or compared to objects presented with matching keywords but on which participants failed to capitalize. Note that the reduction of the N400 component in the insight part also remained robust in this control analysis, *b* = `r table_joint_control$conts["Insight part", "N400_est"]` µV, *p* `r table_joint_control$conts["Insight part", "N400_p"]`.

[^1]: Note that these percentages do not add up to 100% because some objects were excluded from all analyses because participants indicated knowing these objects in the pre-insight part, before any keywords had been presented (see Method).

## Discussion

Experiment 2, which was a direct replication of Experiment 1, confirmed the effects of obtaining semantic information about previously unfamiliar objects on ERPs associated with lower-level visual perception (P1), higher-level visual perception (N170), and semantic processing (N400). Objects for which participants experienced semantically informed perception via matching keywords elicited enlarged N170 amplitudes and reduced N400 amplitudes. When these objects were presented once more, they again elicited reduced N400 amplitudes as well as enlarged P1 amplitudes.

```{r exp3-preparation, eval=run_exp3}
# Read behavioral data for Experiment 3
data_exp3 <- list.files("data/exp3/RT", pattern = ".txt", full.names = TRUE) %>% map(read_behav)

# Check average number of stimuli per condition
(stims_exp3 <- map(data_exp3, function(x) {
  x %<>% filter(part != "IV")
  table(x$condition) / 3
}) %>% bind_rows() %>% colMeans())
```

```{python exp3-preprocessing, eval=run_exp3}
# List raw EEG filenames
fnames_exp3 = sorted(glob.glob("data/exp3/EEG/*.vhdr"))

# Import behavioral data from R
metadata_exp3 = r.data_exp3

# Update stimulus triggers
preproc_params["event_id"] = {
    "match/upright": 241,
    "match/inverted": 242,
    "mismatch/upright": 243,
    "mismatch/inverted": 244,
}

# Check if preprocessing was done already (delete the file to re-run)
if os.path.exists("analysis/export/exp3-epo.fif"):
    # Read preprocessed data from file
    epochs_exp3 = mne.read_epochs("analysis/export/exp3-epo.fif", preload=True)
else:
    # Preprocess EEG data for Experiment 3
    epochs_exp3 = [
        preproc(vhdr_fname=fname, metadata=meta, **preproc_params)
        for fname, meta in zip(fnames_exp3, metadata_exp3)
    ]
    # Combine epochs into a single data set
    epochs_exp3 = mne.concatenate_epochs(epochs_exp3)
    # Backup epochs and evokeds to the export folder
    epochs_exp3.save("analysis/export/exp3-epo.fif")
```

```{r exp3-analysis, eval=run_exp3}
# Re-import behavioral data and re-factorize some columns
data_exp3 <- py$epochs_exp3$metadata %>% mutate(
  part = factor(part, levels = c("I", "II", "III", "IV")),
  position = factor(position, levels = c("Inverted", "Upright")),
  condition = factor(condition, levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")),
  participant = factor(participant), item = factor(item)
)

# Check if ERPs were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp3-erps.RDS")) {
  # Read ERPs from file
  data_exp3 <- readRDS("analysis/export/exp3-erps.RDS")
} else {
  # Compute single-trial ERPs for Experiment 3
  data_exp3 <- pmap_dfc(comps, compute_erps, epochs = py$epochs_exp3$get_data(), els = els) %>%
    set_names(comps$name) %>%
    cbind(data_exp3, .) %T>%
    saveRDS("analysis/export/exp3-erps.RDS")
}

# Remove the epochs to free up memory
py_run_string("del epochs_exp3")
import("gc")$collect()

# Remove any trials from part IV
data_exp3 %<>% filter(part != "IV") %>% droplevels()

# Check number of rejected epochs
rejected_exp3 <- data_exp3 %>% group_by(participant) %>% tally() %>% mutate(n = 672 - n) %>% pull(n)
mean(rejected_exp3)    # Mean
median(rejected_exp3)  # Median
range(rejected_exp3)   # Range

# Remove any trials excluded from conditions
data_exp3 %<>% filter(condition %in% c("Informed", "Naive")) %>% droplevels()

# Contrast coding for position (inverted-upright), condition (informed-naive), and part (2-1, 3-1)
t(contrasts_position <- t(cbind(c("Inverted" = 1, "Upright" = -1))))
contrasts(data_exp3$position) <- MASS::ginv(contrasts_position)
contrasts(data_exp3$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_exp3$part) <- MASS::ginv(contrasts_part)

# New formula for LMMs
form_exp3 <- buildmer::tabulate.formula(
  ~ part * condition * position + (part * condition * position | participant) + (part * condition * position | item)
) %>% mutate(block = replace(block, is.na(grouping), "fixed"))

# New follow-up contrasts
specs_exp3 <- pairwise ~ condition | position * part

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp3-stats.RDS")) {
  # Read models from file
  models_exp3 <- readRDS("analysis/export/exp3-stats.RDS")
} else {
  # Compute LMMs for Experiment 3
  models_exp3 <- map(
    comps$name,
    compute_models,
    formula = form_exp3,
    data = data_exp3,
    control = ctrl_params,
    specs = specs_exp3
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/exp3-stats.RDS")
}
```

```{r exp3-table, include=TRUE, results="asis", eval=run_exp3}
# Create table of models for Experiment 3
table_exp3 <- create_table(
  models_exp3,
  stub_anova = c("Part", "Condition", "Position", "Pt. × con.", "Pt. × pos.", "Ins. × pos.", "Pt. × con. × pos."),
  stub_contrasts = c(
    "Part I, inverted", "Part I, upright", "Part II, inverted",
    "Part II, upright", "Part III, inverted", "Part III, upright"
  ),
  caption = "Results of linear mixed-effects regression models for Experiment 3",
  note = "Pt. = part, con. = condition, pos. = position, est. = estimate, CI = confidence interval."
)
```

# General Discussion

Here we investigated if obtaining a semantic understanding of previously unfamiliar objects has an influence on how we perceive them. To this end, we measured ERPs while participants viewed unfamiliar objects before, while, and after receiving semantic information about them. For half of the objects, this information was matching the object, thus leading to semantically informed perception, whereas for the other half of the objects, it was non-matching, thus keeping the perception of the object semantically naive. We found semantically informed perception to be accompanied by enlarged (i.e., more negative) N170 amplitudes and reduced (i.e., less negative) N400 amplitudes. When the same objects were presented again without the semantic information, the N400 component remained significantly reduced and we also observed a modulation of the P1 component, which was enlarged (i.e., more positive) in response to objects that had previously triggered semantically informed perception.

The reduction of the N400 component (400--700 ms after object onset) during semantically informed perception was the numerically largest and most robust effect. Perhaps least controversially, this effect indicates that acquiring an understanding of the objects (in the insight part) has lessened participants' demand for effortful semantic processing compared to the naive condition [@kutas2011]. This replicates previous work showing that N400 amplitudes are larger in response to pictures when they are either difficult to understand in and of themselves [e.g., @abdelrahman2008; @supp2005] or difficult to integrate into the preceding context [e.g., @barrett1990; @ganis1996; @hirschfeld2011]. The time course of this effect and the computational role of the N400 [@lau2008; @rabovsky2018] suggest that it has a post-perceptual locus.

In contrast to the N400, the N170 component (150--200 ms after object onset) was modulated while but not after the objects were presented together with the relevant semantic information. As such, it can be seen as an online marker of semantic insight, that is, participants suddenly understanding the visual objects in the light of the additional information provided by the verbal keywords. The N170 is typically associated with the holistic perception of faces [@eimer2011; @sagiv2001] and other stimuli of visual expertise [@rossion2002; @tanaka2001]. It being enlarged during semantically informed perception may therefore reflect that the additional semantic information made participants experience the configuration of the visual features of the objects in a new and meaningful way. This interpretation is supported by previous findings using a similar experimental paradigm in the realm of face perception [@bentin2002]: Here, participants showed a face-like (i.e., enlarged) N170 response to a scrambled version of a schematic face after (but not before) they were primed with the intact version of the same face. This effect was absent when a visual control stimulus (a non-face object instead of the intact face) was shown between the two presentations of the scrambled face. In the realm of non-face stimuli, enlarged N170 amplitudes have also been observed when participants were asked to discriminate between composite line drawings of meaningful objects as compared to composite line drawings of non-objects [@beaucousin2011]. Of note, this effect was present only when participants were asked to decide based on the global shape of the object and it was reversed in polarity when they were asked to decide based on the constituent parts of the object. Together with the present study, these findings suggest an online impact of meaningfulness on the higher-level (i.e., holistic) perception of visual objects, integrating across their visual features.

Unlike the N400 and N170 components, the P1 component (100--150 ms after object onset) was modulated by semantic information only one trial after it had been obtained. This is consistent with previous studies showing P1 effects when participants learned meaningful information about previously unfamiliar objects [@abdelrahman2008; @maier2018; @maier2019; @weller2019] or about familiar objects that were rendered difficult to recognize [@samaha2018]. What the present study adds to these findings is that the P1 effect does not take an extensive learning phase to develop (with multiple presentations of the objects together with the respective information) but that it can be observed as soon as one trial after semantic insight has happened. Because the P1 is typically associated with lower-level sensory processing [e.g., @johannes1995; @luck2014; @pratt2011], we take its susceptibility to semantic information as an indicator that knowledge about the function of an object can change how we perceive it visually.

Both the N170 and the P1 components therefore seem to be sensitive to the semantic meaningfulness of visual objects as we perceive them. However, the finding that these two components were modulated in different parts of our experiment suggests that they reflect different aspects top-down processing with different time courses and neuroanatomical implementations. It has been pointed out that the onset of the N170 component is consistent with a top-down influence of (non-visual) areas in the prefrontal and parietal cortices on visual areas, whereas modulations of the P1 component seem reflect recurrent processing *within* the visual system [@wyatte2014]. Here we show that the former pathway seems to be able to convey semantic information instantaneously (i.e., within the same trial), whereas the latter seems to take at least one---but apparently also not more than one---additional presentation of the visual object to emerge.

While the limited spatial resolution of the EEG precludes a precise localization of these effects within the ventral stream of object recognition, there is converging evidence coming from fMRI showing that semantic information can feed back into the earliest of visual areas. @hsieh2010 showed participants indiscernible two-tone ("Mooney") versions of images before and after showing them the original versions. They found that the brain responses to the original image were correlated more strongly with the second presentation of the Mooney image (after insight had taken place) than with the first presentation of the Mooney image (before insight had taken place). This increase in representational similarity was not just observed in higher-level object-sensitive areas in the lateral occipital complex (LOC), but also in early retinotopic cortex (areas V1, V2, and V3). Both of these cortical regions are consistent with the neural generators of the N170 and P1 components of the ERP which we have found to be sensitive to the semantically informed perception of previously unfamiliar objects.

On a theoretical level, the top-down modulation of these visual ERPs by semantic information challenges a modular view of visual perception [@firestone2016; @fodor1983; @pylyshyn1999, but see @clarke2020]. However, proponents of such a modular view have pointed out important conceptual and methodological shortcomings of previous studies claiming to demonstrate top-down effects of cognition on perception [@firestone2016; @machery2015]. We took care to address as many of these shortcomings as possible: First, we showed that no effect had been present before any semantic information was presented (in the pre-insight part). Second, we used an objective and time-resolved measure (ERPs) to disentangle effects with a perceptual locus from those with a post-perceptual locus. Third, we reduced response and demand biases by keeping the manipulation (i.e., matching or non-matching keywords) obscure to the participants and by including well-known objects as filler stimuli. Fourth, we precluded low-level visual differences between conditions by counterbalancing the assignment of objects to conditions across participants. Fifth, we reduced priming and attentional effects by presenting all objects in a randomized order and at the same location. Sixth, we reduced memory effects by using only unfamiliar objects and by measuring online ERPs rather than delayed behavioral responses. We hope that these procedures have effectively ruled out some of the most important alternative explanations for the top-down effects that we have observed, thus making a more compelling case against the cognitive impenetrability of perception.

An interactive view of object vision with an abundance of top-down feedback also challenges the predominantly feed-forward models in machine vision [e.g., @dicarlo2012; @marr1982]. In fact, the lack of a semantic knowledge base that dynamically interacts with the processing of lower-level visual features may be one key reason why state-of-the-art deep-learning algorithms need orders of magnitude more training examples to achieve human-level performance in object recognition. For these network models, single-trial learning of previously unfamiliar objects, as was observed on the behavioral and on the neurophysiological level in the present study, seems to be out of reach until they overcome this "barrier of meaning" [@mitchell2020]. Drawing inspiration from cognitive psychology and human neuroscientific data may help making these models more biologically plausible and, at the same time, more resource and data efficient.

A theoretical framework that would explicitly predict or explain the observed P1 and N170 effects in our study is lacking at present. The effects are consistent, however, with the reverse hierarchy theory [@ahissar2004; @hochstein2002] which posits that objects first enter our visual consciousness at an abstract, conceptual level. Once this initial "vision at a glance" has taken place, feedback connections to earlier layers of the visual system are being accessed to extract relevant lower-level features ("vision with scrutiny"). This reverse trajectory down the visual hierarchy may explain (a) the semantically induced changes to the fMRI signal in LOC and retinotopic cortex [@hsieh2010] as well as (b) the modulations of early visual ERP components observed in the present study and others [@abdelrahman2008; @maier2014; @maier2019; @samaha2018; @weller2019]. Besides the reverse hierarchy theory, an important role of top-down mechanisms for vision or, more specifically, object recognition is also posited by the family of predictive coding and Bayesian inference theories [e.g., @lupyan2015; @panichello2013; @yuille2006]. Despite the theoretical advances, the mechanistic details of these top-down effects at the algorithmic and implementational level [@marr1982] remain to be clarified.

The lack of mechanistic insight into the top-down effects that we have observed is one limitation of the present study. Another one is our reliance on rare and highly specialized objects (see appendix). This was necessary to induce the experience of semantic insight in an adult population of undergraduate students but may not necessarily generalize to the way in which we learn about everyday objects.[^2] One way of addressing this issue would be to adapt the present paradigm to younger participants, using everyday objects which they have not yet learned about. Finally, the number participants (*n* = 24 per experiment) and trials (*k* $\approx$ 30 per part in the insight condition and *k* $\approx$ 50 per part in the naive condition) can be considered small by today's standards [@baker2020]. Our analysis may therefore not have been particularly sensitive, especially since linear mixed-effects regression models often have limited statistical power [see, e.g., the simulations by @matuschek2017].

[^2]:  Note, however, the converging evidence coming from the complementary approach of rendering images of everyday objects difficult to recognize [@samaha2018].

Nevertheless, this study provides preliminary evidence that whenever we acquire semantic information about a previously unfamiliar object, this information influences multiple stages of processing. These influences do not seem to be confined to later, post-perceptual stages---typically associated with semantic processing---but can also be observed at earlier stages---typically reflecting lower levels of visual perception and happening within 200 ms after the object is presented to us.

\newpage

# References

\setlength{\cslhangindent}{0.5in}

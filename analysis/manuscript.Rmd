---
title             : |
  | Event-Related Potentials of the Semantically Informed
  | Perception of Unfamiliar Objects
shorttitle        : "Semantic Knowledge and Unfamiliar Objects"

author: 
  - name          : "Alexander Enge"
    affiliation   : "1,2"
  - name          : "Franziska Süß"
    affiliation   : "3"
  - name          : "Rasha Abdel Rahman"
    affiliation   : "1"
    corresponding : yes
    address       : "Rudower Chaussee 18, 12489 Berlin"
    email         : "rasha.abdel.rahman@hu-berlin.de"

affiliation:
  - id            : "1"
    institution   : "Humboldt-Universität zu Berlin"
  - id            : "2"
    institution   : "Max Planck Institute for Human Cognitive and Brain Sciences"
  - id            : "3"
    institution   : "Fachhochschule des Mittelstands"

authornote: |
  \addORCIDlink{Alexander Enge}{0000-0003-0100-2297}
  
  \addORCIDlink{Rasha Abdel Rahman}{0000-0002-8438-1570}

  The preprocessed data and code for this study are openly available at https://osf.io/uksbc/.
  
  We have no conflict of interest to disclose.

abstract: |
  Does our perception of an object change as soon as we discover what function it serves? This question is relevant not only for our everyday lives, where we may encounter novel tools and gadgets as parts of our dynamic working and private environments; it also pertains to the long-standing debate around the (im)penetrability of perception by higher cognitive capacities. In this experiment, we showed participants (*n* = 24) pictures of 120 unfamiliar objects either together with matching information about their function---leading to semantically informed perception---or together with non-matching information---resulting in naive perception. We measured event-related potentials (ERPs) to investigate at which stages in the visual processing hierarchy these two types of object perception differed from one another. We found that semantically informed as compared to naive perception was associated with larger amplitudes in the N170 component and reduced amplitudes in the N400 component. When the same objects were presented once more (without any information), the N400 effect persisted and we now also observed enlarged amplitudes in the P1 component in response to objects for which semantically informed perception had taken place. We replicated these novel findings in an independent sample (*n* = 24). Consistent with previous work, these results suggest that obtaining semantic information about previously unfamiliar objects alters aspects of their lower-level visual perception (P1 component), higher-level visual perception (e.g., holistic perception; N170 component), and semantic processing (N400 component).
  
keywords          : "objects, semantic knowledge, visual perception, event-related potentials"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

bibliography      : "manuscript_files/r-references.bib"
csl               : "manuscript_files/apa.csl"
zotero            : "aha"
documentclass     : "apa7"
classoption       : "man"
output            :
  papaja::apa6_pdf:
    latex_engine  : "xelatex"

editor_options: 
  chunk_output_type: console

header-includes:
  - \geometry{a4paper}
  - \fancyheadoffset[R,L]{0pt}
  - \raggedbottom
  - \usepackage[all]{nowidow}
  - \usepackage[bottom]{footmisc}
  - \interfootnotelinepenalty=10000
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \usepackage{makecell}
  - \renewcommand{\cellset}{\renewcommand{\arraystretch}{0.7}}
  - \captionsetup[table]{font={stretch=1.5}}
  - \captionsetup[figure]{font={stretch=1.5}}
---

```{r setup, include=FALSE}
# Do you want to re-run the EEG preprocessing? Note that this can only be done with the raw data,
# which are available from the corresponding author (rasha.abdel.rahman@hu-berlin.de) upon request.
rerun_preproc <- TRUE

# Do you want to re-run the linear mixed-effect models? This can be done even if only the
# preprocessed single-trial data are available (from GitHub and/or the OSF). Note, however, that
# fitting all models may take 24 hours or more.
rerun_models <- FALSE

# Do you want to run the analysis for Experiment 3?
run_exp3 <- FALSE

# Load packages
library(papaja)
library(reticulate)
library(tidyverse)
library(magrittr)
library(cowplot)

# Set global chunk options
knitr::opts_chunk$set(include = FALSE, fig.align = "center", out.width = "100%", fig.width = 10)

# Link to the bibliography file
r_refs("analysis/manuscript_files/r-references.bib")

# Source plotting functions
source("analysis/manuscript_files/functions_plots_tables.R")

# Create an export folder for results
dir.create("analysis/export", showWarnings = FALSE)

# Define ERP components of interest
comps <- tibble(
  name = c("P1", "N170", "N400"),
  tmin = c(100, 150, 400),
  tmax = c(150, 200, 700),
  roi = list(
    c("PO3", "PO4", "POz", "O1", "O2", "Oz"),
    c("P7", "P8", "PO7", "PO8", "PO9", "PO10"),
    c("C1", "C2", "Cz", "CP1", "CP2", "CPz")
  )
)

# Define some filenames for I/O
fnames <- list(
  epochs = "analysis/export/aha-epo.fif",
  trials = "analysis/export/aha-trials.csv",
  gavgs = "analysis/export/aha-gavgs.RDS",
  conditions = "analysis/export/aha-obj-per-cond.csv",
  montage = "analysis/export/aha-montage.csv"
)
```

Does our perception of an object change as soon as we discover what function it serves? In this study, we presented participants with unfamiliar objects that were preceded either by matching information about their function, leading to semantically informed perception, or by non-matching information, leading to naive perception. We expected semantically informed perception to differ from naive perception within the object processing hierarchy and we capitalized on the high temporal resolution of event-related potentials (ERPs) to find out at which stage (or stages) this was indeed the case. This allowed us to show for the first time that knowledge about the function of a previously unfamiliar object alters its early cortical processing in the same trial as it is being discovered and within less than 200 ms after the presentation of the object.

The claim that higher-level cognitive capacities such as knowledge or language can modulate lower-level perceptual processes has been the source of a long-running debate. This debate revolves around the question of whether or not our perception of the (visual) world is "cognitively impenetrable" [@pylyshyn1999] or, in other words, determined solely by the visual input reaching our retina. According to one view, higher-level mental states and functions are not able to penetrate perception and kick in only after lower-level perceptual analysis of the visual input has taken pace [e.g., @firestone2016; @fodor1983; @pylyshyn1999]. Visual perception itself is treated as an encapsulated module that processes the retinal input in a feed-forward fashion, progressing from lower areas with smaller receptive field sizes to areas representing increasingly complex shapes and, eventually, whole objects [@dicarlo2012; @marr1982].

This position is challenged by the alternative view that perceptual processing dynamically interacts with other aspects of cognition from early on [e.g., @churchland1994; @lupyan2015; @teufel2017]. The myriad feedback connections from areas higher up the visual hierarchy (e.g., the middle temporal and lateral occipital cortices) to early visual areas [e.g., V1 and V2\; @bullier2001; @gilbert2013]---as well as behavioral evidence---have inspired theories of visual perception that emphasize the top-down influence of cognitive processes. For instance, the reverse hierarchy theory [@ahissar2004; @hochstein2002] assumes that conscious visual perception occurs initially at the level of whole objects or object categories. Only after this high-level interpretation of the stimulus has been obtained, its more for fine grained visual details---if relevant for the current task---are being accessed from lower-level areas via top-down connections. Along similar lines, the label feedback hypothesis [@lupyan2012] posits that the activation of an object's name (i.e., a high level property) transiently warps perceptual space so that its diagnostic visual features are being processed preferentially. Finally, an active role of top-down influences is also promoted by theories of vision in the framework of Bayesian inference and predictive coding [e.g., @clark2013; @lupyan2015; @panichello2013; @yuille2006].

Empirical findings from experimental psychology have recently added additional support to the view that perception can indeed be penetrated by other aspects of cognition [for review, see @collins2014; @vetter2014]. These aspects of cognition may include transient states such as emotions [e.g., @bocanegra2009; @phelps2016] or intentions [e.g., @balcetis2010; @cole2012] as well as learned capacities such as language [e.g., @boutonnet2015; @maier2018; @mo2011] or declarative knowledge about faces [e.g., @abdelrahman2011; @suess2015; @eiserbeck2020] and objects [e.g., @abdelrahman2008; @gauthier2003; @weller2019]. Despite this wealth of empirical evidence, critics of the idea that cognitive functions can penetrate perception have remained skeptical and pointed out important methodological shortcomings in large parts of this literature [@firestone2016; @machery2015]. They argue, for example, that most behavioral paradigms have not been able to discern between perceptual and post-perceptual (e.g., memory-related) effects. Furthermore, cognitive influences in some studies were confounded with differences in the low-level visual input itself. We will revisit these and other potential pitfalls with regards to the present study in the Discussion section once we have laid out our methodology and results.

In general, however, it is important to note that one productive way to circumvent at least some of these concerns is to use neurophysiological recording methods such as ERPs [@luck2014]: Their high temporal resolution allows for a direct test of which processing stages---perceptual and/or post-perceptual---are influenced by higher-level capacities such as semantic knowledge. This is made possible by the fact that when participants are presented with visual stimuli, their averaged brain responses show characteristic deflections (ERP components), each of them with its typical polarity (positive or negative), peak latency, spatial scalp distribution, and functional role(s).

The visual P1 component of the ERP refers to a positive deflection peaking as early as 100--150 ms after stimulus onset at occipital channels. It is generated by multiple sources in the extrastriate cortices of the middle occipital and fusiform gyri [@dirusso2001; @mangun1995]. The P1 is thought to reflect the processing of low-level visual characteristics of the stimulus [e.g., size, luminance, and contrast\; @dirusso2001; @luck2014; @johannes1995], which is why it being modulated by higher-level cognitive capacities would challenge a purely bottom-up, encapsulated view of visual perception. It is well-established that the P1 is enhanced when participants pay attention to the spatial location of a stimulus [@luck2000; @mangun1995; @mangun1991] but there is also more recent evidence for higher-level cognitive influences that go beyond spatial attention (see below).

The visual N170 (or N1) component refers to the negative deflection following the P1 and is maximal around 150--200 ms after stimulus onset at occipito-temporal channels. Just as the P1, it is influenced by visual parameters of the stimulus [@johannes1995] and by selective attention, especially when participants are required to discriminate between different visual stimuli [@vogel2000; @mangun1991]. The N170 is enlarged (i.e., more negative) in response to faces [@bentin1996; @rossion2011] and other visual stimuli for which one happens to be an expert [e.g., birds, dogs, or cars\; @gauthier2003a; @tanaka2001]. This category selectivity seems to reflect both the processing of individual visual features of the respective objects (including faces) as well as the holistic configuration of these features [@eimer2011; @jacques2010; @rossion1999; @sagiv2001]. Thus, a modulation of the N170 by cognitive capacities such as knowledge would suggest these capacities having an influence on higher-level, holistic perception of the visual stimulus.

Finally, the N400 component refers to more negative ERP amplitudes when stimuli require more as compared to less resources for semantic processing or integration [@kutas2011; @lau2008; @rabovsky2018]. It begins approximately 300 ms after stimulus onset and is most pronounced at centro-parietal channels. A modulation of the N400 component by cognitive capacities such as knowledge can be taken as evidence for a post-perceptual influence on stimulus processing.

Previous studies have focused on these and other ERP components to investigate which processing stages are indeed influenced by knowledge about visual objects [@abdelrahman2008; @gratton2009; @maier2014; @maier2019; @rossion2002; @rossion2004; @tanaka2001; @samaha2018; @weller2019]. Together, they suggest that not only higher-level ERPs like the semantic N400 component are influenced by acquiring information about objects, but also earlier components that typically reflect bottom-up visual processing. This was the case, for instance, when participants were presented with a range of unfamiliar objects, receiving in-depth verbal descriptions about their function for half of the objects and irrelevant verbal information (i.e., cooking recipes) for the other half of the objects [@abdelrahman2008, Experiment 1]. After this learning phase, the same objects were presented in three different ERP tasks which did not require explicit access to any of the learned semantic information. In all three tasks, ERP amplitudes in response to objects for which in-depth knowledge had been acquired differed from those in response to objects for which this had not been the case. These differences occurred not just in the N400 component, indexing a modulation of post-perceptual semantic processing, but also in the P1 component, suggesting a top-down modulation of lower-level perceptual processing. Interestingly, the ERPs observed for previously unfamiliar objects with newly acquired in-depth knowledge were qualitatively similar to those for untrained but well-known objects. The modulation of the P1 component has recently been replicated when the same objects were presented under circumstances of limited attentional resources in an attentional blink paradigm [@weller2019]. In this study, the (neurophysiological) differences in P1 amplitudes between objects with in-depth versus minimal knowledge were correlated with (behavioral) differences in their detection rate during the attentional blink. This can be taken as further evidence for an influence of semantic knowledge on the early visual perception of unfamiliar objects.

A complementary approach has been taken in a recent study investigating the ERPs in response to familiar (rather than unfamiliar) objects which were rendered difficult to recognize by converting them to two-tone, "Mooney" images [@samaha2018, Experiment 4]. In this experiment, participants were trained with meaningful verbal cues, telling them what kind of objects they should look for in the images, or with a non-meaningful perceptual task, familiarizing them with the images but not with their semantic content. When the EEG was measured in a delayed matching task later on, the two types of training led to differential effects, again suggesting a modulation of lower-level visual perception: Over left posterior electrodes, P1 amplitudes were larger and alpha-band power was higher for meaning-trained images than for perceptually-trained images. As before, this effect of perceiving objects in a semantically informed way seems to be behaviorally relevant, as participants tended respond faster and more accurately to meaning-trained as compared to perceptually-trained images in the matching task. This alternative approach further corroborates an early influence of semantic information on the visual perception of objects.

All the previously mentioned studies have in common that they investigated the effects of semantic knowledge on the visual perception of objects only *after* an extensive learning phase had taken place [@abdelrahman2008; @gauthier2003; @maier2014; @maier2019; @rossion2002; @rossion2004; @samaha2018; @weller2019]. Participants performed one or multiple training sessions during which they encountered each object together with its respective label or description multiple times. The EEG was usually not measured---or at least not analyzed and reported---during these training sessions, which sometimes took place on a separate day [@abdelrahman2008; @maier2014; @maier2019] or were spread out across multiple days or weeks [@rossion2002; @rossion2004]. While experimental designs with such substantive training maximize the chances of detecting even subtle top-down effects of the knowledge acquired, they leave at least three conceptual questions unresolved.

The first question is if we can detect any electrophysiological correlates of *semantic insight*, that is, the critical presentation during which an understanding of the previously unfamiliar object is happening (instead of asking if this understanding leads to differential effects in an orthogonal task later on). The second question is how much learning is actually necessary before reliable top-down effects of knowledge on ERP amplitudes can be obtained: Does it actually take dozens of repetitions per object or may a single exposure to the object together with the relevant semantic information be enough? The third and closely related question is about the nature of the effects of knowledge on perception: Are they reflecting genuine top-down effects of the semantic system altering perception online (while the object is being perceived), or do they merely reflect the (re-)activation of stored representations of the objects which have been altered over the course of the learning process? By measuring the ERPs before, during, and after participants received semantic hints about unfamiliar objects, we were able to provide tentative answers to all three of these questions.

In the two experiments subsequently reported, participants were presented with real-world objects that were presumed to be unfamiliar to most of them. They first viewed each of these objects without any semantic information. This first part served as a naive baseline to rule out that ERPs would differ in response to the objects based solely on low-level visual differences. Next, participants viewed each unfamiliar object for a second time, now preceded by verbal keywords. These keywords could either be matching the typical function of the object, thus making it possible for participants to understand what kind of object they were viewing, or they could be non-matching by describing the function of a different object, thus keeping the perception of the object semantically naive. During this second part, we were able to measure the online influence of semantic insight on object-evoked ERPs as it happened. Finally, the objects were presented for a third time (without keywords, as in the first part) to investigate downstream effects of having acquired semantic knowledge about them---thereby mimicking previous studies [e.g., @abdelrahman2008; @samaha2018; @weller2019]. In all three parts, we examined the influence of semantic information on ERPs associated with lower-level visual perception (P1 component, 100--150 ms), higher-level visual perception (N170 component, 150--200 ms), and semantic processing (N400 component, 400--700 ms).

# Experiment 1

## Method

### Participants

Participants for Experiment 1 were 24 German native speakers (13 female, 11 male) with a mean age of 24 years (range 18 to 31) and no history of psychological disorder or treatment. No a priori power analysis was carried out and the sample size was chosen in line with previous EEG studies on object processing in our lab. All participants were right-handed according to the Edinburgh inventory [@oldfield1971] and reported normal or corrected-to-normal vision. They gave written informed consent before starting the experiment and received a compensation of €8 per hour for participating.

### Materials

Stimuli for Experiments 1 and 2 consisted of 240 grayscale photographs of real-world objects, 120 of which were well-known everyday objects (e.g., a bicycle, a toothbrush), serving as filler stimuli of no interest, whereas the other 120 were rare objects presumed to be unfamiliar to the majority of participants (e.g., a galvanometer, an udu drum). A list of these unfamiliar objects can be found in Appendix A. All stimuli were presented on a light blue background with a size of 207 × 207 pixels on a 19-inch LCD monitor with a resolution of 1,280 × 1,024 pixels and a refresh rate of 75 Hz. At a standardized viewing distance of 90 cm, the images of the objects subtended approximately 3.9 degrees of participants' horizontal and vertical visual angle.

For each unfamiliar object, a pair of keywords---a noun and a verb---was selected, describing the typical function or use of the object in a way that could typically be related to its visual features and their configuration (e.g., voltage, measuring; clay pot, drumming). As our central experimental manipulation, the presentation of half of the objects was preceded by keywords that correctly matched their respective function, whereas the presentation of the other half of the objects was preceded by non-matching keywords belonging to one of the other objects. The matching keywords were expected to induce semantically informed perception (i.e., participants suddenly understanding what kind of object they were viewing), whereas the non-matching keywords were expected to hamper such an understanding and keep the perception of the object semantically naive. All participants saw each unfamiliar object with only one type of keywords (matching or non-matching). This assignment of keywords to objects was counterbalanced across participants so that each object was presented with matching keywords (leading to semantically informed perception) and non-matching keywords (leading to naive perception) to an equal number of participants. The experiment was programmed and displayed using Presentation® software (Neurobehavioral Systems, Inc., Berkeley, CA, www.neurobs.com).

### Procedure

Each experimental session consisted of three parts (see Figure \@ref(fig:exp1-plot)A). In the *pre-insight* part, after written informed consent had been obtained and the EEG had been prepared, all 240 familiar and unfamiliar objects were presented once in random order and without any keywords. Each trial consisted of a fixation cross presented in the middle of the screen for 0.5 s, followed by the presentation of the object until participants made a response or until a time out after 3 s. The inter-trial interval until the presentation of the next fixation cross was 0.5 s and participants took a self-timed break after each block of 60 objects. The task, which was kept the same across all three parts, was to classify each object using one of four response alternatives: (a) "I know what this is or have a strong assumption," (b) "I have an assumption what this is," (c) "I have rather no assumption what this is," or (d) "I don't know what this is and have no assumption." Participants were asked to respond as quickly and as accurately as possible by pressing one out of four buttons with the index or middle finger of their left or right hand, respectively. The mapping of the rating scale to the four buttons (left to right or right to left) was counterbalanced across participants.

```{python def_functions, eval=rerun_preproc}
# Import libraries
import mne
import numpy as np

# Define function for preprocessing a single participant
def preproc(
    vhdr_fname,
    metadata,
    n_components,
    method,
    l_freq,
    h_freq,
    event_id,
    tmin,
    tmax,
    baseline,
    reject,
):
    # Read EEG data in microvolts
    raw = mne.io.read_raw_brainvision(vhdr_fname, scale=1e6, preload=True)
    # Create virtual EOG channels
    raw = mne.set_bipolar_reference(raw, "Auge_u", "Fp1", ch_name="VEOG", drop_refs=False)
    raw = mne.set_bipolar_reference(raw, "F9", "F10", ch_name="HEOG", drop_refs=False)
    raw.set_channel_types(mapping={"VEOG": "eog", "HEOG": "eog"})
    # Add EasyCap electrode layout, removing any excessive channels
    montage = mne.channels.make_standard_montage("easycap-M1")
    raw.drop_channels(list(set(raw.ch_names) - set(montage.ch_names) - set(["VEOG", "HEOG"])))
    raw.set_montage(montage=montage)
    # Re-reference to common average
    raw, _ = mne.set_eeg_reference(raw, "average")
    # Run ICA on a copy of the data
    filt_raw = raw.copy()
    filt_raw.load_data().filter(l_freq=1, h_freq=None)
    ica = mne.preprocessing.ICA(n_components=n_components, random_state=12345, method=method)
    ica.fit(filt_raw)
    # Remove bad components based on correlations with EOG
    eog_indices, eog_scores = ica.find_bads_eog(raw)
    ica.exclude = eog_indices
    raw = ica.apply(raw)
    # Apply band-pass filter
    raw = raw.filter(l_freq=l_freq, h_freq=h_freq)
    # Epoching including baseline correction
    events, _ = mne.events_from_annotations(raw, verbose=False)
    epochs = mne.Epochs(
        raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=baseline, preload=True
    )
    # Add behavioral data
    epochs.metadata = metadata
    # Reject bad epochs
    epochs = epochs.drop_bad(reject=reject)
    return epochs


# Define function for preprocessing a list of participants
def preproc_all(
    vhdr_fnames,
    metadatas,
    output_fname,
    n_components,
    method,
    l_freq,
    h_freq,
    event_id,
    tmin,
    tmax,
    baseline,
    reject,
):
    # Do the actual preprocessing
    epochs = [
        preproc(
            vhdr_fname=vhdr_fname,
            metadata=metadata,
            n_components=n_components,
            method=method,
            l_freq=l_freq,
            h_freq=h_freq,
            event_id=event_id,
            tmin=tmin,
            tmax=tmax,
            baseline=baseline,
            reject=reject,
        )
        for vhdr_fname, metadata in zip(vhdr_fnames, metadatas)
    ]
    # Concatenate all participants and save
    epochs = mne.epochs.concatenate_epochs(epochs)
    epochs.save(output_fname, overwrite=True)
    # Return the epochs without loading the actual data
    epochs = mne.read_epochs(output_fname, preload=False)
    return epochs


# Define function to compute averaged waveforms by part and condition
def compute_evokeds(epochs_fname, grand=True, data_only=True):
    # Read preprocessed epochs
    epochs = mne.read_epochs(epochs_fname)
    # Define lists of participants, parts, and conditions
    ps = epochs.metadata["participant"].unique()
    pts = ["I", "II", "III"]
    cns = ["Informed", "Naive"]
    # Define function to compute a single by-participant waveform
    def compute_evoked(epochs, pt, cn, p):
        query = 'participant == "' + p + '" and part == "' + pt + '" and condition == "' + cn + '"'
        return epochs[query].average()

    # Create a dict with by-participant waveforms nested in conditions nested in parts
    avgs = {pt: {cn: [compute_evoked(epochs, pt, cn, p) for p in ps] for cn in cns} for pt in pts}
    # If needed, compute grand-averaged waveforms across participants
    if grand:
        avgs = {pt: {cn: mne.grand_average(avgs[pt][cn]) for cn in cns} for pt in pts}
    # If needed, extract only the data from the evoked MNE objects
    if data_only:
        if grand:
            avgs = {pt: {cn: avgs[pt][cn].data for cn in cns} for pt in pts}
        else:
            ps_num = range(0, len(ps))
            avgs = {pt: {cn: [avgs[pt][cn][p].data for p in ps_num] for cn in cns} for pt in pts}
    return avgs


# Define function to compute single-trial mean ERP amplitudes from epochs
def compute_erps(epochs_fname, name, tmin, tmax, roi):
    # Read preprocessed epochs
    epochs = mne.read_epochs(epochs_fname)
    # Average across electrodes in the ROI
    roi_idx = mne.pick_channels(epochs.ch_names, roi)
    epochs_roi = mne.channels.combine_channels(epochs, dict(roi=roi_idx))
    # Average across samples in the time window
    tmin_s, tmax_s = tmin / 1000, tmax / 1000
    times_idx = np.where(np.logical_and(epochs.times >= tmin_s, epochs.times <= tmax_s))
    erps = epochs_roi.get_data()[:, 0, times_idx].mean(axis=(1, 2))
    return erps


```

```{r exp1-preproc, eval=rerun_preproc}
# Define function for reading and formatting the behavioral data
read_behav <- function(txt_fname) {
  # Read the log file
  dat <- suppressWarnings(read_tsv(txt_fname, col_types = cols()))
  # Rename columns for participant and item IDs
  dat %<>% rename(participant = VPNummer, item = StimID) %>%
    # Add a column for the part of the experiment
    mutate(part = case_when(
      Wdh %in% c(211, 231) ~ "I",
      Wdh %in% c(212, 232) ~ "II",
      Wdh %in% c(213, 233) ~ "III",
      Wdh == 234 ~ "IV"
    ) %>% as_factor()) %>%
    # Remove filler stimuli (i.e.,well-known objects)
    filter(bek_unbek != "bekannt")
  # Assign stimuli to conditions based on keywords and button presses
  excl_known <- dat %>%
    filter(part == "I" & Tastencode %in% c(201, 251)) %>%
    pull(item)
  excl_informed <- dat %>%
    filter(part == "II" & grepl("richtig", Bed) & !Tastencode %in% c(201, 202, 251, 252)) %>%
    pull(item)
  excl_naive <- dat %>%
    filter(part == "II" & grepl("falsch", Bed) & !Tastencode %in% c(203, 204, 253, 254)) %>%
    pull(item)
  informed <- dat %>%
    filter(part == "II" & grepl("richtig", Bed) & Tastencode %in% c(201, 202, 251, 252)) %>%
    pull(item)
  naive <- dat %>%
    filter(part == "II" & grepl("falsch", Bed) & Tastencode %in% c(203, 204, 253, 254)) %>%
    pull(item)
  # Create a new condition for the experimental condition
  dat %<>% mutate(condition = case_when(
    item %in% excl_known ~ "Excl_known",
    item %in% excl_informed ~ "Excl_informed",
    item %in% excl_naive ~ "Excl_naive",
    item %in% informed ~ "Informed",
    item %in% naive ~ "Naive"
  ) %>% factor(levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")))
  # In Experiment 3, we need an additional column for position (upright vs. inverted)
  if ("richtig_inv" %in% dat$Bed) {
    dat %<>% mutate(position = as_factor(ifelse(grepl("inv", Bed), "Inverted", "Upright"))) %>%
      # Return only relevant columns
      select(part, position, condition, participant, item, RT)
  } else {
    # Return only relevant columns
    dat %<>% select(part, condition, participant, item, RT)
  }
  return(dat)
}

# Read behavioral data from two batches of participants
paste0("data/rt/exp", 1:2) %>%
  map(list.files, pattern = ".txt", full.names = TRUE) %>%
  map(map, read_behav) -> behav

# Avoid duplicate participant labels across the two batches
behav[[2]] %<>% map(mutate, participant = paste0(participant, "_2"))

# Merge into a single list
behav %<>% flatten()

# Define global parameters for EEG preprocessing
preproc_params <- list(
  # Number of ICA components
  n_components = as.integer(15),
  # ICA methods
  method = "fastica",
  # Lower edge of pass-band filter
  l_freq = 0.1,
  # Upper edge of pass-band filter
  h_freq = 30,
  # Event trigger codes
  event_id = dict(match = as.integer(221), mismatch = as.integer(222)),
  # Beginning of epochs in s
  tmin = -0.5,
  # End of epochs in s
  tmax = 1.498,
  # Baseline interval in s
  baseline = tuple(-0.2, 0),
  # Artifact rejection threshold in microvolts
  reject = dict(eeg = 200)
)

# Preprocess the EEG for Experiment 1 in Python
paste0("data/eeg/exp", 1:2) %>%
  list.files(pattern = ".vhdr", full.names = TRUE) %>%
  exec(
    .fn = py$preproc_all,
    vhdr_fnames = .,
    metadatas = behav,
    output_fname = fnames$epochs,
    !!!preproc_params
  ) -> epochs

# Compute mean ERP amplitudes and bind to the behavioral data
pmap_dfc(comps, py$compute_erps, epochs_fname = fnames$epochs) %>%
  set_names(comps$name) %>%
  bind_cols(epochs$metadata, .) %T>%
  write_tsv(fnames$trials) -> aha

# Compute grand averages per part and condition
py$compute_evokeds(epochs_fname = fnames$epochs, grand = TRUE, data_only = TRUE) %>%
  saveRDS(fnames$gavgs)

# Backup number of stimuli assigned to each condition for all participants
map(behav, function(x) table(x$condition) / 3) %>%
  bind_rows() %>%
  mutate(Participant = unique(aha$participant), .before = 1) %>%
  write_csv(fnames$conditions)

# Get standard EasyCap electrode positions and project to 2D space
py$mne$channels$`_standard_montage_utils`$MONTAGE_PATH %>%
  list.files(pattern = "easycap-M1.txt", full.names = TRUE) %>%
  read_tsv(col_types = cols()) %>%
  rename(electrode = Site) %>%
  inner_join(tibble(electrode = epochs$ch_names), ., by = "electrode") %>%
  mutate(
    x = eegUtils:::deg2rad(Theta) * cos(eegUtils:::deg2rad(Phi)),
    y = eegUtils:::deg2rad(Theta) * sin(eegUtils:::deg2rad(Phi)),
  ) %>%
  write_csv(fnames$montage)
```

```{r exp1-models, eval=rerun_models}
# Load preprocessed single-trial data
read_tsv("analysis/export/aha-trials.csv", col_types = cols()) %>%
  # Factorize columns
  mutate(
    part = factor(part, levels = c("I", "II", "III")),
    condition = factor(condition, levels = c(
      "Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known"
    )),
    participant = factor(participant),
    item = factor(item)
  ) %>%
  # Remove any trials excluded from conditions
  filter(condition %in% c("Informed", "Naive")) %>%
  droplevels() -> aha

# Define contrast coding for condition (informed-naive) and part (2-1, 3-1)
contrasts_condition <- t(cbind(
  c("Informed" = 1, "Naive" = -1)
))
contrasts_part <- t(cbind(
  c("I" = -1, "II" = 1, "III" = 0),
  c("I" = 0, "II" = -1, "III" = 1)
))

# Apply these contrasts to the factors in the data
contrasts(aha$condition) <- MASS::ginv(contrasts_condition)
contrasts(aha$part) <- MASS::ginv(contrasts_part)

# Specify model formula and follow-up contrasts
form <- erp ~ part * condition +
  (part * condition | participant) +
  (part * condition | item)
specs <- pairwise ~ condition | part

# Calculate linear mixed-effects models and contrasts
map(comps$name, function(comp, formula = form, data = aha, specs = specs) {
  require(afex)
  require(emmeans)
  data %<>% rename(erp = all_of(comp))
  mod <- mixed(
    formula,
    data,
    method = "S",
    check_contrasts = FALSE,
    all_fit = TRUE
  )
  em <- emmeans(mod, specs = specs, infer = TRUE, data = data)
  modlist <- list(
    model = mod,
    anova = anova(mod),
    summary = summary(mod),
    means = as.data.frame(em$means),
    contrasts = as.data.frame(em$contrasts)
  )
}) %>%
  set_names(comps$name) %T>%
  saveRDS("analysis/export/aha_models_freq.RDS") -> mods_freq
```

```{r exp1-import}
# Load average number of objects per condition
(read_tsv("analysis/export/exp1-objects.tsv", col_types = cols()) %>%
   summarise(across(.cols = -Participant, .fns = mean)) %>%
   slice(1) %>% unlist() -> objects_exp1)

# Load preprocessed single-trial data
read_tsv("analysis/export/exp1-data.tsv", col_types = cols()) %>%
  # Factorize columns
  mutate(
    part = factor(part, levels = c("I", "II", "III")),
    condition = factor(condition, levels = c(
      "Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")
    ),
    participant = factor(participant),
    item = factor(item)
  ) -> aha

# Check number of rejected epochs
aha %>%
  group_by(participant) %>%
  tally() %>%
  mutate(n = 360 - n) %>%
  pull(n) -> rejected_exp1
mean(rejected_exp1)    # Mean
median(rejected_exp1)  # Median
range(rejected_exp1)   # Range

# Load grand-averaged ERPs and linear mixed models
evokeds_exp1 <- readRDS("analysis/export/exp1-gaverages.RDS")
models_exp1 <- readRDS("analysis/export/exp1-models.RDS")
```

```{r bayes, eval=FALSE}
library(brms)
options(mc.cores = 6)

data_joint_subs %>%
  filter(condition %in% c("Informed", "Naive")) %>%
  droplevels() %>%
  select(part, condition, participant, item, P1, N170, N400) %>%
  na.omit() %>%
  as.list() -> data_brm

contrasts_condition <- t(cbind(c("Informed" = 1, "Naive" = -1)))
contrasts_part <- t(cbind(c("I" = -1, "II" = 1, "III" = 0), c("I" = 0, "II" = -1, "III" = 1)))
contrasts(data_brm$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_brm$part) <- MASS::ginv(contrasts_part)

formula <- P1 ~ part / condition + (part / condition | participant) + (part / condition | item)
prior <- set_prior("cauchy(0, 0.707)", class = "b")

mod_brm <- brm(
  formula = formula,
  data = data_brm,
  family = gaussian(),
  prior = prior,
  sample_prior = "yes",
  save_pars = save_pars(all = TRUE),
  chains = 4,
  iter = 20000,
  warmup = 2000,
  cores = 4,
  control = list(adapt_delta = 0.9)
)

# mod_prior <- brm(formula = formula, data = data_brm, prior = prior, sample_prior = "only")
# prior_summary(mod_prior)
# mcmc_plot(mod_prior)
#
# mod_post <- update(mod_prior, sample_prior = "yes")
# summary(mod_post)
# mcmc_plot(mod_post, pars = "b")
# plot(mod_post)

hypothesis <- paste0("part", c("I", "II", "III"), ":condition1 = 0")
bf_brms <- hypothesis(mod_post, hypothesis = hypothesis)
bf_brms$hypothesis$bf10 <- 1 / bf_brms$hypothesis$Evid.Ratio
bf_brms

bf_bayestest <- bayestestR::bayesfactor(mod_post)
bf_bayestest

mod_brm_s <- brm(
  bf(P1 ~ part / condition + (part / condition | participant) + (part / condition | item),
     sigma ~ 1 + (1 | participant)),
  prior = c(
    prior(cauchy(0, 0.707), class = b),
    prior(student_t(3, 0, 5.7), class = Intercept, dpar = sigma),
    prior(normal(0, 5), class = sd, group = participant, dpar = sigma)),
  data = data_brm,
  family = gaussian(),
  sample_prior = "yes",
  save_pars = save_pars(all = TRUE),
  chains = 4,
  iter = 20000,
  warmup = 2000,
  cores = 4,
  control = list(adapt_delta = 0.9)
)
```

```{python tfr, eval=FALSE}
import mne
import numpy as np
#import matplotlib.pyplot as plt

# epochs = preproc(
#   vhdr_fname="data/exp1/eeg/Vp0004.vhdr",
#   metadata=r.aha.query("participant == 'VP_04'"),
#   n_components=15,
#   method="fastica",
#   l_freq=None,
#   h_freq=None,
#   event_id=dict(match=221, mismatch=222),
#   tmin=-0.5,
#   tmax=1.498,
#   baseline=(-0.2, 0),
#   reject=dict(eeg=200)
# )
# epochs.save("analysis/export/vp04-epo.fif")

epochs = mne.read_epochs("analysis/export/unf-exp1-epo.fif")

evokeds = compute_evokeds(epochs_c, grand=True, data_only=False)
evokeds_i = evokeds["II"]["Informed"]
evokeds_n = evokeds["II"]["Naive"]

freqs = np.logspace(*np.log10([6, 35]), num=8)
n_cycles = freqs / 2.
power_i = mne.time_frequency.tfr_morlet(evokeds_i, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True, n_jobs=1)
power_n = mne.time_frequency.tfr_morlet(evokeds_n, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True, n_jobs=1)
power_i = power_i.apply_baseline(mode='mean', baseline=(-.400, -.100))
power_n = power_n.apply_baseline(mode='mean', baseline=(-.400, -.100))

power_d = power_i.copy()
power_d.data = power_i.data - power_n.data


power_d.plot()


# epochs.resample(250., npad='auto')

roi = r.comps["roi"][0] + r.comps["roi"][1]
idxs = np.where(np.in1d(epochs.ch_names, roi))

epochs_i = epochs["part == 'II' & condition == 'Informed'"].pick_channels(roi)
epochs_n = epochs["part == 'II' & condition == 'Naive'"].pick_channels(roi)

epochs_c = mne.channels.combine_channels(epochs, groups=dict(avg=range(62)))

epochs_c.metadata = epochs.metadata
epochs_s = epochs_c["part == 'II'"]
# epochs_i = epochs_s["condition == 'Informed'"]
# epochs_n = epochs_s["condition == 'Naive'"]

# power_i = mne.time_frequency.tfr_morlet(epochs_i, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True, n_jobs=1)
# power_n = mne.time_frequency.tfr_morlet(epochs_n, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True, n_jobs=1)
# power_d = power_i - power_n
# 
# power_d.plot([0], baseline=(-0.5, 0), mode='percent', vmin=-1, vmax=1)

# define frequencies of interest (log-spaced)
#freqs = np.logspace(*np.log10([6, 35]), num=8)
freqs = np.arange(3, 50, 1.6)
#n_cycles = freqs / 2.  # different number of cycle per frequency
n_cycles = np.linspace(3, 10, len(freqs))
power = mne.time_frequency.tfr_morlet(epochs_c, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=True, n_jobs=1)

power.plot([0], baseline=(-0.5, 0), mode='percent')

power.plot_topo(baseline=(-0.5, 0), mode='logratio')
power.plot_topomap(ch_type='eeg', tmin=0.15, tmax=0.2, fmin=8, fmax=12, baseline=(-0.5, 0), mode='logratio', title='Alpha')
power.plot_topomap(ch_type='eeg', tmin=0.15, tmax=0.2, fmin=13, fmax=25, baseline=(-0.5, 0), mode='logratio', title='Beta')


import numpy as np
import matplotlib.pyplot as plt
from mne.baseline import rescale
from mne.stats import bootstrap_confidence_interval
iter_freqs = [
    ('Theta', 4, 7),
    ('Alpha', 8, 12),
    ('Beta', 13, 25),
    ('Gamma', 30, 45)
]
raw_fname = "data/exp1/EEG/Vp0004.vhdr"
tmin, tmax = -0.5, 1.498
baseline = None
frequency_map = list()
event_id = dict(match=222)
for band, fmin, fmax in iter_freqs:
    # (re)load the data to save memory
    raw = mne.io.read_raw_brainvision(raw_fname, scale=1e6, preload=True)
    raw.filter(fmin, fmax, n_jobs=1,  # use more jobs to speed up.
               l_trans_bandwidth=1,  # make sure filter params are the same
               h_trans_bandwidth=1)  # in each band and skip "auto" option.
    events, _ = mne.events_from_annotations(raw, verbose=False)
    epochs = mne.Epochs(raw, events, event_id, tmin, tmax, baseline=baseline,
                        reject=dict(eeg=200), preload=True)
    # remove evoked response
    epochs.subtract_evoked()
    # get analytic signal (envelope)
    epochs.apply_hilbert(envelope=True)
    frequency_map.append(((band, fmin, fmax), epochs.average()))
    del epochs
del raw
# Helper function for plotting spread
def stat_fun(x):
    """Return sum of squares."""
    return np.sum(x ** 2, axis=0)
# Plot
fig, axes = plt.subplots(4, 1, figsize=(10, 7), sharex=True, sharey=True)
colors = plt.get_cmap('winter_r')(np.linspace(0, 1, 4))
for ((freq_name, fmin, fmax), average), color, ax in zip(
        frequency_map, colors, axes.ravel()[::-1]):
    times = average.times * 1e3
    gfp = np.sum(average.data ** 2, axis=0)
    gfp = mne.baseline.rescale(gfp, times, baseline=(None, 0))
    ax.plot(times, gfp, label=freq_name, color=color, linewidth=2.5)
    ax.axhline(0, linestyle='--', color='grey', linewidth=2)
    ci_low, ci_up = bootstrap_confidence_interval(average.data, random_state=0,
                                                  stat_fun=stat_fun)
    ci_low = rescale(ci_low, average.times, baseline=(None, 0))
    ci_up = rescale(ci_up, average.times, baseline=(None, 0))
    ax.fill_between(times, gfp + ci_up, gfp - ci_low, color=color, alpha=0.3)
    ax.grid(True)
    ax.set_ylabel('GFP')
    ax.annotate('%s (%d-%dHz)' % (freq_name, fmin, fmax),
                xy=(0.95, 0.8),
                horizontalalignment='right',
                xycoords='axes fraction')
axes.ravel()[-1].set_xlabel('Time [ms]')
plt.show()
```

```{r exp1-plot, include=TRUE, fig.height=11, fig.cap = "(ref:figure-1-caption)"}
# Load sample time points and electrode montage
montage <- read_tsv("analysis/export/montage.tsv", col_types = cols())

# Create plot for Experiment 1
plot_grid(
  plot_trials(),
  plot_erps(comps = comps, evokeds = evokeds_exp1, models = models_exp1, montage = montage),
  nrow = 2, rel_heights = c(5, 6), labels = c("A", "B"), label_fontfamily = "Helvetica"
) + # Add some lines (to separate the parts) and the legend
  annotate(geom = "segment", x = 0.328, xend = 0.328, y = -Inf, yend = 0.53, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.672, y = -Inf, yend = 0.53, linetype = "dashed") +
  annotate(geom = "segment", x = 0.328, xend = 0.28, y = 0.53, yend = 0.58, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.72, y = 0.53, yend = 0.58, linetype = "dashed") +  
  annotate(geom = "segment", x = 0.28, xend = 0.28, y = 0.58, yend = Inf, linetype = "dashed") +
  annotate(geom = "segment", x = 0.72, xend = 0.72, y = 0.58, yend = Inf, linetype = "dashed") +
  draw_plot(plot_legends_erps(direction = "vertical"), x = 0.405, y = 0.422, width = 1, height = 1)
```

(ref:figure-1-caption) Procedure and Results of Experiment 1\smallskip

<!--

The papaja package does not yet deal with the placement of figure captions above rather than below the figures. Therefore, we manually need to post-process the .tex file, moving the caption{} above includegraphics{} and copying the figure note manually below includegraphics{}:

\bigskip\small\textit{Note.} 
(A) In the pre-insight part, participants were presented with 120 unfamiliar objects and indicated whether they knew what kind of object they were viewing. In the insight part, half of these objects were presented with matching keywords (in purple color for illustration), leading to semantically informed perception, and the other half with non-matching keywords (in petrol color for illustration), leading to naive perception. In the post-insight part, the same objects were presented again without the keywords. (B) ERP waveforms and scalp topographies are shown for objects with semantically informed versus naive perception within the three different parts. Semantically informed perception was associated with significantly more negative amplitudes in the N170 component in the insight part, significantly less negative amplitudes in the N400 component in the insight and post-insight parts, and significantly more positive amplitudes in the P1 component in the post-insight part. Ampl. = amplitude.\newline
*\textit{p} \textless{} .05. **\textit{p} \textless{} .01. ***\textit{p} \textless{} .001.

-->

In the *insight* part, the 120 unfamiliar objects were presented for a second time, now preceded either by matching keywords (leading to semantically informed perception) or by non-matching keywords (leading to naive perception). Each trial consisted of a fixation cross presented for 0.5 s, followed by the presentation of the keywords for 2.5 s. Then, an asterisk was presented in the middle of the screen for another 0.5 s, followed by the presentation of the object until a response was made or until a time out after 3 s. The objects were presented in blocks of 30 trials so that within each block (a) there were 15 objects from each of the two experimental conditions and (b) objects were heterogeneous in terms of their shape, visual complexity, and functional category (e.g., medical devices, musical instruments).

Finally, in the *post-insight* part, the unfamiliar objects were presented for a third time with an identical trial structure as in the pre-insight part, that is, without any keywords. Note that the insight and post-insight parts were presented in an interleaved fashion so that after the presentation of one block of 30 objects in the insight part (with keywords), participants took a self-timed break and continued with the same block of 30 objects in the post-insight part (without keywords) before moving on to the next block consisting of 30 different objects. They continued like this until all four blocks were completed in both parts. In total, the experiment consisted of 480 trials (120 familiar objects in the pre-insight part and 120 unfamiliar objects in the pre-insight, insight, and post-insight parts). It took participants approximately 35 minutes to complete.

### EEG Recording and Preprocessing

The continuous EEG was recorded from 62 Ag/AgCl scalp electrodes placed according to the extended 10--20 system [@americanelectroencephalographicsociety1991] and referenced online to an external electrode placed on the left mastoid (M1). Two additional external electrodes were placed on the right mastoid (M2) and below the left eye (IO1), respectively. During the recording, electrode impedance was kept below 5 kΩ. An online band-pass filter with a high-pass time-constant of 10 s (0.016 Hz) and a low-pass cutoff frequency of 1000 Hz was applied before digitizing the signal at a sampling rate of 500 Hz.

Offline, the data were preprocessed using the MNE software [Version 0.21.0\; @gramfort2013] in Python [Version 3.8.5\; @vanrossum2009]. First, all scalp electrodes were re-referenced to the common average. Next, artifacts resulting from blinks and eye movements were removed using independent component analysis (ICA). The first 15 components were extracted by the FastICA algorithm [@hyvärinen1999] after temporarily low-pass filtering the data at 1 Hz. Any components showing substantive correlations with either of two virtual EOG channels (VEOG: IO1 - Fp1, HEOG: F9 - F10) were removed automatically using the *find\_bads\_eog* function. After artifact correction, a zero-phase, non-causal FIR filter with a lower pass-band edge at 0.1 Hz (transition bandwidth: 0.1 Hz) and an upper pass-band edge at 30 Hz (transition bandwidth: 7.5 Hz) was applied. Next, the continuous EEG was epoched into segments of 2 s, starting 500 ms before the onset of the visual presentation of each unfamiliar object. The epochs were baseline-corrected by subtracting the average voltage during the 200 ms before stimulus onset. Epochs containing artifacts despite ICA, defined as peak-to-peak amplitudes exceeding 200 µV, were removed from further analysis. This led to the exclusion of an average of `r format(round(mean(rejected_exp1), 1), nsmall = 1)` trials per participant (= `r scales::percent(mean(rejected_exp1)/360, accuracy = 0.1)`; range `r min(rejected_exp1)` to `r max(rejected_exp1)` trials). Single-trial event-related potentials were computed as the mean amplitude across time windows and regions of interests (ROIs) defined a priori, namely 100--150 ms after object onset at electrodes PO3, PO4, POz, O1, O2, and Oz for the P1 component, 150--200 ms after object onset at electrodes P7, P8, PO7, PO8, PO9, and PO10 for the N170 component, and 400--700 ms after object onset at electrodes C1, C2, Cz, CP1, CP2, and CPz for the N400 component (for the positions of these electrodes on the scalp, see the topographies in Figure \@ref(fig:exp1-plot)B).

### Statistical Analysis

First, because we were interested in the effects of knowledge on perceiving *unfamiliar* objects only, we excluded objects from all further analyses when participants classified them as being known in the pre-insight part (i.e., before any keywords were presented). This led to the exclusion of an average of `r format(round(objects_exp1["Excl_known"], 1), nsmall = 1)` objects per participant (= `r scales::percent(objects_exp1["Excl_known"]/120, accuracy = 0.1)` of all unfamiliar objects). Next, to clearly delineate semantically informed and naive perception, the assignment of all other objects to one of these two conditions for statistical analysis was co-determined by our experimental manipulation (matching versus non-matching keywords in the insight part) and the behavioral responses of the participants themselves (see Figure \@ref(fig:exp1-plot)A). Objects were assigned to the semantically informed condition only if they were presented with matching keywords *and* if participants indicated knowing what the object was or having an assumption. This was the case for an average of `r format(round(objects_exp1["Informed"], 1), nsmall = 1)` objects per participant (= `r scales::percent(objects_exp1["Informed"]/60, accuracy = 0.1)` of objects presented with matching keywords). Complementarily, objects were assigned to the naive condition only if they were presented with non-matching keywords *and* if participants indicated not knowing what the object was or having rather no assumption. This was the case for an average of `r format(round(objects_exp1["Naive"], 1), nsmall = 1)` objects per participant (= `r scales::percent(objects_exp1["Naive"]/60, accuracy = 0.1)` of objects presented with non-matching keywords). Although this assignment was based on the manipulation and responses in the second part---when insight was thought to occur---the same assignment was used to analyze the data from the other two parts. This allowed us to test, on the one hand, if the objects from both conditions differed in important aspects even before any keywords were presented (pre-insight part) and, on the other hand, if the semantic understanding acquired in the insight part had any down-stream effects on the subsequent perception of the objects (post-insight part).

The event-related potentials in response to objects from both conditions and all three parts were analyzed on the single trial level using linear mixed-effects regression models [@baayen2008; @frömer2018]. For the purpose of the present study, these models have at least three desirable properties compared to traditional approaches such as analyses of variance (ANOVAs) performed on by-participant averages. First, they can leverage all of the information that is present in the data set on the single trial level, some of which would be lost when computing averages. Second, they can account simultaneously for the non-independence of data points coming from the same participant and/or from the same item. In contrast, the neglect of the item as a random variable in by-participant ANOVAs can lead to anti-conservative test statistics and strictly does not allow for inferences beyond the stimulus set under study [@bürki2018; @judd2012]. Finally, mixed-effects models can flexibly deal with unbalanced designs in which the number of trials differs across (combinations of) conditions. Such a situation is inevitable when the assignment of trials to conditions is co-determined by the experimental manipulation and the behavior of participants in response to this manipulation [e.g., @fröber2017].

Three separate models were computed predicting P1, N170, and N400 mean amplitudes, respectively. All models included three fixed effects: (a) the part of the experiment, coded as a repeated contrast (i.e., subtracting the first from the second part and the second from the third part, the intercept being the grand mean across all three parts), (b) the condition of the object, coded as a scaled sum contrast (i.e., subtracting the naive condition from the semantically informed condition, the intercept being the grand mean across both conditions), and (c) the two-way interaction of part and condition. For details on these and other contrast coding schemes in linear (mixed-effects) models, please refer to @schad2020. To determine the random effects structure, we always started with a maximal model containing by-participant and by-item random intercepts and random slopes for all fixed effects [@barr2013]. We then performed a model selection algorithm as proposed by @matuschek2017 in order to increase statistical power and avoid overparameterization: Iteratively, each random effect was removed and the resulting, more parsimonious model was compared to the previous, more complex model by means of a likelihood ratio test. Only if the parsimonious model explained the data equally well as the complex model [determined by *p* ≥ .20\; @matuschek2017] did we leave the random effect out, otherwise it was kept in the final model (see Appendix B for the formulas after model selection). All models were computed in R [Version `r as.character(packageVersion("base"))`\; @R-base] using the lme4 package [Version `r as.character(packageVersion("lme4"))`\; @R-lme4]. The optimizer function *bobyqa* with 20,000 iterations was used for maximum likelihood estimation. The model selection algorithm via likelihood ratio tests was performed using the buildmer package [Version `r as.character(packageVersion("buildmer"))`\; @R-buildmer].

Finally, to answer our research question of whether or not semantically informed perception had an influence on the ERP components within each part, planned follow-up comparisons were calculated, contrasting the semantically informed against the naive condition within the pre-insight, \mbox{insight}, and post-insight parts. This was achieved using the emmeans package [Version `r as.character(packageVersion("emmeans"))`\; @R-emmeans]. All *p*-values were computed by approximating the relevant denominator degrees of freedom using Satterthwaite's method as implemented in the lmerTest package [Version `r as.character(packageVersion("lmerTest"))`\; @R-lmerTest].

The materials, single trial behavioral and ERP data, and all code for data analysis can be accessed via the Open Science Framework (<https://osf.io/uksbc/>). We are not able to share the raw EEG data openly because we did not obtain participants' informed consent to do so.

## Results

Single-trial ERPs were analyzed in response to unfamiliar objects before (pre-insight part), while (insight part), and after (post-insight part) participants obtained relevant semantic information about their function. In the insight part, half of the objects were preceded by matching keywords, fostering semantically informed perception. The other half were preceded by non-matching keywords, keeping the perception of the object semantically naive. The objects were analyzed according to this manipulation in combination with participants' self-report in the insight part (see Figure \@ref(fig:exp1-plot)A), thereby making sure that semantically informed and naive perception did indeed occur. The analysis focused on differences between these two conditions in the P1 component (100--150 ms) as an index of lower-level visual perception, the N170 component (150--200 ms) as index of higher-level visual perception, and the N400 component (400--700 ms) as an index of semantic processing.

```{r exp1-table, include=TRUE, results="asis"}
# Create table of models for Experiment 1
table_exp1 <- create_table(
  models = models_exp1,
  stub_anova = c("Part", "Condition", "Pt. × con."),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption = "Results of Linear Mixed-Effects Regression Models for Experiment 1\\smallskip",
  note = "Pt. = part, con. = condition, est. = estimate, CI = 95\\% confidence interval."
)
```

Averaged across conditions, P1, N170, and N400 amplitudes differed as a function of the part of the experiment, all *F*s ≥ `r table_exp1$anov["Part", "P1_f"]`, all *p*s `r table_exp1$anov["Part", "P1_p"]` (see Table \@ref(tab:exp1-table)). In addition, N400 amplitudes differed between the informed and the naive condition averaged across the three parts of the experiment, *F*`r table_exp1$anov["Condition", "N400_df"]` = `r table_exp1$anov["Condition", "N400_f"]`, *p* = `r table_exp1$anov["Condition", "N400_p"]`. Crucially, the part × condition interaction was significant in the N170 component, *F*`r table_exp1$anov["Pt. × con.", "N170_df"]` = `r table_exp1$anov["Pt. × con.", "N170_f"]`, *p* = `r table_exp1$anov["Pt. × con.", "N170_p"]`, and in the N400 component, *F*`r table_exp1$anov["Pt. × con.", "N400_df"]` = `r table_exp1$anov["Pt. × con.", "N400_f"]`, *p* `r table_exp1$anov["Pt. × con.", "N400_p"]`, while also showing a weak statistical trend in the P1 component, *F*`r table_exp1$anov["Pt. × con.", "P1_df"]` = `r table_exp1$anov["Pt. × con.", "P1_f"]`, *p* = `r table_exp1$anov["Pt. × con.", "P1_p"]`. To answer our main research question, we decomposed these interactions into the differences between the semantically informed condition and the naive condition within the three different parts of the experiment.

### ERPs Before Insight Was Occurring

In the pre-insight part, when objects were unfamiliar to participants and presented without keywords, no differences emerged between the semantically informed and the naive condition in the P1, N170, or N400 component, all *p*s ≥ `r table_exp1$conts["Pre-insight part", "N400_p"]` (see Table \@ref(tab:exp1-table) and Figure \@ref(fig:exp1-plot)B). On the one hand, this was to be expected given that the critical presentation of the keywords (leading to semantically informed vs. naive perception) had not yet taken place. On the other hand, the absence of reliable differences in this part can be taken as evidence---with the usual caveats when interpreting null effects---that any subsequent effect of the semantic information in the other two parts cannot be accounted for by visual differences between the objects in the two conditions. Although the presentation of matching or non-matching keywords for each object was counterbalanced across participants, the fact that different numbers of objects were assigned to the two conditions based on participants' self-report would have made it possible for such visual differences to emerge as a confounding factor. If they did, however, one would expect to detect these differences even before any keywords were presented, which was apparently not the case.

### ERPs While Insight Was Occurring

In the insight part, half of the unfamiliar objects were presented with matching keywords (for forming the semantically informed condition) and the other half were presented with non-matching keywords (for forming the naive condition). When semantic information informed the perception of the object, the amplitude of the N170 component was significantly enlarged (i.e., more negative), *b* = `r table_exp1$conts["Insight part", "N170_est"]` µV, *p* = `r table_exp1$conts["Insight part", "N170_p"]`, and the amplitude of the N400 component was significantly reduced (i.e., less negative), *b* = `r table_exp1$conts["Insight part", "N400_est"]` µV, *p* `r table_exp1$conts["Insight part", "N400_p"]`, compared to when the object was viewed naively without relevant semantic information. As in the pre-insight part, there were no reliable differences in the P1 component, *b* = `r table_exp1$conts["Insight part", "P1_est"]` µV, *p* = `r table_exp1$conts["Insight part", "P1_p"]`.

### ERPs After Insight Had Occurred

In the post-insight part, the unfamiliar objects were presented for a third time, again without the keywords (as in the pre-insight part). This allowed us to test if the semantic information had any lasting effects on the processing of the objects. As in the insight part, the N400 component remained significantly reduced during semantically informed as compared to naive perception, *b* = `r table_exp1$conts["Post-insight part", "N400_est"]` µV, *p* `r table_exp1$conts["Post-insight part", "N400_p"]`, whereas the effect in the N170 component did not reoccur, *b* = `r table_exp1$conts["Post-insight part", "N170_est"]` µV, *p* = `r table_exp1$conts["Post-insight part", "N170_p"]`. Instead, we now observed an even earlier modulation in the P1 component which was significantly enlarged (i.e., more positive) in response to objects for which semantically informed perception had taken place, *b* = `r table_exp1$conts["Post-insight part", "P1_est"]` µV, *p* = `r table_exp1$conts["Post-insight part", "P1_p"]`.

## Discussion

In Experiment 1, we measured the ERPs of participants viewing unfamiliar objects before (pre-insight part), while (insight part) and after (post-insight part) they were able to understand what kind of object they were seeing. To induce this semantically informed perception, half of the objects in the insight part were preceded by matching verbal keywords about the object's typical function or use, whereas the other half were preceded by non-matching keywords, serving as a naive baseline condition.

In the insight part, we found that semantically informed perception was associated with enlarged amplitudes in the N170 component (150--200 ms after object onset) and reduced amplitudes in the N400 component (400--700 ms). When the same objects were presented once more in the post-insight part, the reduction of the N400 component reoccurred and we also observed a modulation of the P1 component (100--150 ms), which was significantly larger for objects for which semantically informed perception had taken place.

Because of the novelty of our experimental paradigm---measuring the ERPs while rather than after participants received semantic information about unfamiliar objects---and because of the novelty of our findings, we ran a replication study with another sample of participants to assess the robustness of these effects.

# Experiment 2

## Method

### Participants

Participants for Experiment 2 were 24 German native speakers (15 female, 9 male) with a mean age of 26 years (range 19 to 29 years) who had not participated in Experiment 1. They had no history of psychological disorders or treatment, were right-handed, and reported normal or corrected-to-normal vision. They gave written informed consent before starting the experiment and received a compensation of €8 per hour for participating.

### Materials, Procedure, and Analysis

```{r exp2-preproc, eval=rerun_preproc}
# Read behavioral data for Experiment 2
list.files("data/exp2/RT", pattern = ".txt", full.names = TRUE) %>% map(read_behav) -> behav_exp2

# Preprocess the EEG for Experiment 2 in Python
map2(
  .x = list.files("data/exp2/EEG", pattern = ".vhdr", full.names = TRUE),
  .y = behav_exp2,
  ~ do.call(py$preproc, c(list(vhdr_fname = .x, metadata = .y), preproc_params))
) %>%
  # Concatenate epochs of all participants
  py$mne$epochs$concatenate_epochs() -> epochs_exp2

# Backup epochs to the export folder
epochs_exp2$save("analysis/export/exp2-epo.fif")

# Compute mean ERP amplitudes and bind to the behavioral data
pmap_dfc(comps, py$compute_erps, epochs = epochs_exp2) %>%
  set_names(comps$name) %>%
  bind_cols(as_tibble(epochs_exp2$metadata), .) %T>%
  write_tsv("analysis/export/exp2-data.tsv") -> data_exp2

# Compute grand averages per part and condition
py$compute_evokeds(epochs = epochs_exp2, grand = TRUE, data_only = TRUE) %>%
  saveRDS("analysis/export/exp2-gaverages.RDS")

# Backup number of stimuli assigned to each condition for all participants
map(behav_exp2, function(x) table(x$condition) / 3) %>%
  bind_rows() %>%
  mutate(Participant = unique(data_exp2$participant), .before = 1) %>%
  write_tsv("analysis/export/exp2-objects.tsv")

# Remove the epochs to free up memory
rm(epochs_exp2)
```

```{r exp2-models, eval=rerun_models}
# Load preprocessed single-trial data
read_tsv("analysis/export/exp2-data.tsv", col_types = cols()) %>%
  # Factorize columns
  mutate(
    part = factor(part, levels = c("I", "II", "III")),
    condition = factor(condition, levels = c(
      "Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")
    ),
    participant = factor(participant),
    item = factor(item)
  ) %>%
  # Remove any trials excluded from conditions
  filter(condition %in% c("Informed", "Naive")) %>%
  droplevels() -> data_exp2

# Apply contrasts to the factors in the data
contrasts(data_exp2$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_exp2$part) <- MASS::ginv(contrasts_part)

# Compute the models for Experiment 2
map(
  comps$name,
  compute_models,
  formula = formula_exp12,
  data = data_exp2,
  control = control,
  specs = specs_exp12
) %>%
  set_names(comps$name) %T>%
  saveRDS("analysis/export/exp2-models.RDS") -> models_exp2
```

```{r exp2-import}
# Load average number of objects per condition
(read_tsv("analysis/export/exp2-objects.tsv", col_types = cols()) %>%
   summarise(across(.cols = -Participant, .fns = mean)) %>%
   slice(1) %>% unlist() -> objects_exp2)

# Load preprocessed single-trial data
read_tsv("analysis/export/exp2-data.tsv", col_types = cols()) %>%
  # Factorize columns
  mutate(
    part = factor(part, levels = c("I", "II", "III")),
    condition = factor(condition, levels = c(
      "Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")
    ),
    participant = factor(participant),
    item = factor(item)
  ) -> data_exp2

# Check number of rejected epochs
data_exp2 %>%
  group_by(participant) %>%
  tally() %>%
  mutate(n = 360 - n) %>%
  pull(n) -> rejected_exp2
mean(rejected_exp2)    # Mean
median(rejected_exp2)  # Median
range(rejected_exp2)   # Range

# Load grand-averaged ERPs and linear mixed models
evokeds_exp2 <- readRDS("analysis/export/exp2-gaverages.RDS")
models_exp2 <- readRDS("analysis/export/exp2-models.RDS")
```

All materials, procedures, EEG-related methods, and statistical analyses were identical to Experiment 1. An average of `r format(round(objects_exp2["Excl_known"], 1), nsmall = 1)` objects per participant (= `r scales::percent(objects_exp2["Excl_known"]/120, accuracy = 0.1)` of all unfamiliar objects) was classified as being known in the pre-insight part and excluded from all further analyses. Based on participants' responses in the insight part, an average of `r format(round(objects_exp2["Informed"], 1), nsmall = 1)` objects was assigned to the semantically informed condition (= `r scales::percent(objects_exp2["Informed"]/60, accuracy = 0.1)` of objects presented with matching keywords) and an average of `r format(round(objects_exp2["Naive"], 1), nsmall = 1)` objects was assigned to the naive condition (= `r scales::percent(objects_exp2["Naive"]/60, accuracy = 0.1)` of objects presented with non-matching keywords). Automatic rejection of EEG epochs containing artifacts led to the exclusion of `r format(round(mean(rejected_exp2), 1), nsmall = 1)` trials per participant (= `r scales::percent(mean(rejected_exp2)/360, accuracy = 0.1)`; range `r min(rejected_exp2)` to `r max(rejected_exp2)` trials).

## Results

```{r exp2-table, include=TRUE, results="asis"}
# Create table of models for Experiment 2
table_exp2 <- create_table(
  models_exp2,
  stub_anova = c("Part", "Condition", "Pt. × con."),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption = "Results of Linear Mixed-Effects Regression Models for Experiment 2\\smallskip",
  note = "Pt. = part, con. = condition, est. = estimate, CI = 95\\% confidence interval."
)
```

As in Experiment 1, P1, N170, and N400 amplitudes differed between the three different parts of the experiments, all *F*s ≥ `r table_exp2$anov["Part", "N170_f"]`, all *p*s `r table_exp2$anov["Part", "N170_p"]` (see Table \@ref(tab:exp2-table)). Also as in Experiment 1, N400 amplitudes differed between the informed and the naive condition averaged across parts, *F*`r table_exp2$anov["Condition", "N400_df"]` = `r table_exp2$anov["Condition", "N400_f"]`, *p* `r table_exp2$anov["Condition", "N400_p"]`. The part × condition interaction was significant in the P1 component, *F*`r table_exp2$anov["Pt. × con.", "P1_df"]` = `r table_exp2$anov["Pt. × con.", "P1_f"]`, *p* = `r table_exp2$anov["Pt. × con.", "P1_p"]`, and in the N400 component, *F*`r table_exp2$anov["Pt. × con.", "N400_df"]` = `r table_exp2$anov["Pt. × con.", "N400_f"]`, *p* `r table_exp2$anov["Pt. × con.", "N400_p"]`, but not in the N170 component, *F*`r table_exp2$anov["Pt. × con.", "N170_df"]` = `r table_exp2$anov["Pt. × con.", "N170_f"]`, *p* = `r table_exp2$anov["Pt. × con.", "N170_p"]`.

```{r exp2-plot, include=TRUE, fig.height=7.5, fig.cap = "(ref:figure-2-caption)"}
# Create plot for Experiment 2
plot_grid(
  #plot_bars(data = data_exp2, stars = stars_exp2, ymin = -2.5),
  plot_headings(spacing = c(10, 1, 10, 1, 10)),
  plot_erps(comps = comps, evokeds = evokeds_exp2, models = models_exp2, montage = montage),
  plot_legends_erps(direction = "horizontal"),
  nrow = 3, rel_heights = c(0.3, 6, 1.2), labels = NULL
) + # Add some lines (to separate the parts)
  annotate(geom = "segment", x = 0.328, xend = 0.328, y = 0.15, yend = Inf, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.672, y = 0.15, yend = Inf, linetype = "dashed")
```

(ref:figure-2-caption) Results of Experiment 2\smallskip

<!--

The papaja package does not yet deal with the placement of figure captions above rather than below the figures. Therefore, we manually need to post-process the .tex file, moving the caption{} above includegraphics{} and copying the figure note manually below includegraphics{}:

\bigskip\small\textit{Note.} 
ERP waveforms and scalp topographies are shown for objects for which participants experienced semantically informed versus naive perception within the pre-insight, insight, and post-insight parts of the experiment. In an exact replication of Experiment 1, the effect of semantic information on the N400 component in the insight and post-insight parts and on the P1 component in post-insight part remained statistically significant, while the effect on the N170 component in the insight part marginally failed to reach statistical significance (\textit{p} = .064). Ampl. = amplitude.\newline*\textit{p} \textless{} .05. **\textit{p} \textless{} .01. ***\textit{p} \textless{} .001.

-->

### ERPs Before Insight Was Occurring

As in Experiment 1, no differences between objects in the semantically informed and the naive condition emerged in the P1, N170, or N400 component, all *p*s ≥ `r table_exp2$conts["Pre-insight part", "P1_p"]` (see Table \@ref(tab:exp2-table) and Figure \@ref(fig:exp2-plot)).

### ERPs While Insight Was Occurring

As in Experiment 1, semantically informed as compared to naive perception (induced by matching vs. non-matching keywords) was associated with an enhancement of the N170 component, although this effect marginally failed to reach statistical significance, *b* = `r table_exp2$conts["Insight part", "N170_est"]` µV, *p* = `r table_exp2$conts["Insight part", "N170_p"]`. The reduction of the N400 component during semantically informed perception remained significant, *b* = `r table_exp2$conts["Insight part", "N400_est"]` µV, *p* `r table_exp2$conts["Insight part", "N400_p"]`.

### ERPs After Insight Had Occurred

As in Experiment 1, the presentation of the same unfamiliar objects for a third time (without keywords, as in the pre-insight part) again led to significantly reduced amplitudes in the N400 component in response to objects for which semantically informed perception had occurred, *b* = `r table_exp2$conts["Post-insight part", "N400_est"]` µV, *p* = `r table_exp2$conts["Post-insight part", "N400_p"]`. Also, P1 amplitudes in response these objects were significantly enlarged, *b* = `r table_exp2$conts["Post-insight part", "P1_est"]` µV, *p* = `r table_exp2$conts["Post-insight part", "P1_p"]`.

### Joint Analysis of Experiments 1 and 2

In an attempt to maximize statistical power, we combined the ERP data sets from Experiments 1 and 2. This allowed us to determine (a) if the above effects---including the marginally significant ones---were reliable when tested in a larger sample, and (b) if there were significant differences in the ERP amplitudes between Experiments 1 and 2. Methods for statistical analysis were kept unchanged apart from the addition of a new factor denoting the experiment, coded as a scaled sum contrast [i.e., subtracting Experiment 1 from Experiment 2, the intercept being the grand mean across both experiments\; @schad2020]. This factor and its interactions with part, condition, and part × condition were included in the linear mixed-effects regression models as fixed effects and as potential by-item random slopes. They were not included as by-participant random slopes because different participants took part in Experiments 1 and 2. Note that, as above, random effects were eventually included only if their omission led to a significant decline in model fit [@matuschek2017; @R-buildmer]. The resulting formulas for the models can be found in Appendix B.

```{r joint-import}
# Combine data from Experiments 1 and 2 (requires changing the participant IDs for Experiment 2)
data_exp2 %>%
  mutate(participant = fct_relabel(participant, ~ paste0(., "_2"))) %>%
  bind_rows(aha, ., .id = "experiment") %>%
  mutate(experiment = as_factor(experiment)) -> data_joint

# Subset only the relevant conditions
data_joint %>%
  filter(condition %in% c("Informed", "Naive")) %>%
  droplevels() -> data_joint_subs
```

```{r joint-models, eval=rerun_models}
# Contrast coding for experiment (2-1), condition (informed-naive), and part (2-1, 3-1)
t(contrasts_experiment <- t(cbind(c("1" = -1, "2" = 1))))
contrasts(data_joint_subs$experiment) <- MASS::ginv(contrasts_experiment)
contrasts(data_joint_subs$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_joint_subs$part) <- MASS::ginv(contrasts_part)

# New formula for LMMs
form_joint <- buildmer::tabulate.formula(
  ~ part * condition * experiment + 
    (part * condition | participant) + 
    (part * condition * experiment | item)
) %>% mutate(block = replace(block, is.na(grouping), "fixed"))

# Apply the function to compute the joint models for Experiments 1 and 2
map(
  comps$name,
  compute_models,
  formula = form_joint,
  data = data_joint_subs,
  control = control,
  specs = specs_exp12
) %>%
  set_names(comps$name) %T>%
  saveRDS("analysis/export/joint-models.RDS") -> models_joint
```

```{r joint-table, include=TRUE, results="asis"}
# Load linear mixed models
models_joint <- readRDS("analysis/export/joint-models.RDS")

# Create table of models for Experiments 1 and 2 combined
table_joint <- create_table(
  models_joint,
  stub_anova = c(
    "Part", "Condition", "Experiment", "Pt. × con.",
    "Pt. × exp.", "Ins. × exp.", "Pt. × con. × exp."
  ),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption =
    "Results of Linear Mixed-Effects Regression Models for Experiments 1 and 2 Combined\\smallskip",
  note =
    "Pt. = part, con. = condition, exp. = experiment, est. = estimate, CI = 95\\% confidence interval."
)

# Extract the follow-up contrast for the main effect of experiment
suppressMessages(emmeans::emm_options(lmerTest.limit = Inf))
suppressMessages(
  emmeans::emmeans(
    models_joint$N400$model,
    specs = pairwise ~ experiment, infer = TRUE
  )$contrasts %>%
    as.data.frame() %>%
    transmute(
      est = round(estimate, 2) %>%
        format(nsmall = 2, trim = TRUE),
      p = round(`p.value`, 3) %>%
        format(nsmall = 3, trim = TRUE) %>%
        as.character() %>%
        substr(2, nchar(.))
    ) -> conts_N400_joint
)
```

As shown in Table \@ref(tab:joint-table), the main effect of part was significant in the P1, N170, and N400 component, all *F*s ≥ `r table_joint$anov["Part", "P1_f"]`, all *p*s `r table_joint$anov["Part", "P1_p"]`, as was the main effect of condition in the N400, *F*`r table_joint$anov["Condition", "N400_df"]` = `r table_joint$anov["Condition", "N400_f"]`, *p* `r table_joint$anov["Condition", "N400_p"]`. Furthermore, the part × condition interaction was now observed reliably in all three components, all *F*s ≥ `r table_joint$anov["Pt. × con.", "P1_f"]`, all *p*s ≤ `r table_joint$anov["Pt. × con.", "P1_p"]`. There was a main effect of experiment in the N400, *F*`r table_joint$anov["Experiment", "N400_df"]` = `r table_joint$anov["Experiment", "N400_f"]`, *p* = `r table_joint$anov["Experiment", "N400_p"]`, with more negative amplitudes in Experiment 1 as compared to Experiment 2, *b* = `r conts_N400_joint$est`. However, the absence of any significant interactions of experiment with part or condition indicated that the effects of our experimental manipulations did not differ between Experiments 1 and 2.

Based on the part × condition interaction, we again computed follow-up comparisons between the semantically informed and the naive condition within each part, now collapsed across the data from both experiments (see Table \@ref(tab:joint-table) and Figure \@ref(fig:joint-plot)). This confirmed the absence of any reliable differences between the two conditions in the pre-insight part, all *p*s ≥ `r table_joint$conts["Pre-insight part", "N400_p"]`, the significant enhancement of the N170 component in the insight part, while the semantic information was obtained, *b* = `r table_joint$conts["Insight part", "N170_est"]` µV, *p* = `r table_joint$conts["Insight part", "N170_p"]`, the significant reduction of the N400 component in the insight part, while the semantic information was obtained, *b* = `r table_joint$conts["Insight part", "N400_est"]` µV, *p* `r table_joint$conts["Insight part", "N400_p"]`, and in the post-insight part, after the information had been obtained, *b* = `r table_joint$conts["Post-insight part", "N400_est"]` µV, *p* `r table_joint$conts["Post-insight part", "N400_p"]`, as well as the significant enhancement of the P1 component in the post-insight part, after the information had been obtained, *b* = `r table_joint$conts["Post-insight part", "P1_est"]` µV, *p* = `r table_joint$conts["Post-insight part", "P1_p"]`.

(ref:figure-3-caption) Measured and Modeled ERP Amplitudes for Experiments 1 and 2 Combined\smallskip

<!--

The papaja package does not yet deal with the placement of figure captions above rather than below the figures. Therefore, we manually need to post-process the .tex file, moving the caption{} above includegraphics{} and copying the figure note manually below includegraphics{}:

\bigskip\small\textit{Note.} 
The violins and box plots show the distributions of the by-participant averaged ERPs during semantically informed perception (in purple color) and naive perception (in petrol color), separately for the pre-insight part (before insight was occurring), the insight part (while insight was occurring), and the post-insight (after insight had occurred). The yellow dots show the corresponding means of these conditions as predicted by linear mixed-effects modeling (see main text and Table \ref{tab:joint-table}), together with their respective 95\% confidence interval. Ampl. = amplitude, 95\% CI = 95\% confidence interval, LMM = linear mixed-effects model.\newline*\textit{p} \textless{} .05. **\textit{p} \textless{} .01. ***\textit{p} \textless{} .001.

-->

```{r joint-plot, include=TRUE, fig.height=8, fig.cap = "(ref:figure-3-caption)"}
# Create violin plot for Experiments 1 and 2 combined
plot_grid(
  plot_headings(spacing = c(10, 2, 10, 2, 10)),
  plot_violins(data = data_joint_subs, models = models_joint),
  nrow = 2, rel_heights = c(0.3, 6.7)
) %>% plot_grid(
  plot_legends_violins(),
  nrow = 1, rel_widths = c(8.5, 1.5)
) + # Add some lines (to separate the parts)
  annotate(
    geom = "segment", x = 11 / 34 * 0.85, xend = 11 / 34 * 0.85,
    y = -Inf, yend = Inf, linetype = "dashed"
  ) +
  annotate(
    geom = "segment", x = 23 / 34 * 0.85, xend = 23 / 34 * 0.85,
    y = -Inf, yend = Inf, linetype = "dashed"
  )
```

### Control Analysis

```{r control-models, eval=rerun_models}
# This time, we keep only the two conditions with matching keywords
data_joint %>%
  filter(condition %in% c("Informed", "Excl_informed")) %>%
  droplevels() -> data_joint_control

# Contrast coding for condition (informed-no_informed) and part (2-1, 3-1)
contrasts(data_joint_control$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_joint_control$part) <- MASS::ginv(contrasts_part)

# Apply the function to re-compute the joint models
map(
  comps$name,
  compute_models,
  formula = form_joint,
  data = data_joint_control,
  control = control,
  specs = specs_exp12
) %>%
  set_names(comps$name) %T>%
  saveRDS("analysis/export/joint-control-models.RDS") -> models_joint_control
```

```{r control-table}
# Calculate average number of objects per condition in Experiments 1 and 2
(stims_joint <- (objects_exp1 + objects_exp2) / 2)

# Load linear mixed models
models_joint_control <- readRDS("analysis/export/joint-control-models.RDS")

# Create a table but don't print
table_joint_control <- create_table(
  models = models_joint_control,
  stub_anova = c(
    "Part", "Condition", "Experiment", "Pt. × con.",
    "Pt. × exp.", "Ins. × exp.", "Pt. × con. × exp."
  ),
  stub_contrasts = c("Pre-insight part", "Insight part", "Post-insight part"),
  caption =
    "Results of Linear Mixed-Effects Regression Models With a Different Baseline",
  note =
    "Pt. = part, con. = condition, exp. = experiment, est. = estimate, CI = 95\\% confidence interval."
)
```

One may raise concerns whether the modulation of the N170 component in the insight part genuinely reflects the semantically informed perception of the objects in the respective condition, or---as an alternative explanation---whether it may be driven by the objects in the other, semantically naive condition. Remember that these objects were preceded by non-matching keywords which were picked so that they could not be related to the visual features of the object and their configuration. Thus, the modulation of the ERP components in the insight part could be a mismatch response to those objects---reflecting, for example, the fact that the visual features of the object shown in Figure \@ref(fig:exp1-plot)A cannot be reconciled with the function of signaling messages. To preclude this alternative explanation, we repeated our analysis using a different baseline against which the objects in the semantically informed condition were contrasted. Instead of the naive condition (where objects were presented with non-matching keywords), we now used those objects which were presented with matching keywords (as in the informed condition), but which were excluded from the main analysis because participants indicated behaviorally that they did not understand the object they were seeing. Across both experiments, this was the case for `r scales::percent(stims_joint["Excl_informed"]/60, accuracy = 0.1)` of objects presented with matching keywords, as compared to `r scales::percent(stims_joint["Informed"]/60, accuracy = 0.1)` of objects which did indeed lead to semantically informed perception.[^1] Just as above, this control analysis revealed a robust N170 effect in the insight part, *b* = `r table_joint_control$conts["Insight part", "N170_est"]` µV, *p* `r table_joint_control$conts["Insight part", "N170_p"]`. Thus, this enhanced negativity seems to be a genuine marker of semantically informed perception, no matter if compared to objects presented with non-matching keywords or compared to objects presented with matching keywords on which participants failed to capitalize. Note that the reduction of the N400 component in the insight part also remained robust in this control analysis, *b* = `r table_joint_control$conts["Insight part", "N400_est"]` µV, *p* `r table_joint_control$conts["Insight part", "N400_p"]`.

[^1]: Note that these percentages do not add up to 100% because some objects were excluded from all analyses if participants indicated knowing these objects in the pre-insight part, before any keywords had been presented (see Method).

## Discussion

Experiment 2, which was an exact replication of Experiment 1, confirmed the effects of obtaining semantic information about previously unfamiliar objects on ERPs associated with lower-level visual perception (P1), higher-level visual perception (N170), and semantic processing (N400). Objects for which participants experienced semantically informed perception via matching keywords elicited enlarged N170 amplitudes and reduced N400 amplitudes. Note, however, that the effect in the N170 component marginally failed to reach the conventional level for statistical significance in this replication experiment. When the objects were presented once more without any semantic information, they again elicited reduced N400 amplitudes as well as enlarged P1 amplitudes.

```{r exp3-preproc, eval=run_exp3}
# Read behavioral data for Experiment 1
list.files("data/exp3/RT", pattern = ".txt", full.names = TRUE) %>% map(read_behav) -> behav_exp3

# Update stimulus triggers
preproc_params$event_id <- dict(
  "match/upright" = as.integer(241),
  "match/inverted" = as.integer(242),
  "mismatch/upright" = as.integer(243),
  "mismatch/inverted" = as.integer(244)
)

# Preprocess the EEG for Experiment 3 in Python
fnames_exp3 <- list.files("data/exp3/EEG", pattern = ".vhdr", full.names = TRUE)
py_run_string(
  "epochs_exp3 = [preproc(vhdr_fname=fname, metadata=meta, **r.preproc_params)
                  for fname, meta in zip(r.fnames_exp3, r.behav_exp3)]"
)
message("This worked!!!")
py_run_string("epochs_exp3 = mne.epochs.concatenate_epochs(epochs_exp3)")
message("This also worked!!!")
py_run_string("epochs_exp3.save('export/exp3-epo.fif', overwrite=True)")
py_run_string("del epochs_exp3")

# Preprocess the EEG for Experiment 3 in Python
map2(
  .x = list.files("data/exp3/EEG", pattern = ".vhdr", full.names = TRUE),
  .y = behav_exp3,
  ~ do.call(py$preproc, c(list(vhdr_fname = .x, metadata = .y), preproc_params))
) %>%
  # Concatenate epochs of all participants
  py$mne$epochs$concatenate_epochs() -> epochs_exp3

# Backup epochs to the export folder
epochs_exp3$save("analysis/export/exp3-epo.fif")

# Compute mean ERP amplitudes and bind to the behavioral data
pmap_dfc(comps, py$compute_erps, epochs = epochs_exp3) %>%
  set_names(comps$name) %>%
  bind_cols(as_tibble(epochs_exp3$metadata), .) %T>%
  write_tsv("analysis/export/exp3-data.tsv") -> data_exp3

# Compute grand averages per part and condition
py$compute_evokeds(epochs = epochs_exp3, grand = TRUE, data_only = TRUE) %>%
  saveRDS("analysis/export/exp3-gaverages.RDS")

# Backup number of stimuli assigned to each condition for all participants
map(behav_exp3, function(x) table(x$condition) / 3) %>%
  bind_rows() %>%
  mutate(Participant = unique(data_exp3$participant), .before = 1) %>%
  write_tsv("analysis/export/exp3-objects.tsv")

# Remove the epochs to free up memory
rm(epochs_exp3)
```

```{r exp3-models, eval=run_exp3}
# Load preprocessed single-trial data
read_tsv("analysis/export/exp3-data.tsv", col_types = cols()) %>%
  # Factorize columns
  mutate(
    part = factor(part, levels = c("I", "II", "III")),
    position = factor(position, levels = c("Inverted", "Upright")),
    condition = factor(condition, levels = c(
      "Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known"
    )),
    participant = factor(participant),
    item = factor(item)
  ) %>%
  # Remove any trials excluded from conditions and parts
  filter(condition %in% c("Informed", "Naive") & part != "IV") %>%
  droplevels() -> data_exp3

# Define contrasts for position (inverted-upright), condition (informed-naive), and part (2-1, 3-1)
t(contrasts_position <- t(cbind(c("Inverted" = 1, "Upright" = -1))))
contrasts(data_exp3$position) <- MASS::ginv(contrasts_position)
contrasts(data_exp3$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_exp3$part) <- MASS::ginv(contrasts_part)

# Define new formula for LMMs
formula_exp3 <- buildmer::tabulate.formula(
  ~ part * condition * position +
    (part * condition * position | participant) +
    (part * condition * position | item)
) %>% mutate(block = replace(block, is.na(grouping), "fixed"))

# Define new follow-up contrasts
specs_exp3 <- pairwise ~ condition | position * part

# Compute the models for Experiment 3
map(
  comps$name,
  compute_models,
  formula = formula_exp3,
  data = data_exp3,
  control = control,
  specs = specs_exp3
) %>%
  set_names(comps$name) %T>%
  saveRDS("analysis/export/exp3-models.RDS") -> models_exp3
```

```{r exp3-table, include=TRUE, results="asis", eval=run_exp3}
# Create table of models for Experiment 3
table_exp3 <- create_table(
  models_exp3,
  stub_anova = c(
    "Part", "Condition", "Position", "Pt. × con.",
    "Pt. × pos.", "Ins. × pos.", "Pt. × con. × pos."
  ),
  stub_contrasts = c(
    "Part I, inverted", "Part I, upright", "Part II, inverted",
    "Part II, upright", "Part III, inverted", "Part III, upright"
  ),
  caption = "Results of linear mixed-effects regression models for Experiment 3",
  note = "Pt. = part, con. = condition, pos. = position, est. = estimate, CI = 95\\% confidence interval."
)
```

# General Discussion

Here we investigated if obtaining a semantic understanding of previously unfamiliar objects has an influence on how we perceive them. To this end, we measured ERPs while participants viewed unfamiliar objects before, while, and after receiving semantic information about them. For half of the objects, this information was matching the object, thus leading to semantically informed perception, whereas for the other half of the objects, the information was non-matching, thus keeping the perception of the object semantically naive. We found semantically informed perception to be accompanied by enlarged (i.e., more negative) N170 amplitudes and reduced (i.e., less negative) N400 amplitudes. When the same objects were presented again without the semantic information, the N400 component remained significantly reduced and we also observed a modulation of the P1 component, which was enlarged (i.e., more positive) in response to objects that had previously triggered semantically informed perception. We will discuss each of these effects in turn, starting with the latest (i.e., post-perceptual) effect and moving backward in time to the earlier (i.e., more perceptual) effects.

The reduction of the N400 component (400--700 ms after object onset) during semantically informed perception was the numerically largest and most robust effect. Perhaps least controversially, this effect indicates that acquiring an understanding of the objects (in the insight part) lessened participants' demand for effortful semantic processing in comparison to the naive condition [@kutas2011]. This replicates previous work showing that N400 amplitudes are larger in response to pictures when they are either difficult to understand in and of themselves [e.g., @abdelrahman2008; @supp2005] or difficult to integrate into the preceding context [e.g., @barrett1990; @ganis1996; @hirschfeld2011]. The time course of this effect and the computational role of the N400 [@lau2008; @rabovsky2018] suggest that it has a post-perceptual locus.

In contrast to the N400, the N170 component (150--200 ms after object onset) was modulated while but not after the objects were presented together with the relevant semantic information. As such, it can be seen as an online marker of semantic insight, that is, participants suddenly understanding the visual objects in the light of the additional information provided by the verbal keywords. The N170 is typically associated with the holistic perception of faces [@eimer2011; @sagiv2001] and other stimuli of visual expertise [@rossion2002; @tanaka2001]. It being enlarged during semantically informed perception may therefore reflect that the additional semantic information made participants experience the configuration of the visual features of the objects in a new and meaningful way. This interpretation is supported by previous findings with a similar experimental paradigm in the domain of face perception [@bentin2002]: In this study, participants showed a face-like (i.e., enlarged) N170 response to a scrambled version of a schematic face after (but not before) they were primed with the intact version of the same face. This effect was absent when a visual control stimulus (a non-face object) was shown instead of the intact face. In the domain of non-face stimuli, enlarged N170 amplitudes have also been observed when participants were asked to discriminate between composite line drawings of meaningful objects as compared to composite line drawings of non-objects [@beaucousin2011]. Of note, this effect was present only when participants were asked to decide based on the global shape of the object and it was reversed in polarity when they were asked to decide based on the constituent parts of the object. Together with the present study, these findings suggest an online impact of meaningfulness on the higher-level (i.e., holistic) perception of visual objects, integrating across their visual features.

The P1 component (100--150 ms after object onset), unlike the N400 and N170 components, was modulated by semantic information only one trial after the information had been obtained. This is consistent with previous studies showing modulations of the P1 component when participants learned meaningful information about previously unfamiliar objects [@abdelrahman2008; @maier2018; @maier2019; @weller2019] or about familiar objects that were rendered difficult to recognize [@samaha2018]. What the present study adds to these findings is that the P1 effect does not take an extensive learning phase to develop (with multiple presentations of the objects together with the respective information). Instead, it can be observed as soon as one trial after semantic insight has happened. Because the P1 is typically associated with lower-level sensory processing [e.g., @johannes1995; @luck2014; @pratt2011], we take its susceptibility to semantic information as an indicator that knowledge about the function of an object can change how we perceive it visually.

Both the N170 and the P1 components therefore seem to be sensitive to the semantic meaningfulness of visual objects. However, the finding that these two components were modulated in different parts of our experimental design suggests that they reflect different aspects of top-down processing with different time courses and neuroanatomical implementations. It has been pointed out that the time course of the N170 component is consistent with a top-down influence of (non-visual) areas in the prefrontal and parietal cortices on visual areas, whereas modulations of the P1 component seem to reflect recurrent processing *within* the visual system [@wyatte2014]. Here we could show that the former pathway seems to be able to convey semantic information instantaneously (i.e., within the same trial), whereas the latter seems to take at least one---but apparently also not more than one---additional presentation of the visual object to emerge.

While the limited spatial resolution of the EEG precludes a precise localization of these effects within the ventral stream for object recognition, there is converging evidence coming from fMRI showing that semantic information can feed back into the earliest of visual areas. @hsieh2010 showed participants indiscernible two-tone ("Mooney") versions of images before and after showing them the original versions. They found that the brain responses to the original image were correlated more strongly with the second presentation of the Mooney image (after insight had taken place) than with the first presentation of the Mooney image (before insight had taken place). This increase in representational similarity was not just observed in higher-level object-sensitive areas in the lateral occipital cortex (LOC), but also in early retinotopic cortex (areas V1, V2, and V3). Both of these cortical regions are consistent with the neural generators of the N170 and P1 components in the ERP which we have found to be sensitive to the semantically informed perception of previously unfamiliar objects.

On a theoretical level, the top-down modulation of these visual ERPs by semantic information challenges a modular view of visual perception [@firestone2016; @fodor1983; @pylyshyn1999; but see @clarke2020]. However, proponents of such a modular view have pointed out important shortcomings of previous studies that claimed to demonstrate top-down effects of cognition on perception [@firestone2016; @machery2015]. We took care to address as many of these shortcomings as possible: First, we showed that no effect had been present before any semantic information was being presented (in the pre-insight part). Second, we used an objective and time-resolved measure (ERPs) to disentangle effects with a perceptual locus from those with a post-perceptual locus. Third, we reduced response and demand biases by keeping the manipulation (i.e., matching or non-matching keywords) obscure to the participants and by including well-known objects as filler stimuli. Fourth, we precluded low-level visual differences between conditions by counterbalancing the assignment of objects to conditions across participants. Fifth, we reduced priming and attentional effects by presenting all objects in a randomized order and at the same location. Sixth, we reduced memory effects by using only unfamiliar objects and by measuring online ERPs rather than delayed behavioral responses. We hope that these procedures have effectively ruled out some of the most important alternative explanations for the top-down effects that we have observed, thus making a more compelling case against the cognitive impenetrability of perception.

An interactive view of object vision with an abundance of top-down feedback also challenges the predominantly feed-forward models in computer vision [e.g., @marr1982; @lindsay2020]. In fact, the lack of a semantic knowledge base that dynamically interacts with the processing of lower-level visual features may be one key reason why even state-of-the-art deep-learning algorithms need orders of magnitude more training examples to achieve human-level performance in object recognition. For these network models, single-trial learning of previously unfamiliar objects, as was observed on the behavioral and on the neurophysiological level in the present study, seems to be out of reach until they overcome this "barrier of meaning" [@mitchell2020]. Drawing inspiration from cognitive psychology and human neuroscientific data may help to make these models more biologically plausible and, at the same time, more data efficient.

A theoretical framework that would explicitly predict or explain the observed P1 and N170 effects in our study is lacking at present. The effects are consistent, however, with the reverse hierarchy theory [@ahissar2004; @hochstein2002] which posits that objects first enter our visual consciousness at an abstract, conceptual level. Once this initial "vision at a glance" has taken place, feedback connections to earlier layers of the visual system are being accessed to extract the relevant lower-level features ("vision with scrutiny"). This reverse trajectory down the visual hierarchy may explain (a) the semantically induced changes to the fMRI signal in LOC and retinotopic cortex [@hsieh2010] as well as (b) the modulations of early visual ERP components observed in the present study and others [@abdelrahman2008; @maier2014; @maier2019; @samaha2018; @weller2019]. Besides this specific theory, an important role of top-down mechanisms for vision or, more specifically, object recognition is also posited by the family of predictive coding and Bayesian inference theories [e.g., @clark2013; @lupyan2015; @panichello2013; @yuille2006]. Despite the theoretical advances, the mechanistic details of these top-down effects at the algorithmic and implementational level [@marr1982] remain to be clarified.

The lack of mechanistic insight into the top-down effects that we have observed is one limitation of the present study. Another one is our reliance on rare and highly specialized objects (see Appendix A). This was necessary to induce the experience of semantic insight in a population of undergraduate students but the results may not necessarily generalize to the way in which we learn about everyday objects.[^2] One way of addressing this issue would be to adapt the present paradigm to younger participants, using everyday objects which they have not yet learned about. Finally, the number participants (*n* = 24 per experiment) and trials (*k* $\approx$ 30 per part in the insight condition and *k* $\approx$ 50 per part in the naive condition) can be considered small by today's standards [@baker2020]. Our analysis may therefore not have been particularly sensitive, especially given that linear mixed-effects regression models tend to have limited statistical power under a wide range of circumstances [see, e.g., the simulations by @matuschek2017]. This lack of power may also explain why one of the effects that had been observed in the first experiment (i.e., the enlargement of the N170 component, *p* = `r table_exp1$conts["Insight part", "N170_p"]`) failed to reach statistical significance in the replication experiment (*p* = `r table_exp2$conts["Insight part", "N170_p"]`). To discern if this was due to a lack of statistical power or due to the actual absence of an effect, it would take yet another (more highly powered) replication study and/or a different data-analytic approach where it is possible to estimate the evidence both for the alternative hypothesis and for the null hypothesis (e.g., Bayesian linear mixed models). It should be noted, however, that the joint analysis of both data sets yielded a very robust N170 effect (*p* = `r table_joint$conts["Insight part", "N170_p"]`). This makes it less likely that the effect observed in Experiment 1 was due to a false positive and instead suggests that statistical power in Experiment 2 was insufficient to render the effect significant. Also note that the effect in the P1 component was statistically significant in both experiments individually, making an even stronger case for the susceptibility of early cortical processing to newly acquired knowledge about objects.

[^2]: Note, however, the converging evidence coming from the complementary approach of rendering images of everyday objects difficult to recognize [@samaha2018].

Taken together, this study provides preliminary evidence that whenever we receive meaningful semantic information about a previously unfamiliar object, this information has an immediate influence on the processing of the object. This influence is immediate in at least two different ways: First, it does not require extensive training and can instead be observed within the same trial in which the information has been presented (and/or one trial later). Second, the time course of this influence suggests that it manifests itself not only at later, post-perceptual stages---typically associated with semantic processing---but also at earlier stages---typically associated with visual perception itself and happening within less than a fifth of a second after the object is presented to us.

\newpage

# References

\setlength{\parindent}{-0.5in}

---
title             : |
  | Event-Related Potentials of the Semantically
  | Informed Perception of Unfamiliar Objects
shorttitle        : "Semantic Knowledge and Unfamiliar Objects"

author: 
  - name          : "Alexander Enge"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Rudower Chaussee 18, 12489 Berlin"
    email         : "alexander.enge@hu-berlin.de"

affiliation:
  - id            : "1"
    institution   : "Humboldt-Universität zu Berlin"

authornote: |
  Alexander Enge, Department of Psychology, Humboldt-Universität zu Berlin, Germany, https://orcid.org/0000-0003-0100-2297, student no. 581331.

  The thesis was handed December 18, 2020 and has been supervised by Prof. Dr. Rasha Abdel Rahman, Humboldt-Universität zu Berlin, Germany, and Prof. Dr. Franziska Süß, Fachhochschule des Mittelstands, Bamberg, Germany.

abstract: |
  Abstract goes here.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : "manuscript_files/r-references.bib"
zotero            : "aha"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

csl               : "manuscript_files/apa.csl"
documentclass     : "apa7"
classoption       : "man,11pt"
output:
  papaja::apa6_pdf:
    latex_engine:   "xelatex"

header-includes:
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \AtBeginEnvironment{lltable}{\doublespacing}
  - \AtBeginEnvironment{tablenotes}{\doublespacing}
  - \captionsetup[table]{font={stretch=1.5}}
  - \captionsetup[figure]{font={stretch=1,scriptsize}}
  - \raggedbottom
---

```{r setup, include=FALSE}
# Load packages
library(papaja)
library(reticulate)
library(tidyverse)
library(magrittr)
library(cowplot)

# Chunk options
knitr::opts_chunk$set(include = FALSE, fig.align = "center", fig.out = "100%", fig.width = 10)

# Specify Python environment
renv::use_python(type = "virtualenv")

# Bibliography file
r_refs("analysis/manuscript_files/r-references.bib")

# Create export folder for results
dir.create("analysis/export", showWarnings = FALSE)

# Do we want to run analysis for Experiment 3?
run_exp3 <- FALSE
```

How does our perception of an object change as soon as we discover what it is for? In this study, we presented participants with unfamiliar objects, preceded in half of the cases by valid information about their function (leading to semantically informed perception) and in the other half of the cases by misleading information (leading to naive perception). Capitalizing on the high temporal resolution of event-related potentials (ERPs), we investigated if semantically informed versus naive perception led to differential effects at different stages in the processing hierarchy of visual objects.

Learning semantic knowledge about objects has an influence on how we process them [@abdelrahman2008; @gauthier2003; @gratton2009; @maier2014; @maier2019; @rossion2002; @rossion2004; @tanaka2001; @samaha2018; @weller2019]. Recently, evidence has accumulated that it does so not only at "higher" stages in the visual processing hierarchy, i.e. after "lower" stages of perceptual processing have been completed. It rather seems as though knowledge can also alter early processes, previously thought to be determined solely by the visual input itself, in a top-down manner. Such a top-down influence of semantic knowledge or expertise may manifest itself in behavioral changes such as improved visual matching [e.g.  @gauthier2003] and visual search performance [@maier2014; @holmes2012] when having learned that two objects share the same semantic category or verbal label. Furthermore, acquiring semantically meaningful knowledge about previously unfamiliar objects has been shown to increase their likelihood of gaining access to conuscious visual awareness under circumstances when attentional resources are limited [@weller2019]. Although these behavioral effects hint at an early locus of semantic knowledge effects in object processing, they in and of themselves cannot convincingly demonstrate that not just downstream, postperceptual steps are being affected [@firestone2016].

Stronger evidence comes from some of the above and other studies concurrently measuring event-related brain potentials (ERPs) while participants are performing different tasks on the objects about which they had previously acquired semantic knowledge. In their ERP study, [@abdelrahman2008] had participants first learn in-depth knowledge about 20 unfamiliar objects by listening to a verbal descripition of the function of each object while it was presented visually on the screen, whereas for 20 different objects they did not receive any relevant semantic information, instead listening to unrelated cooking recipes while the objects were presented. As in the present study, the assignment of unfamiliar objects to either of the two conditions was counterbalanced ascross participants as to prevent any subsequent effects of semantic knowledge from being confounded with visual differences between the objects. In a separate test session, event-related potentials were measured while participants performed three different tasks on those objects (intermixed with well known objects that had not been part of the training session). ERP amplitudes in response to objects for which in-depth versus minimal knowledge had been acquired differed in the time window of the N400 component (300--500 ms after stimulus onset), typically associated with semantic processing, but, interestingly, aslo in the time window of the visual P1 component (100--150 ms after stimulus onset), typcially associated with lower-level visual processing [[@abdelrahman2008]; Experiment 1]. This effect ocurred across three different tasks (none of which explicitly required accessing any of the aqcuired information) and was qualitatively similar to the ERPs observed for untrained but well-known objects. A modulation of the P1 component has also been replicated using similar sets of objects in the above mentioned studies showing effects of object knowledge in visual search and the visual awareness tasks [@maier2019; @weller2019]. It thus seems as though learning semantic information can influence steps of the processing hierarchy previously thought to be determined by the low-level visual input.

A different strategy to investaige the influence of semantic knowledge on perception has been taken in a recent ERP study by [@samaha2018]. Instead of using unfamiliar objects as stimuli, participants viewed images of familiar everyday objects that had been converted to two-tone "Mooney" images, making the object itself difficult to recognize. For half of the images, participants performed a semantic training where they were told which object to look for in each image. For the other half of images, they performed a perceptual (1-back) control task to equate the amount familiarity across all images. In a subsequent visual discrimination task, participants tended to respond faster and more accurate to the semantically trained images than to the perceptually trained ones [[@samaha2018]; Experiment 4]. Crucially, the semantic training also manifested itself in increased pre-stimulus alpha-band power and larger P1 amplitudes at left temporo-occipital electrodes, further coroborating an early influence of semantically informed as compared to naive perception of objects.

What all the previous studies have in common is that they investigated the effects of semantic knoweldge on object perception *after* an extensive learning phase [@abdelrahman2008; @gauthier2003; @maier2014; @maier2019; @rossion2002; @rossion2004; @samaha2018; @weller2019]. Participants usually performed a seperate training session where they encountered each object together with its respective label or description multiple times. The EEG was usually not measured (or at least not analyzed and reported) during this training phase, which sometimes took place on a separate day [@abdelrahman2008; @maier2014; @maier2019] or was even spread out across multiple days or weeks [@rossion2002; @rossion2004]. While designs with such substantive learning maximizes the chances of detecting even subtle top-down effects of the acquired knowledge, they necessarily leave at least three important conceptual questions unresolved. The first question is if we can detect any electrophysiological correlates of semantic insight, i.e. the critical presentation during which an understanding of a previously unfamiliar or meaningless objects is happening, instead of asking if this understanding leads to differential effects later in an orthogonal task. The second question is how much learning is actually necessary before reliable top-down effects of knowledge (e.g. on object-related P1 amplitudes) can be obtained: Does it actually take dozens of repitions per object or may a single exposure to the object together with the relevant semantic information be enough? The third and closely related question is related to one of the critiques sometimes raised against the nature of knowledge effects: Are they reflecting genuine top-down effects of semantic processes, altering perception online (while the object is being perceived), or do they merely reflect the (re-)activation of stored representations of the objects which have been created or altered throughout the learning process? By measuring the ERPs before, during, and after participants received semantic hints about the unfamiliar objects, we hope to provide tentative answers to all of these questions.

-   One paragraph explaining that previous research has shown that semantic knowledge modulates early stages of visual perception (e.g. Abdel Rahman & Sommer, 2008; Gauthier et al., 2003; Maier & Abdel Rahman, 2019; Maier et al., 2014; Rossion et al., 2002, 2004; Samaha et al., 2018; Tanaka & Curran, 2001; Weller et al., 2019).

-   Potentially one paragraph describing Abdel Rahman & Sommer (2008) in more detail (participants acquiring knowledge about existing but unfamiliar objects).

-   One paragraph describing Samaha et al. (2018) in more detail (participants being trained to see familiar objects in otherwise meaningless images). Possibly hypothesize a special role of insight (= the moment when understanding is acquired), although this is not directly captured by the study.

-   One paragraph explaining how all of these studies measured ERPs only *after* an extensive learning phase, making it difficult to determine (a) how much learning is necessary for semantic knowledge to influence perception and, related to that, (b) if the modulation of early ERP components reflects genuine online effects of semantic knowledge or rather the activation of altered visual representations (Firestone & Scholl, 2015) (?).

-   One paragraph on theories of top-down/language effects on visual perception (reverse hierarchy theory).

-   Possibly another paragraph on theories of top-down effects on visual perception (label feedback hypothesis) if it fits thematically.

-   Do we need to refer to the role of attention already in the introduction? If so, potentially another paragraph here. I might prefer to leave that for the discussion.

-   Possibly one more paragraph on why ERPs are so suitable to answer this research question, including a brief characterization of the relevant components (P1: lower-level visual perception, N1: higher-level visual perception, N400: semantic processing).

-   The present study: Design of the study with a focus on distinguishing between online effects and altered representations; no specific hypotheses but looking for influences of semantic knowledge on the three ERP components at three different stages (before, during, and after acquisition).

# Experiment 1

```{r exp1-preparation}
# Define function for reading and formatting behavioral data
read_behav <- function(txt_fname) {
  # Read log file
  dat <- suppressMessages(suppressWarnings(read_tsv(txt_fname)))
  # Rename columns for participant and item IDs
  dat %<>% rename(participant = VPNummer, item = StimID) %>%
    # Add a column for the part of the experiment
    mutate(part = case_when(
      Wdh %in% c(211, 231) ~ "I",
      Wdh %in% c(212, 232) ~ "II",
      Wdh %in% c(213, 233) ~ "III",
      Wdh == 234 ~ "IV"
    ) %>% as_factor()) %>%
    # Remove filler stimuli (i.e. well-known objects)
    filter(bek_unbek != "bekannt")
  # Assign stimuli to conditions based on keywords and button presses
  excl_known <- dat %>%
    filter(part == "I" & Tastencode %in% c(201, 251)) %>%
    pull(item)
  excl_informed <- dat %>%
    filter(part == "II" & grepl("richtig", Bed) & !Tastencode %in% c(201, 202, 251, 252)) %>%
    pull(item)
  excl_naive <- dat %>%
    filter(part == "II" & grepl("falsch", Bed) & !Tastencode %in% c(203, 204, 253, 254)) %>%
    pull(item)
  informed <- dat %>%
    filter(part == "II" & grepl("richtig", Bed) & Tastencode %in% c(201, 202, 251, 252)) %>%
    pull(item)
  naive <- dat %>%
    filter(part == "II" & grepl("falsch", Bed) & Tastencode %in% c(203, 204, 253, 254)) %>%
    pull(item)
  # Create a new condition for the experimental condition
  dat %<>% mutate(condition = case_when(
    item %in% excl_known ~ "Excl_known",
    item %in% excl_informed ~ "Excl_informed",
    item %in% excl_naive ~ "Excl_naive",
    item %in% informed ~ "Informed",
    item %in% naive ~ "Naive"
  ) %>%
    factor(levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")))
  # In Experiment 3, we need an additional column for position (upright vs. inverted)
  if ("richtig_inv" %in% dat$Bed) {
    dat %<>% mutate(position = as_factor(ifelse(grepl("inv", Bed), "Inverted", "Upright"))) %>%
      # Return only relevant columns
      select(part, position, condition, participant, item, RT)
  } else {
    # Return only relevant columns
    dat %<>% select(part, condition, participant, item, RT)
  }
  return(dat)
}

# Read behavioral data for Experiment 1
data_exp1 <- list.files("data/exp1/RT", pattern = ".txt", full.names = TRUE) %>% map(read_behav)

# Check average number of stimuli per condition
(stims_exp1 <- map(data_exp1, function(x) {
  table(x$condition) / 3
}) %>% bind_rows() %>% colMeans())
```

```{python exp1-preprocessing}
# Import libraries
import mne
import glob
import os

# Set parameters for EEG preprocessing
preproc_params = dict(
    n_components=15, method="fastica",         # ICA parameters
    l_freq=0.1, h_freq=30,                     # Filter edges
    event_id={"match": 221, "mismatch": 222},  # EEG triggers
    tmin=-0.5, tmax=1.498,                     # Length of epochs
    baseline=(-0.2, 0),                        # Baseline correction
    reject=dict(eeg=200)                       # Rejection threshold
)

# # Debug
# locals().update(preproc_params)
# vhdr_fname = 'data/exp1/EEG/Vp0001.vhdr'

# Define function for preprocessing EEG data
def preproc(vhdr_fname, metadata, n_components, method, l_freq, h_freq, event_id, tmin, tmax, baseline, reject):
    # Read EEG data in microvolts
    raw = mne.io.read_raw_brainvision(vhdr_fname, scale=1e6, preload=True)
    # Create virtual EOG channels
    raw = mne.set_bipolar_reference(raw, "Auge_u", "Fp1", ch_name="VEOG", drop_refs=False)
    raw = mne.set_bipolar_reference(raw, "F9", "F10", ch_name="HEOG", drop_refs=False)
    raw.set_channel_types(mapping={"VEOG": "eog", "HEOG": "eog"})
    # Add EasyCap electrode layout, removing any excessive channels
    montage = mne.channels.make_standard_montage("easycap-M1")
    raw.drop_channels(list(set(raw.ch_names) - set(montage.ch_names) - set(["VEOG", "HEOG"])))
    raw.set_montage(montage=montage)
    # # Interpolate bad channels
    # if vhdr_fname=='data/exp3/EEG/VP05.vhdr':
    #   raw.info['bads'].extend(['AF8', 'TP9'])
    #   raw = raw.interpolate_bads()
    # Re-reference to common average
    raw, _ = mne.set_eeg_reference(raw, "average")
    # Run ICA on a copy of the data
    filt_raw = raw.copy()
    filt_raw.load_data().filter(l_freq=1, h_freq=None)
    ica = mne.preprocessing.ICA(n_components=n_components, random_state=12345, method=method)
    ica.fit(filt_raw)
    # Remove bad components based on correlations with EOG
    eog_indices, eog_scores = ica.find_bads_eog(raw)
    ica.exclude = eog_indices
    raw = ica.apply(raw)
    # Apply band-pass filter
    raw = raw.filter(l_freq=l_freq, h_freq=h_freq)
    # Epoching including baseline correction
    events, _ = mne.events_from_annotations(raw, verbose=False)
    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=baseline, preload=True)
    # Add behavioral data
    epochs.metadata = metadata
    # Reject bad epochs
    epochs = epochs.drop_bad(reject=reject)
    return epochs


# List raw EEG filenames
fnames_exp1 = sorted(glob.glob("data/exp1/EEG/*.vhdr"))

# Import behavioral data from R
metadata_exp1 = r.data_exp1

# Check if preprocessing was done already (delete the file to re-run)
if os.path.exists("analysis/export/exp1-epo.fif"):
    # Read preprocessed data from file
    epochs_exp1 = mne.read_epochs("analysis/export/exp1-epo.fif", preload=True)
else:
    # Preprocess EEG data for Experiment 1
    epochs_exp1 = [
        preproc(vhdr_fname=fname, metadata=meta, **preproc_params) for fname, meta in zip(fnames_exp1, metadata_exp1)
    ]
    # Combine epochs into a single data set
    epochs_exp1 = mne.concatenate_epochs(epochs_exp1)
    # Backup epochs to the export folder
    epochs_exp1.save("analysis/export/exp1-epo.fif")

# # Get indices of bad epochs despite ICA
# rej_exp1 = epochs_exp1.drop_log
# rej_exp1[:] = [item for item in rej_exp1 if item != ['IGNORED']]
# rej_exp1 = [i for i in range(len(rej_exp1)) if rej_exp1[i] != []]

# Define function to compute grand averages per condition
def compute_evokeds(epochs):
    evokeds = dict()
    evokeds_dat = dict()
    # Each part becomes a dictionary of conditions
    for pt in ["I", "II", "III"]:
        evokeds[pt] = dict()
        evokeds_dat[pt] = dict()
        # Each condition becomes a list of participants
        for cn in ["Informed", "Naive"]:
            evokeds[pt][cn] = list()
            evokeds_dat[pt][cn] = list()
            # For every participant we average trials separately for all parts and conditions
            for vp in epochs.metadata["participant"].unique():
                query = 'participant == "' + vp + '" and part == "' + pt + '" and condition == "' + cn + '"'
                evokeds[pt][cn].append(epochs[query].average())
            # Compute grand averages across participants for this part and condition
            evokeds[pt][cn] = mne.grand_average(evokeds[pt][cn])
            # Export only the actual data for R
            evokeds_dat[pt][cn] = evokeds[pt][cn].data
    return (evokeds, evokeds_dat)


# Compute grand averages for Experiment 1
evokeds_exp1, evokeds_dat_exp1 = compute_evokeds(epochs=epochs_exp1)

# # Plot evoked potentials
# import matplotlib.pyplot as plt
# mne.viz.plot_compare_evokeds(evokeds=evokeds_exp1['II'], picks='P8'); plt.show()
# evokeds_exp1['II']['Informed'].data = evokeds_exp1['II']['Informed'].data - evokeds_exp1['II']['Naive'].data
# evokeds_exp1['II']['Informed'].plot_topomap(times=[0.125, 0.175, 0.5], average=0.05, vmin=-1, vmax=1, cmap='viridis'); plt.show()
```

```{r exp1-analysis}
# Re-import behavioral data and re-factorize some columns
data_exp1 <- py$epochs_exp1$metadata %>% mutate(
  part = factor(part, levels = c("I", "II", "III")),
  condition = factor(condition, levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")),
  participant = factor(participant),
  item = factor(item)
)

# Check number of rejected epochs
rejected_exp1 <- data_exp1 %>%
  group_by(participant) %>%
  tally() %>%
  mutate(n = 360 - n) %>%
  pull(n)
mean(rejected_exp1)    # Mean
median(rejected_exp1)  # Median
range(rejected_exp1)   # Range

# Create vectors with sample time points (in ms) and electrodes
tmin <- py$preproc_params$tmin * 1000
tmax <- py$preproc_params$tmax * 1000
sfreq <- py$epochs_exp1$info["sfreq"]
times <- seq(tmin, tmax, 1000 / sfreq)
els <- py$epochs_exp1$ch_names

# Define ERP components of interest
comps <- tibble(
  name = c("P1", "N1", "N400"),
  start = c(100, 150, 400),
  stop = c(150, 200, 700),
  roi = list(
    c("PO3", "PO4", "POz", "O1", "O2", "Oz"),
    c("P7", "P8", "PO7", "PO8", "PO9", "PO10"),
    c("C1", "C2", "Cz", "CP1", "CP2", "CPz")
  )
)

# Define function to compute single-trial mean ERP amplitudes
compute_erps <- function(epochs, els, name, start, stop, roi) {
  erps <- epochs[, which(els %in% roi), which(times %in% start:stop)]
  erps <- apply(erps, 1, mean, na.rm = TRUE)
  return(erps)
}

# Check if single-trial ERPs were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp1-erps.RDS")) {
  # Read data from file
  data_exp1 <- readRDS("analysis/export/exp1-erps.RDS")
} else {
  # Compute single-trial ERPs for Experiment 1
  data_exp1 <- pmap_dfc(comps, compute_erps, epochs = py$epochs_exp1$get_data(), els = els) %>%
    set_names(comps$name) %>%
    cbind(data_exp1, .) %T>%
    saveRDS("analysis/export/exp1-erps.RDS")
}

# Remove the epochs to free up memory
py_run_string("del epochs_exp1")
import("gc")$collect()

# Remove any trials excluded from conditions
data_exp1 %<>% filter(condition %in% c("Informed", "Naive")) %>% droplevels()

# Contrast coding for condition (informed-naive) and part (2-1, 3-1)
contrasts_condition <- t(cbind(c("Informed" = 1, "Naive" = -1)))
contrasts_part <- t(cbind(c("I" = -1, "II" = 1, "III" = 0), c("I" = 0, "II" = -1, "III" = 1)))
contrasts(data_exp1$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_exp1$part) <- MASS::ginv(contrasts_part)

# Set formula and parameters for linear mixed-effects regression models (LMMs)
form_exp12 <- buildmer::tabulate.formula(
  ~ part * condition + (part * condition | participant) + (part * condition | item)
) %>% mutate(block = replace(block, is.na(grouping), "fixed"))
ctrl_params <- lme4::lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e5))

# Set formula for follow-up contrasts
specs_exp12 <- pairwise ~ condition | part

# Define function to compute LMMs and follow-up contrasts for each component
compute_models <- function(dep, formula, data, control, specs) {
  require(lmerTest)
  require(buildmer)
  require(emmeans)
  build <- buildmer(
    control = control,
    REML = FALSE,
    buildmerControl = buildmerControl(
      formula = formula,
      data = data,
      direction = c("backward", "backward"),
      elim = function(logp) exp(logp) >= .20,
      calc.anova = TRUE,
      ddf = "Satterthwaite",
      dep = dep
    )
  )
  emm_options(lmer.df = "Satterthwaite", lmerTest.limit = Inf)
  means <- emmeans(build@model, specs = specs, infer = TRUE, data = data)$emmeans %>% as.data.frame()
  conts <- emmeans(build@model, specs = specs, infer = TRUE, data = data)$contrasts %>% as.data.frame()
  mod <- list(model = build@model, anova = build@anova, summary = build@summary, means = means, contrasts = conts)
  return(mod)
}

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp1-stats.RDS")) {
  # Read models from file
  models_exp1 <- readRDS("analysis/export/exp1-stats.RDS")
} else {
  # Compute LMMs for Experiment 1
  models_exp1 <- map(
    comps$name,
    compute_models,
    formula = form_exp12,
    data = data_exp1,
    control = ctrl_params,
    specs = specs_exp12
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/exp1-stats.RDS")
}
```

## Methods

### Participants

Participants for Experiment 1 were 24 German native speakers (13 female, 11 male) with a mean age of 24 years (range 18 to 31) and no history of psychological disorder or treatment. No a priori power analysis was carreid out and the sample size was chosen according to the default for EEG studies in our lab at the time of data collection. All participants were right-handed according to the Edinburgh inventory [@oldfield1971] and reported normal or corrected-to-normal vision. They gave written informed consent before starting the experiment and received a compensation of €8 per hour for participating. All experiments subsequently reported were carried out in accordance with the Declaration of Helsinki and approved by the local ethics committee.

### Materials and Procedure

Stimuli for Experiments 1 and 2 consisted of 240 grayscale photographs of real-world objects, 120 of which were well-known everyday objects (e.g. a bicycle, a toothbrush), serving as filler stimuli of no interest, whereas the other 120 were rare objects presumed to be unfamiliar for the majority of participants (e.g. a galvanometer, an udu drum). A list of these unfamiliar objects can be found in Appendix A. All stimuli were presented on a light blue background with a size of 207 × 207 pixels on a 19-inch LCD monitor with a resolution of 1,280 × 1,024 pixels and a refresh rate of 75 Hz. At a standardized viewing distance of 90 cm, the images of the objects subtended approximately 3.9 degrees of participants' horizontal and vertical visual angle.

For each unfamiliar object, a pair of keywords---a noun and a verb---was selected describing the object's typical function or use in a way that could typically be related to its visual features and their configuration (e.g. current--measuring, pottery--drumming). As our central experimental manipulation, the presentation of each unfamiliar object was preceded by its correctly matching keywords for half of the objects, whereas the other half were preceded by non-matching keywords belonging to one of the other objects. The matching keywords were expected to induce semantically informed perception (i.e. participants suddenly understanding what kind of object they were viewing), whereas the non-matching keywords were expected hamper such an effect and keep the perception of the object semantically naive. All participants saw each unfamiliar object in only one of the two conditions and the assignment of objects to conditions was counterbalanced across participants. The experiment was programmed and displayed using Presentation® software (Neurobehavioral Systems, Inc., Berkeley, CA, www.neurobs.com).

Each experimental session consisted of three parts (Figure \@ref(fig:exp1-plot)A). In the *pre-insight* part (Part I), after written informed consent had been obtained and the EEG had been prepared, all 240 familiar and unfamiliar objects were presented once in random order and without any keywords. Each trial consisted of a fixation cross presented in the middle of the screen for 0.5 s, followed by the presentation of the object until participants made a response or until a time out after 3 s. The inter-trial interval until the presentation of the next fixation cross was 0.5 s and participants took a self-timed break after each block of 60 objects. The task, which was kept the same throughout all parts and experiments, was to classify each object using one of four response alternatives: (a) "I know what this is or have a strong assumption," (b) "I have an assumption what this is," (c) "I have rather no assumption what this is," or (d) "I do not know what this is and have no assumption." Participants were asked to respond as quickly and as accurately as possible by pressing one out of four buttons with the index or middle finger of their left or right hand, respectively. The mapping of the rating scale to the four buttons (left to right or right to left) was counterbalanced across participants.

(ref:figure-1-caption) Procedure and results of Experiment 1. *(A)* In the pre-insight part (Part I) participants were presented with 120 unfamiliar objects and indicated whether they knew what kind of object they were viewing. In the insight part (Part II), half of these objects were presented with matching keywords (in purple color for illustration), leading to semantically informed perception, and the other half with non-matching keywords (in petrol color for illustration), leading to naive perception. In the post-insight part (Part III), the same objects were presented again without the keywords. *(B)* ERP waveforms and scalp topographies separately for objects with semantically informed versus naive perception in Parts I, II, and III. Semantically informed perception was associated with significantly more negative amplitudes in the N1 component in Part II, significantly less negative amplitudes in the N400 component in Parts II and III, and significantly more positive amplitudes in the P1 component in Part III. Ampl. = amplitude.\newline\**p* \< .05. \*\**p* \< .01. \*\*\**p* \< .001.

```{r exp1-plot, include=TRUE, fig.height=11, fig.cap = "(ref:figure-1-caption)"}
# Manually extract significance levels from contrasts for plotting
stars_exp1 <- list(
  P1 = c(I = NA, II = NA, III = "*"),
  N1 = c(I = NA, II = "*", III = NA),
  N400 = c(I = NA, II = "***", III = "***")
)

# Import grand averaged evoked potentials from Python
evokeds_exp1 <- py$evokeds_dat_exp1

# Retrieve locations for the relevant electrodes from standard montage
fname_montage <- list.files(path = "renv/python", pattern = "easycap-M1.txt", recursive = TRUE, full.names = TRUE)[1]
montage <- py$evokeds_exp1$I$Informed$ch_names %>%
  as_tibble() %>%
  rename(electrode = value) %>%
  left_join(read_tsv(fname_montage) %>% rename(electrode = Site)) %>%
  mutate(
    x = eegUtils:::deg2rad(Theta) * cos(eegUtils:::deg2rad(Phi)),
    y = eegUtils:::deg2rad(Theta) * sin(eegUtils:::deg2rad(Phi))
  )

# Read SVG file with the trial structure (created using MS PowerPoint)
plot_trials <- grImport2::readPicture("analysis/manuscript_files/figure-1a.svg") %>%
  grImport2::pictureGrob(expansion = 0)

# Define function to plot trial structure
plot_trial <- function() {
  require(magick)
  colour_informed <- viridisLite::viridis(1, begin = 0.1, end = 0.1)
  colour_naive <- viridisLite::viridis(1, begin = 0.5, end = 0.5)
  trial_I <- ggplot() +
    coord_fixed() +
    theme_void() +
    xlim(c(0, 74)) +
    ylim(c(-5, 68)) +
    geom_text(aes(x = 37, y = 68, label = "Part I"), size = 14 / .pt, family = "Helvetica", fontface = "bold") +
    geom_rect(aes(xmin = 18, xmax = 38, ymin = 13, ymax = 33), color = "black", fill = "white") +
    geom_text(aes(x = 28, y = 23, label = "+"), size = 5, family = "Helvetica", fontface = "bold") +
    geom_text(aes(x = 28, y = 10, label = "0.5 s"), color = "black", size = 10 / .pt, family = "Helvetica") +
    geom_rect(aes(xmin = 36, xmax = 56, ymin = 26, ymax = 46), color = "black", fill = "white") +
    draw_image("analysis/manuscript_files/potato_masher.png", x = 36, y = 26, width = 20, height = 20) +
    geom_text(aes(x = 47, y = 23, label = "max. 3 s"), color = "black", size = 10 / .pt, family = "Helvetica") +
    geom_segment(aes(x = 46, y = 13, xend = 56, yend = 20.222), arrow = arrow(length = unit(0.2, "cm")),
                 color = "black")
  trial_II <- ggplot() +
    coord_fixed() +
    theme_void() +
    xlim(c(0, 74)) +
    ylim(c(-5, 68)) +
    geom_text(aes(x = 37, y = 68, label = "Part II"), size = 14 / .pt, family = "Helvetica", fontface = "bold") +
    geom_rect(aes(xmin = 0, xmax = 20, ymin = 0, ymax = 20), color = "black", fill = "white") +
    geom_text(aes(x = 10, y = 10.5, label = "+"), size = 5, family = "Helvetica", fontface = "bold") +
    geom_text(aes(x = 10, y = -3, label = "0.5 s"), color = "black", size = 10 / .pt, family = "Helvetica") +
    geom_rect(aes(xmin = 18, xmax = 38, ymin = 13, ymax = 33), color = colour_informed, fill = "white") +
    geom_text(aes(x = 28, y = 23, label = "Potato\nmashing"), color = colour_informed, size = 10 / .pt,
              family = "Helvetica") +
    geom_text(aes(x = 28, y = 10, label = "2.5 s"), color = "black", size = 10 / .pt, family = "Helvetica") +
    geom_rect(aes(xmin = 18, xmax = 38, ymin = 39, ymax = 59), color = colour_naive, fill = "white") +
    geom_text(aes(x = 28, y = 49, label = "Message\nmorsing"), color = colour_naive, size = 10 / .pt,
              family = "Helvetica") +
    geom_text(aes(x = 28, y = 36, label = "or"), color = "black", size = 10 / .pt, family = "Helvetica") +
    geom_rect(aes(xmin = 36, xmax = 56, ymin = 26, ymax = 46), color = "black", fill = "white") +
    geom_text(aes(x = 46, y = 34, label = "*"), size = 8, family = "Helvetica") +
    geom_text(aes(x = 46, y = 23, label = "0.5 s"), color = "black", size = 10 / .pt, family = "Helvetica") +
    geom_rect(aes(xmin = 54, xmax = 74, ymin = 39, ymax = 59), color = "black", fill = "white") +
    draw_image("analysis/manuscript_files/potato_masher.png", x = 54, y = 39, width = 20, height = 20) +
    geom_text(aes(x = 65, y = 36, label = "max. 3 s"), color = "black", size = 10 / .pt, family = "Helvetica",
              lineheight = 1) +
    geom_segment(aes(x = 28, y = 0, xend = 74, yend = 33.222), arrow = arrow(length = unit(0.2, "cm")),
                 color = "black")
  trial_III <- ggplot() +
    coord_fixed() +
    theme_void() +
    xlim(c(0, 74)) +
    ylim(c(-5, 68)) +
    geom_text(aes(x = 37, y = 68, label = "Part III"), size = 14 / .pt, family = "Helvetica", fontface = "bold") +
    geom_rect(aes(xmin = 18, xmax = 38, ymin = 13, ymax = 33), color = "black", fill = "white") +
    geom_text(aes(x = 28, y = 23, label = "+"), size = 5, family = "Helvetica", fontface = "bold") +
    geom_text(aes(x = 28, y = 10, label = "0.5 s"), color = "black", size = 10 / .pt, family = "Helvetica") +
    geom_rect(aes(xmin = 36, xmax = 56, ymin = 26, ymax = 46), color = "black", fill = "white") +
    draw_image("analysis/manuscript_files/potato_masher.png", x = 36, y = 26, width = 20, height = 20) +
    geom_text(aes(x = 47, y = 23, label = "max. 3 s"), color = "black", size = 10 / .pt, family = "Helvetica") +
    geom_segment(aes(x = 46, y = 13, xend = 56, yend = 20.222), arrow = arrow(length = unit(0.2, "cm")),
                 color = "black")
  plot_grid(trial_I, NULL, trial_II, NULL, trial_III, nrow = 1, rel_widths = c(10, 1, 10, 1, 10))
}

# Define function to plot mean ERP amplitudes
plot_bars <- function(data, stars, ymin) {
  parts <- map(c("I", "II", "III"), function(part) {
    # For which components in the current part do we have a significant effect?
    sig <- map(stars, part) %>%
      as.data.frame() %>%
      t() %>%
      as.data.frame() %>%
      set_colnames("label") %>%
      rownames_to_column("comp") %>%
      na.omit()
    # Create bar plot for the current part
    map(c("P1", "N1", "N400"), function(comp) {
      p <- data %>%
        filter(part == !!part) %>%
        Rmisc::summarySEwithin(measurevar = comp, withinvars = "condition", idvar = "participant") %>%
        mutate(comp = !!comp) %>%
        rename(amplitude = !!comp)
    }) %>%
      bind_rows() %>%
      mutate(comp = fct_relevel(comp, "P1", "N1", "N400")) %>%
      ggplot(aes(x = comp, y = amplitude, fill = condition)) +
      # geom_bar(aes(fill = condition), stat = "identity", position = position_dodge()) +
      # annotate("tile", x = sig$comp, y = -0.4, width = 0.45, height = 0.8, colour = "black", fill = "white", size = 0.5) +
      # annotate("rect", xmin = -Inf, xmax = Inf, ymin = -0.2, ymax = Inf, fill = "white") +
      # geom_bar(aes(y = amplitude - se - 0.2), stat = "identity", position = position_dodge()) + scale_fill_manual(values = c("white", "white")) +
      # new_scale_fill() +
      # annotate("label", x = sig$comp, y = -0.96, label = sig$label, size = 7, family = "Helvetica", label.size = 0) +
      geom_bar(aes(fill = condition), stat = "identity", position = position_dodge()) +
      scale_fill_viridis_d(begin = 0.1, end = 0.5) +
      geom_errorbar(aes(ymin = amplitude - se, ymax = amplitude + se),
                    position = position_dodge(width = 0.9), width = 0.5) +
      annotate("text", x = sig$comp, y = ymin, label = sig$label, size = 6, family = "Helvetica") +
      geom_hline(yintercept = 0) +
      coord_cartesian(ylim = c(ymin, ymin + 7)) +
      ylab("ROI ampl.\n(µV)") +
      scale_y_continuous(breaks = seq(trunc(ymin), trunc(ymin) + 6, 2)) +
      theme_classic() +
      theme(
        legend.position = "none",
        panel.grid = element_blank(),
        axis.ticks = element_line(color = "black"),
        axis.title.x = element_blank(),
        axis.title.y = element_text(family = "Helvetica", color = "black", size = 10),
        axis.text = element_text(family = "Helvetica", color = "black", size = 10),
        plot.margin = margin(t = 0.3, r = 0, b = 0.4, l = 0.2, "cm")
      )
  })
  plot_grid(parts[[1]], NULL, parts[[2]], NULL, parts[[3]], nrow = 1, rel_widths = c(10, 1, 10, 1, 10))
}

# Define function to plot ERP waveforms and topographies
plot_erps <- function(comps, evokeds, montage, stars) {
  suppressWarnings(suppressMessages(
    pmap(comps, function(name, start, stop, roi) {
      # N400 gets a different scale than P1 and N1
      ymin <- ifelse(name == "N400", -3, -4)
      # Loop through parts
      parts <- map(c("I", "II", "III"), function(part) {
        # Create (conditions x time points) x electrodes tibble
        data_plot <- evokeds[[part]] %>%
          map(function(data) {
            data %>%
              t() %>%
              as_tibble(.name_repair = "unique") %>%
              set_colnames(montage$electrode) %>%
              mutate(.time = times, roi = rowMeans(select(., all_of(roi))))
          }) %>%
          bind_rows(.id = "condition")
        # Shade background depending on whether the effect is significant or not
        star <- stars[[name]][[part]]
        if (is.na(star)) {
          shade <- annotate("rect", xmin = start, xmax = stop, ymin = ymin + 0.1, ymax = ymin + 11.9, color = "black", fill = NA)
        } else {
          shade <- annotate("rect", xmin = start, xmax = stop, ymin = ymin + 0.1, ymax = ymin + 11.9, color = "black", fill = "gray90")
        }
        xstar <- case_when(str_length(star) == 1 ~ start + 11, str_length(star) == 3 ~ start + 15)
        # # Prepare waveform
        # wave <- ggplot(data = data_plot, aes(x = .time, y = roi, color = condition)) +
        #   annotate("rect", xmin = start, xmax = stop, ymin = -Inf, ymax = ymin + 12, fill = "gray90")
        # # Shade significant area
        # star <- stars[[name]][[part]]
        # if (!is.na(star)) {
        #   wave <- wave + geom_ribbon(
        #     data = data_plot %>% filter(.time >= start & .time <= stop) %>%
        #       group_by(.time) %>% summarise(ymin = min(roi), ymax = max(roi)),
        #     aes(x = .time, xmin = start, xmax = stop, ymin = ymin, ymax = ymax),
        #     inherit.aes = FALSE, fill = viridisLite::viridis(n = 1, begin = 1)
        #   )
        # } +
        # annotate("text", label = star, x = start + (stop - start)/2, y = -3.2, size = 7, family = "Helvetica")}
        # Add waves and styling

        # Plot waveform
        wave <- ggplot(data = data_plot, aes(x = .time, y = roi, color = condition)) +
          shade +
          annotate("text", label = star, x = xstar, y = ymin + 11, size = 6, hjust = 0, family = "Helvetica") +
          annotate("segment", x = -200, xend = 800, y = 0, yend = 0) +
          annotate("segment", x = 0, xend = 0, y = ymin, yend = ymin + 12) +
          annotate("segment", x = seq(-100, 700, 200), xend = seq(-100, 700, 200), y = -0.3, yend = 0) +
          annotate("segment", x = -12, xend = 0, y = seq(-2, 8, 4), yend = seq(-2, 8, 4)) +
          annotate("text", x = seq(-100, 700, 200), y = -0.9, label = seq(-100, 700, 200), size = 10 / .pt, family = "Helvetica") +
          annotate("text", x = -20, y = seq(-2, 8, 4), label = seq(-2, 8, 4), size = 10 / .pt, family = "Helvetica", hjust = 1) +
          annotate("text", x = 400, y = ymin + 0.8, label = "Time (ms)", size = 10 / .pt, family = "Helvetica", lineheight = 0.9) +
          annotate("text", x = -100, y = 4, label = paste(name, "ampl.\n(µV)"), size = 10 / .pt, family = "Helvetica", angle = 90, lineheight = 0.9) +
          geom_line() +
          scale_color_viridis_d(begin = 0.1, end = 0.5) +
          coord_cartesian(xlim = c(-200, 800), ylim = c(ymin, ymin + 14), expand = FALSE) +
          theme_void() +
          theme(legend.position = "none")
        # Plot topography
        topo <- data_plot %>%
          filter(.time >= start & .time <= stop) %>%
          group_by(condition) %>%
          summarise(across(montage$electrode, mean)) %>%
          pivot_longer(-condition) %>%
          pivot_wider(names_from = condition) %>%
          transmute(amplitude = Informed - Naive) %>%
          bind_cols(montage) %>%
          eegUtils::topoplot(limits = c(-1, 1), palette = "viridis", contour = FALSE, highlights = roi, scaling = 0.1) +
          theme(legend.position = "none")
        topo$layers[[3]]$aes_params$size <- topo$layers[[4]]$aes_params$size <- topo$layers[[5]]$aes_params$size <- 0.6
        # Combine waveform and topography
        wave + draw_plot(topo, width = 320, height = 9, x = 500, y = ymin + 6.4)
      })
      # Combine plots for the different parts
      plot_grid(parts[[1]], NULL, parts[[2]], NULL, parts[[3]], nrow = 1, rel_widths = c(10, 1, 10, 1, 10))
      # Combine plots for the different components
    }) %>% plot_grid(plotlist = ., nrow = 3)
  ))
}

# Creata a combined legend and colorbar
legends <- function(direction){
  if (direction == "vertical"){mar <- margin(l = 6, r = 4, b = 10, t = 6); vjust <- 3.5}
  else {mar <- margin(l = 5, r = 14, b = 3, t = 5); vjust <- 0.7}
  get_legend(
    tibble(amplitude = c(-1, 1), condition = c("Informed", "Naive")) %>% 
      ggplot(aes(x = 0, y = 0, color = condition, fill = amplitude)) +
      geom_line() +
      geom_raster() +
      labs(color = "Conditions", fill = "Informed - naive (µV)") +
      scale_color_viridis_d(
        begin = 0.1, end = 0.5,
        guide = guide_legend(direction = "vertical", title.position = "top")
      ) +
      scale_fill_viridis_c(
        guide = guide_colorbar(direction = direction, ticks = FALSE, title.position = "top", title.vjust = vjust)
      ) +
      theme(
        legend.direction = direction,
        legend.box = direction,
        legend.box.background = element_rect(colour = "black", size = 0.5),
        legend.box.margin = mar,
        legend.background = element_blank(),
        legend.title = element_text(family = "Helvetica", size = 10, face = "bold"),
        legend.text = element_text(family = "Helvetica", size = 10),
        legend.key = element_rect(fill = NA),
        legend.key.width = unit(0.8, "cm"),
        legend.key.height = unit(0.4, "cm"),
        legend.spacing.x = unit(0.3, "cm")
      ))
}

# Create plot for Experiment 1
plot_grid(
  #plot_trial(),
  plot_trials,
  #plot_bars(data = data_exp1, stars = stars_exp1, ymin = -1.5),
  plot_erps(comps = comps, evokeds = evokeds_exp1, montage = montage, stars = stars_exp1),
  #plot_legends(),
  #nrow = 4, rel_heights = c(3, 2, 6, 1), labels = c("A", "B", "C", ""), label_fontfamily = "Helvetica"
  nrow = 2, rel_heights = c(5, 6), labels = c("A", "B"), label_fontfamily = "Helvetica"
  #nrow = 4, rel_heights = c(4, 2, 5, 1), labels = c("A", "B", "C", ""), label_fontfamily = "Helvetica"
) + # Add some lines to separate the parts (+ the legends)
  annotate(geom = "segment", x = 0.328, xend = 0.328, y = -Inf, yend = 0.52, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.672, y = -Inf, yend = 0.52, linetype = "dashed") +
  annotate(geom = "segment", x = 0.328, xend = 0.28, y = 0.52, yend = 0.57, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.72, y = 0.52, yend = 0.57, linetype = "dashed") +  
  annotate(geom = "segment", x = 0.28, xend = 0.28, y = 0.57, yend = Inf, linetype = "dashed") +
  annotate(geom = "segment", x = 0.72, xend = 0.72, y = 0.57, yend = Inf, linetype = "dashed") +
  draw_plot(legends(direction = "vertical"), x = 0.41, y = 0.397, width = 1, height = 1)
```

In the *insight* part (Part II), only the 120 unfamiliar objects were presented for a second time, now preceded either by matching keywords (leading to semantically informed perception) or by non-matching keywords (leading to naive perception). Each trial consisted of a fixation cross presented for 0.5 s, followed by the presentation of the keywords for 2.5 s. Then, an asterisk was presented in the middle of the screen for another 0.5 s, followed by the presentation of the object until a response was made or until a time out after 3 s. The objects were presented in blocks of 30 trials so that within each block (a) there were 15 objects from each of the two experimental conditions and (b) objects were heterogeneous in terms of their shape, visual complexity, and functional category (e.g. medical devices, musical instruments).

In the post-insight part (Part III), the unfamiliar objects were presented for a third time with an identical trial structure as in Part I, i.e. without any keywords. Note that Parts II and III were presented in an interleaved fashion so that after the presentation of one block of 30 objects in Part II (with keywords), participants took a self-timed break and continued with the same block of 30 objects in Part III (without keywords) before moving on to the next block consisting of 30 different objects. They continued like this until all four blocks were completed in both Parts II and III. In total, the experiment consisted of 480 trials (120 familiar objects in Part I and 120 unfamiliar objects in Parts I, II, and III) and took participants approximately 35 minutes to complete.

### EEG Recording and Preprocessing

The continuous EEG was recorded from 62 Ag/AgCl scalp electrodes placed according to the extended 10--20 system [@americanelectroencephalographicsociety1991] and referenced online to an external electrode placed on the left mastoid (M1). Two additional external electrodes were placed on the right mastoid (M2) and below the left eye (IO1), respectively. During the recording, electrode impedance was kept below 5 kΩ. An online band-pass filter with a high-pass time-constant of 10 s (0.016 Hz) and a low-pass cutoff frequency of 1,000 Hz was applied before digitizing the signal at a sampling rate of 500 Hz.

Offline, the data were preprocessed using the MNE software [Version 0.21.0; @gramfort2013] in Python [Version 3.8.5; @vanrossum2009]. First, all scalp electrodes were re-referenced to the common average. Next, artifacts resulting from blinks and eye movements were removed using independent component analysis (ICA). The first 15 components were extracted by the FastICA algorithm [@hyvärinen1999] after temporarily low-pass filtering the data at 1 Hz. Those components showing substantive correlations with either of two virtual EOG channels (VEOG: IO1 minus Fp1, HEOG: F9 minus F10) were removed automatically using the *find\_bads\_eog* function. After artifact correction, a zero-phase, non-causal FIR filter with a lower pass-band edge at 0.1 Hz (transition bandwidth: 0.1 Hz) and an upper pass-band edge at 30 Hz (transition bandwidth: 7.5 Hz) was applied. Next, the continuous EEG was epoched into segments of 2,000 ms, starting 500 ms before the onset of the visual presentation of each unfamiliar object. The epochs were baseline-corrected by subtracting the average voltage during the 200 ms before stimulus onset. Epochs containing artifacts despite ICA, defined as peak-to-peak amplitudes exceeding 200 µV, were removed from further analysis. This led to the exclusion of an average of `r format(round(mean(rejected_exp1), 1), nsmall = 1)` trials (`r scales::percent(mean(rejected_exp1)/360, accuracy = 0.1)`) per participant (range `r min(rejected_exp1)` to `r max(rejected_exp1)`). Single-trial event-related potentials were computed as the mean amplitude across time windows and regions of interests (ROIs) defined a priori, namely 100--150 ms after object onset at electrodes PO3, PO4, POz, O1, O2, and Oz for the P1 component, 150--200 ms after object onset at electrodes P7, P8, PO7, PO8, PO9, and PO10 for the N1 component, and 400--700 ms at electrodes C1, C2, Cz, CP1, CP2, and CPz for the N400 component.

### Statistical Analysis

First, because we were interested in the effects of knowledge on perceiving *unfamiliar* objects only, we excluded from all furthere analyses those objects which participants classified as being known to them in Part I (i.e. before any keywords were presented), which led to the exclusion of an average of `r format(round(stims_exp1["Excl_known"], 1), nsmall = 1)` objects per participant (`r scales::percent(stims_exp1["Excl_known"]/120, accuracy = 0.1)`). Next, to clearly delineate semantically informed and naive perception, the assignment of all other objects to one of these two conditions for statistical analyses was co-determined by our experimental manipulation (matching versus non-matching keywords in Part II) and the behavioral responses of the participants themselves (Figure 1A). Objects were assigned to the semantically informed condition only if they were presented with matching keywords *and* if participants indicated knowing what the object was or having an assumption. This was the case for an average of `r format(round(stims_exp1["Informed"], 1), nsmall = 1)` objects per participant (`r scales::percent(stims_exp1["Informed"]/60, accuracy = 0.1)` of objects presented with matching keywords). Complementarily, objects were assigned to the naive condition only if they were presented with non-matching keywords *and* if participants indicated not knowing what the object was or having rather no assumption. This was the case for an average of `r format(round(stims_exp1["Naive"], 1), nsmall = 1)` objects per participant (`r scales::percent(stims_exp1["Naive"]/60, accuracy = 0.1)` of objects presented with non-matching keywords). Although the assignment was purely based on the manipulation and responses in Part II, when insight was thought to occurr, the same assignment was used to analyze the data from the other two parts. This allowed us to test on the one hand if the objects from both conditions differed in important aspects even before any keywords were presented (in Part I), and on the other hand if the semantic understanding acquired in Part II had any lasting effects on a subsequent, third presentation of the objects (in Part III).

The event-related potentials in response to objects from both conditions and all three parts were analyzed on the single trial level using linear mixed-effects regression models [@baayen2008; @frömer2018]. For the purpose of the present study, these models have at least two desirable properties compared to more traditional approaches, such as analyses of variance (ANOVAs) performed on by-participant grand averages. First, they can account simultaneously for the non-independence of data points coming from the same participant or from the same item, whereas the neglect of the item as a random variable in ANOVAs leads to anti-conservative test statistics and strictly does not allow for inferences beyond the stimulus set under study [@bürki2018; @judd2012]. Second, they can flexibly deal with unbalanced designs in which the number of trials differs across (combinations of) conditions, which is inevitable in designs where the assignment of trials to conditions is (co-)determined by the responses of the participants rather than by the experimental manipulation alone [e.g.  @fröber2017].

Three separate models were computed predicting P1, N1, and N400 mean amplitudes, respectively. All models included three fixed effects: (a) the part of the experiment, coded as a repeated contrast (i.e. subtracting Part I from Part II and Part II from Part III, the intercept being the grand mean across all three parts), (b) the condition of the object, coded as a scaled sum contrast (i.e. subtracting the naive from the semantically informed condition, the intercept being the grand mean across both conditions), and (c) the two-way interaction of part and condition. For details on these and other contrast coding schemes in linear (mixed-effects) models, please refer to [@schad2020]. To determine the random effects structure, we always started with a maximal model containing by-participant and by-item random intercepts and random slopes for all fixed effects [@barr2013]. We then performed a model selection algorithm as proposed by [@matuschek2017] in order to increase statistical power and avoid overparameterization: Iteratively, each random effects was removed and the resulting, more parsimonious model was compared to the previous, more complex model by means of a likelihood ratio test. Only if the parsimonious model explained the data equally well as the complex model [determined by *p* \> .20; @matuschek2017] did we leave the random effect out, otherwise it was kept in the final model. All models were calculated in R [Version `r packageVersion("base")`; @R-base] using the lme4 package [Version `r packageVersion("lme4")`; @R-lme4]. The optimizer function *bobyqa* with 2·10^5^ iterations was used for maximum likelihood estimation. The model selection algorithm via likelihood ratio tests was performed using the buildmer package [Version `r packageVersion("buildmer")`; @R-buildmer]. Finally, to answer our research question of whether or not semantically informed perception had an influence on the ERP components within each part, planned follow-up comparisons were calculated, contrasting the informed against the naive condition within Parts I, II, and III. All *p* values subsequently reported were computed by approximating the relevant denominator degrees of freedom using Satterthwaite's methods as implemented in the lmerTest package [@R-lmerTest].

The materials, single trial behavioral and ERP data, and all code for data analysis can be accessed via the Open Science Framework ([https://osf.io/.../](https://osf.io/myprojects/)). To avoid potential misuse, the raw EEG are only made available by the corresponding author upon request.

## Results

Single-trial ERPs were analyzed in response to unfamiliar objects before (Part I), while (Part II), and after (Part III) participants obtained relevant semantic information about their function. Only in Part II, half of the objects were preceded by a matching description, fostering semantically informed perception to occur, whereas the other half were preceded by a non-matching description, leading to naive perception of the object. The objects were analyzed according to this manipulation in combination with participants' self-report in Part II (Figure 1A), thus making sure that semantically informed (or naive) perception did indeed occur. The analysis focused on differences between these two conditions in the P1 component (100--150 ms) as an index of lower-level visual perception, in the N1 component (150--200 ms) as index of higher-level visual perception, and in the N400 component (400--700 ms) as an index of semantic processing.

```{r exp1-table, include=TRUE, results="asis"}
# Define function to print ANOVA-style table
create_table <- function(models, stub_anova, stub_contrasts, caption, note) {
  anov <- map(models, function(model) {
    data.frame(
      "f" = format(round(model$anova$`F value`, 2), trim = FALSE, nsmall = 2),
      "df" = paste0("(", model$anova$NumDF, ", ", format(round(model$anova$DenDF, 1), trim = TRUE, nsmall = 1), ")"),
      "p" = format(round(model$anova$`Pr(>F)`, 3), trim = FALSE, nsmall = 3)
    ) %>%
      mutate(p = ifelse(p == "0.000", "< .001", substr(p, 2, nchar(p)))) %>%
      set_rownames(c(stub_anova))
  })
  anov_print <- anov %>%
    map(unite, col = fdf, f, df, sep = " ") %>%
    map(add_row, fdf = "\\textit{F} (\\textit{df})", p = "\\textit{p}", .before = 1) %>%
    bind_cols() %>%
    set_rownames(c("\\textit{Fixed effects}", stub_anova))
  anov %<>% 
    bind_cols() %>% 
    set_colnames(paste(rep(names(models), each = length(names(models))), c("f", "df", "p"), sep = "_"))
  conts <- map(models, function(model) {
    data.frame(
      "est" = format(round(model$contrasts$estimate, 2), trim = FALSE, nsmall = 2),
      "ci" = paste0(
        "[", format(round(model$contrasts$lower.CL, 2), trim = TRUE, nsmall = 2), ", ",
        format(round(model$contrasts$upper.CL, 2), trim = TRUE, nsmall = 2), "]"
      ),
      "p" = format(round(model$contrasts$`p.value`, 3), trim = FALSE, nsmall = 3)
    ) %>%
      mutate(p = ifelse(p == "0.000", "< .001", substr(p, 2, nchar(p)))) %>%
      set_rownames(stub_contrasts)
  })
  conts_print <- conts %>%
    map(unite, col = estci, est, ci, sep = " ") %>%
    map(add_row, estci = "Est. [95% CI]", p = "\\textit{p}", .before = 1) %>%
    bind_cols() %>%
    set_rownames(c("\\textit{Informed $-$  naive}", stub_contrasts))
  conts %<>% 
    bind_cols() %>% 
    set_colnames(paste(rep(names(models), each = length(names(models))), c("est", "ci", "p"), sep = "_"))
  cnames <- c("\\textit{Fixed effects}", as.character(anov_print[1, ]))
  list(anov_print[2:nrow(anov_print), ], conts_print) %>%
    map(set_colnames, paste0("V", 1:ncol(anov_print))) %>%
    apa_table(
      col.names = cnames, col_spanners = list("\\textbf{P1}" = 2:3, "\\textbf{N1}" = 4:5, "\\textbf{N400}" = 6:7),
      midrules = nrow(anov_print), font_size = "footnotesize", align = "lcccccc", escape = FALSE,
      caption = caption, note = note
    ) %>% cat()
  return(list("anov" = anov, "conts" = conts))
}

# Create table of models for Experiment 1
table_exp1 <- create_table(
  models = models_exp1,
  stub_anova = c("Part", "Condition", "Pt. × con."),
  stub_contrasts = c("Part I", "Part II", "Part III"),
  caption = "Results of linear mixed-effects regression models for Experiment 1",
  note = "Pt. = part, con. = condition, est. = estimate, CI = confidence interval."
)
```

Averaged across conditions, P1, N1, and N400 amplitudes differed as a function of the part of the experiment, all *F*s \> `r table_exp1$anov["Part", "P1_f"]`, all *p*s `r table_exp1$anov["Part", "P1_p"]` (Table \@ref(tab:exp1-table)). In addition, N400 amplitudes differed between the informed and the naive condition averaged across the three parts of the experiment, *F*`r table_exp1$anov["Informed", "N400_df"]` = `r table_exp1$anov["Informed", "N400_f"]`, *p* = `r table_exp1$anov["Informed", "N400_p"]`. Crucially, the part × condition interaction was significant in the N1 component, *F*`r table_exp1$anov["Pt. × con.", "N1_df"]` = `r table_exp1$anov["Pt. × con.", "N1_f"]`, *p* = `r table_exp1$anov["Pt. × con.", "N1_p"]`, and in the N400 component, *F*`r table_exp1$anov["Pt. × con.", "N400_df"]` = `r table_exp1$anov["Pt. × con.", "N400_f"]`, *p* `r table_exp1$anov["Pt. × con.", "N400_p"]`, while also being marginally significant in the P1 component, *F*`r table_exp1$anov["Pt. × con.", "P1_df"]` = `r table_exp1$anov["Pt. × con.", "P1_f"]`, *p* = `r table_exp1$anov["Pt. × con.", "P1_p"]`. To answer our main research question, we decomposed these interactions into the differences between the semantically informed condition and the naive condition within the three different parts of the experiment.

### ERPs in Part I (Pre-Insight)

In Part I, when objects were unfamiliar to the participant and presented without keywords, no differences emerged between the semantically informed and the naive condition in the P1, N1, or N400 component, all *p*s \> `r table_exp1$conts["Part I", "N400_p"]` (Table \@ref(tab:exp1-table), Figure \@ref(fig:exp1-plot)B & C). On the one hand, this was to be expected given that the critical presentation of the keywords (leading to semantically informed vs. naive perception) had not yet taken place. On the other hand, the absence of reliable differences in Part I can be taken as evidence---with the usual caveats when interpreting null effects---that any subsequent effect of the semantic information in Parts II and III cannot be accounted for by visual differences between the objects in the two conditions. Although the presentation of a matching or non-matching keywords for each object was counterbalanced across participants, the fact that different numbers of objects were assigned to the two conditions based on participants' self report in Part II would have made it possible for such visual differences to emerge as a confounding factor. If they did, however, one would expect to detect these differences even before any keywords were presented, which we have now seen was not the case.

### ERPs in Part II (Insight)

In Part II, half of the unfamiliar objects were presented with matching keywords (for forming the semantically informed condition) and the other half were presented with non-matching keywords (for forming the naive condition). When semantic information informed the perception of the object (akin to an insight experience), the amplitude of the N1 component was significantly enlarged (i.e. more negative), *b* = `r table_exp1$conts["Part II", "N1_est"]` µV, *p* = `r table_exp1$conts["Part II", "N1_p"]`, and the amplitude of the N400 component was significantly reduced (i.e. less negative), *b* = `r table_exp1$conts["Part II", "N400_est"]` µV, *p* `r table_exp1$conts["Part II", "N400_p"]`, compared to when an object was viewed naively without relevant semantic information. As in Part I, there were no reliable differences in the P1 component, *p* = `r table_exp1$conts["Part II", "P1_p"]`.

### ERPs in Part III (Post-Insight)

In Part III, the unfamiliar objects were presented for a third time, again without the keywords (as in Part I), to test whether the semantic information had any lasting effects on the processing of the objects. As in Part II, the N400 component remained significantly reduced during semantically informed as compared to naive perception, *b* = `r table_exp1$conts["Part III", "N400_est"]` µV, *p* `r table_exp1$conts["Part III", "N400_p"]`, whereas the early effect in the N1 component did not reoccur, *p* = `r table_exp1$conts["Part III", "N1_p"]`. Instead, we now observed an even earlier modulation in the P1 component, which was significantly enlarged (i.e. more positive) in response to objects for which semantically informed perception had taken place, *b* = `r table_exp1$conts["Part III", "P1_est"]` µV, *p* = `r table_exp1$conts["Part III", "P1_p"]`.

## Discussion

In Experiment 1, we measured event-related brain potentials from participants viewing unfamiliar objects before (Part I), while (Part II) and after (Part III) they were able to understand what kind of object they saw. To induce this semantically informed perception, half of the objects in Part II were preceded by matching verbal keywords about their typical function or use, whereas the other half were preceded by non-matching keywords, serving as a naive baseline condition.

Participants' semantically informed perception of the objects in Part II was associated with a significantly enlarged N1 component, indicating that the sudden acquisition of knowledge about the object influenced aspects of its higher-level visual processing [@rossion2011; @tanaka2001]. The fact that this effect did not reoccur in Part III, when the objects were presented once more without keywords, suggests it being a marker of insight through semantic information that is altering object processing online. In contrast, we also observed a modulation of the N400 component, which was reduced for objects while insight was taking place (in Part II) and remained so when the same objects were presented repeatedly (in Part III). The N400 is most often discussed as an index of increased demands for semantic processing or integration [@kutas2011; @lau2008; @rabovsky2018]. Its reduction can thus be interpreted as lowered semantic processing demands in response to unfamiliar objects once participants had understood what kind of object they were viewing. Finally, semantically informed perception was also associated with increased amplitudes in the P1 component, but only once the objects were presented for a third time (in Part III), after the critical presentation during which insight had occurred (in Part II). This effect, which replicates previous work on obtaining knowledge about previously unfamiliar images [e.g.  @samaha2018], may be associated with the newly acquired semantic knowledge exerting an influence on lower-level visual perception, either online or through altered visual representations of the objects for which insight had occurred.

Because of the exploratory nature of the present study and the novelty of the ERP effects observed in Experiment 1, we ran a replication study to assess the robustness of these findings in another samples of participants.

# Experiment 2

## Methods

```{r exp2-preparation}
# Read behavioral data for Experiment 2
data_exp2 <- list.files("data/exp2/RT", pattern = ".txt", full.names = TRUE) %>% map(read_behav)

# Check average number of stimuli per condition
(stims_exp2 <- map(data_exp2, function(x) {
  table(x$condition) / 3
}) %>% bind_rows() %>% colMeans())
```

```{python exp2-preprocessing}
# List raw EEG filenames
fnames_exp2 = sorted(glob.glob("data/exp2/EEG/*.vhdr"))

# Import behavioral data from R
metadata_exp2 = r.data_exp2

# Check if preprocessing was done already (delete the file to re-run)
if os.path.exists("analysis/export/exp2-epo.fif"):
    # Read preprocessed data from file
    epochs_exp2 = mne.read_epochs("analysis/export/exp2-epo.fif", preload=True)
else:
    # Preprocess EEG data for Experiment 2
    epochs_exp2 = [
        preproc(vhdr_fname=fname, metadata=meta, **preproc_params)
        for fname, meta in zip(fnames_exp2, metadata_exp2)
    ]
    # Combine epochs into a single data set
    epochs_exp2 = mne.concatenate_epochs(epochs_exp2)
    # Backup epochs to the export folder
    epochs_exp2.save("analysis/export/exp2-epo.fif")

# # Get indices of bad epochs despite ICA
# rej_exp2 = epochs_exp2.drop_log
# rej_exp2[:] = [item for item in rej_exp2 if item != ['IGNORED']]
# rej_exp2 = [i for i in range(len(rej_exp2)) if rej_exp2[i] != []]

# Compute grand averages for Experiment 2
evokeds_exp2, evokeds_dat_exp2 = compute_evokeds(epochs=epochs_exp2)
```

```{r exp2-analysis}
# Re-import behavioral data and re-factorize some columns
data_exp2 <- py$epochs_exp2$metadata %>% mutate(
  part = factor(part, levels = c("I", "II", "III")),
  condition = factor(condition, levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")),
  participant = factor(participant),
  item = factor(item)
)

# Check number of rejected epochs
rejected_exp2 <- data_exp2 %>%
  group_by(participant) %>%
  tally() %>%
  mutate(n = 360 - n) %>%
  pull(n)
mean(rejected_exp2)    # Mean
median(rejected_exp2)  # Median
range(rejected_exp2)   # Range

# Check if single-trial ERPs were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp2-erps.RDS")) {
  # Read data from file
  data_exp2 <- readRDS("analysis/export/exp2-erps.RDS")
} else {
  # Compute single-trial ERPs for Experiment 2
  data_exp2 <- pmap_dfc(comps, compute_erps, epochs = py$epochs_exp2$get_data(), els = els) %>%
    set_names(comps$name) %>%
    cbind(data_exp2, .) %T>%
    saveRDS("analysis/export/exp2-erps.RDS")
}

# Remove the epochs to free up memory
py_run_string("del epochs_exp2")
import("gc")$collect()

# Remove any trials excluded from conditions
data_exp2 %<>% filter(condition %in% c("Informed", "Naive")) %>% droplevels()

# Contrast coding for condition (informed-naive) and part (2-1, 3-1)
contrasts(data_exp2$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_exp2$part) <- MASS::ginv(contrasts_part)

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp2-stats.RDS")) {
  # Read models from file
  models_exp2 <- readRDS("analysis/export/exp2-stats.RDS")
} else {
  # Compute LMMs for Experiment 2
  models_exp2 <- map(
    comps$name,
    compute_models,
    formula = form_exp12,
    data = data_exp2,
    control = ctrl_params,
    specs = specs_exp12
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/exp2-stats.RDS")
}
```

### Participants

Participants for Experiment 2 were 24 German native speakers (15 female, 9 male) with a mean age of 26 years (range 19 to 29) who had not participated in Experiment 1. They had no history of psychological disorder or treatment, were right-handed and reported normal or corrected-to-normal vision. They gave written informed consent before starting the experiment and received a compensation of €8 per hour for participating.

### Materials, Procedure, and Analysis

All materials, procedures, EEG-related methods, and statistical analyses were identical to Experiment 1. An average of `r format(round(stims_exp2["Excl_known"], 1), nsmall = 1)` objects per participant (`r scales::percent(stims_exp2["Excl_known"]/120, accuracy = 0.1)`) was classified as being known in Part I and excluded from all further analyses. Based on participants' responses in Part II, an average of `r format(round(stims_exp2["Informed"], 1), nsmall = 1)` objects were assigned to the semantically informed condition (`r scales::percent(stims_exp2["Informed"]/60, accuracy = 0.1)` of objects presented with matching keywords) and an average of `r format(round(stims_exp2["Naive"], 1), nsmall = 1)` objects were assigned to the naive condition (`r scales::percent(stims_exp2["Naive"]/60, accuracy = 0.1)` of objects presented with a non-matching description). Automatic rejection of EEG epochs containing artifacts led to the exclusion of `r format(round(mean(rejected_exp2), 1), nsmall = 1)` trials per participant (`r scales::percent(mean(rejected_exp2)/360, accuracy = 0.1)`, range `r min(rejected_exp2)` to `r max(rejected_exp2)`).

## Results

```{r exp2-table, include=TRUE, results="asis"}
# Create table of models for Experiment 2
table_exp2 <- create_table(
  models_exp2,
  stub_anova = c("Part", "Condition", "Pt. × con."),
  stub_contrasts = c("Part I", "Part II", "Part III"),
  caption = "Results of linear mixed-effects regression models for Experiment 2",
  note = "Pt. = part, con. = condition, est. = estimate, CI = confidence interval."
)
```

As in Experiment 1, P1, N1, and N400 amplitudes differed between the three different parts of the experiments, all *F*s \> `r table_exp2$anov["Part", "N1_f"]`, all *p*s `r table_exp2$anov["Part", "N1_p"]` (Table \@ref(tab:exp2-table)). Also as in Experiment 1, N400 amplitudes differed between the informed and the naive condition averaged across parts, *F*`r table_exp2$anov["Informed", "N400_df"]` = `r table_exp2$anov["Informed", "N400_f"]`, *p* `r table_exp2$anov["Informed", "N400_p"]`. The part × condition interaction was significant in the P1 component, *F*`r table_exp2$anov["Pt. × con.", "P1_df"]` = `r table_exp2$anov["Pt. × con.", "P1_f"]`, *p* = `r table_exp2$anov["Pt. × con.", "P1_p"]`, and in the N400 component, *F*`r table_exp2$anov["Pt. × con.", "N400_df"]` = `r table_exp2$anov["Pt. × con.", "N400_f"]`, *p* `r table_exp2$anov["Pt. × con.", "N400_p"]`, but not in the N1 component, *F*`r table_exp2$anov["Pt. × con.", "N1_df"]` = `r table_exp2$anov["Pt. × con.", "N1_f"]`, *p* = `r table_exp2$anov["Pt. × con.", "N1_p"]`.

(ref:figure-2-caption) Results of Experiment 2. ERP waveforms and scalp topographies separately for objects for which participants experienced semantically informed versus naive perception in Parts I, II, and III. In a direct replication of Experiment 1, the effect of semantic information on the N400 component in Parts II and III and on the P1 component in Part III remained statistically significant, while the effect on the N1 component in Part II remained only marginally significant. Ampl. = amplitude.\newline\**p* \< .05. \*\*\**p* \< .001.

```{r exp2-plot, include=TRUE, fig.height=7.5, fig.cap = "(ref:figure-2-caption)"}
# Manually extract significance levels from contrasts for plotting
stars_exp2 <- list(
  P1 = c(I = NA, II = NA, III = "*"),
  N1 = c(I = NA, II = NA, III = NA),
  N400 = c(I = NA, II = "***", III = "*")
)

# Import grand averaged evoked potentials from Python
evokeds_exp2 <- py$evokeds_dat_exp2

# Create headings for Parts I, II, and III
plot_headings <- function(){
  plot_grid(
    ggplot() + annotate("text", label = "Part I (pre-insight)", x = 0, y = 0, size = 14 / .pt, family = "Helvetica", fontface = "bold") + theme_void(), NULL,
    ggplot() + annotate("text", label = "Part II (insight)", x = 0, y = 0, size = 14 / .pt, family = "Helvetica", fontface = "bold") + theme_void(), NULL,
    ggplot() + annotate("text", label = "Part III (post-insight)", x = 0, y = 0, size = 14 / .pt, family = "Helvetica", fontface = "bold") + theme_void(),
    nrow = 1, rel_widths = c(10, 1, 10, 1, 10)
  )
}

# Create plot for Experiment 2
plot_grid(
  #plot_bars(data = data_exp2, stars = stars_exp2, ymin = -2.5),
  plot_headings(),
  plot_erps(comps = comps, evokeds = evokeds_exp2, montage = montage, stars = stars_exp2),
  legends(direction = "horizontal"),
  nrow = 3, rel_heights = c(0.3, 6, 1.2), labels = NULL
) +
  annotate(geom = "segment", x = 0.328, xend = 0.328, y = 0.15, yend = Inf, linetype = "dashed") +
  annotate(geom = "segment", x = 0.672, xend = 0.672, y = 0.15, yend = Inf, linetype = "dashed")
```

### ERPs in Part I (Pre-Insight)

As in Experiment 1, no differences between objects in the semantically informed and the naive condition emerged in the P1, N1, or N400 component, all *p*s \> `r table_exp2$conts["Part I", "P1_p"]` (Table \@ref(tab:exp2-table), Figure \@ref(fig:exp2-plot)).

### ERPs in Part II (Insight)

As in Experiment 1, semantically informed as compared to naive perception (induced by matching versus non-matching keywords) was associated with a (marginally) significant enhancement of the N1 component, *b* = `r table_exp2$conts["Part II", "N1_est"]` µV, *p* = `r table_exp2$conts["Part II", "N1_p"]`, and a significant reduction of the N400 component, *b* = `r table_exp1$conts["Part II", "N400_est"]` µV, *p* `r table_exp1$conts["Part II", "N400_p"]`.

### ERPs in Part III (Post-Insight)

As in Experiment 1, the presentation of the same unfamiliar objects for a third time (without keywords, as in Part I) led to significantly larger amplitudes in the P1 component in response to objects for which semantically informed perception had occurred, *b* = `r table_exp2$conts["Part III", "P1_est"]` µV, *p* = `r table_exp2$conts["Part III", "P1_p"]`. Also, N400 amplitudes in response to these objects remained significantly reduced, *b* = `r table_exp2$conts["Part III", "N400_est"]` µV, *p* = `r table_exp2$conts["Part III", "N400_p"]`.

### Joint Analysis of Experiments 1 and 2

In an attempt to maximize statistical power, we combined the ERP data sets from Experiments 1 and 2. This allowed us to determine (a) if the above effects---including the marginally significant ones---were reliable when tested in a larger sample, and (b) if there were significant differences in the ERP amplitudes between Experiments 1 and 2. Methods for statistical analysis were kept unchanged apart from the addition of a new factor denoting the experiment, coded as a scaled sum contrast [i.e. subtracting Experiment 1 from Experiment 2, the intercept being the grand mean across both experiments; @schad2020]. This factor and its possible interactions with part, condition, and part × condition were included in the linear mixed-effects regression models as fixed effects and as potential by-item random slopes. They were not included as by-participant random slopes since different participants took part in Experiments 1 and 2. Note that, as above, random effects were eventually included only if their omission led to a significant decline in model fit [@matuschek2017; @R-buildmer].

```{r joint-analysis}
# Combine data from Experiments 1 and 2 (requires changing the participant IDs for Experiment 2)
data_joint <- data_exp2 %>%
  mutate(participant = fct_relabel(participant, ~ paste0(., "_2"))) %>%
  bind_rows(data_exp1, ., .id = "experiment") %>%
  mutate(experiment = as_factor(experiment))

# Contrast coding for experiment (2-1), condition (informed-naive), and part (2-1, 3-1)
t(contrasts_experiment <- t(cbind(c("1" = -1, "2" = 1))))
contrasts(data_joint$experiment) <- MASS::ginv(contrasts_experiment)
contrasts(data_joint$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_joint$part) <- MASS::ginv(contrasts_part)

# New formula for LMMs
form_joint <- buildmer::tabulate.formula(
  ~ part * condition * experiment + (part * condition | participant) + (part * condition * experiment | item)
) %>% mutate(block = replace(block, is.na(grouping), "fixed"))

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/joint-stats.RDS")) {
  # Read models from file
  models_joint <- readRDS("analysis/export/joint-stats.RDS")
} else {
  # Compute LMMs for Experiments 1 and 2 combined
  models_joint <- map(
    comps$name,
    compute_models,
    formula = form_joint,
    data = data_joint,
    control = ctrl_params,
    specs = specs_exp12
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/joint-stats.RDS")
}
```

```{r joint-table, include=TRUE, results="asis"}
# Create table of models for Experiments 1 and 2 combined
table_joint <- create_table(
  models_joint,
  stub_anova = c("Part", "Condition", "Experiment", "Pt. × con.", "Pt. × exp.", "Ins. × exp.", "Pt. × con. × exp."),
  stub_contrasts = c("Part I", "Part II", "Part III"),
  caption = "Results of linear mixed-effects regression models for Experiments 1 and 2 combined",
  note = "Pt. = part, con. = condition, exp. = experiment, est. = estimate, CI = confidence interval."
)
```

```{r joint-plot, eval=FALSE}
map(c("I", "II", "III"), function(part){
  map(c("P1", "N1", "N400"), function(comp){
    
    ymin <- case_when(comp == "P1" ~ -6, comp == "N1" ~ -11, comp == "N400" ~ -10)
    
    data_joint %>%
      filter(part == !!part) %>%
      rename(amplitude = !!comp) %>%
      group_by(participant, part, condition) %>% summarise(across(amplitude, mean)) %>%
      ggplot(aes(x = condition, y = amplitude, fill = condition)) +
      geom_violin() +
      geom_boxplot(width = 0.3) +
      coord_cartesian(ylim = c(ymin, ymin + 24)) +
      scale_y_continuous(breaks = seq(ymin, ymin + 24, 4)) +
      theme_classic() +
      theme(
        legend.position = "none"
      )
    
  }) %>% plot_grid(plotlist = ., nrow = 3)
}) %>% plot_grid(plotlist = ., nrow = 1)

# data_joint %>%
#   pivot_longer(cols = c(P1, N1, N400), names_to = "comp", values_to = "amplitude") %>%
#   mutate(comp = factor(comp, levels = c("P1", "N1", "N400"))) %>%
#   group_by(participant, part, condition, comp) %>% summarise(amplitude = mean(amplitude)) %>%
#   ggplot(aes(x = condition, y = amplitude, fill = condition)) +
#   geom_violin() +
#   geom_boxplot(width = 0.3) +
#   coord_cartesian(ylim = c(-10, 10)) +
#   facet_grid(comp ~ part)
```

As shown in Table \@ref(tab:joint-table), the main effect of the part of the experiment was significant in the P1, N1, and N400 component, all *F*s \> `r table_joint$anov["Part", "P1_f"]`, all *p*s `r table_joint$anov["Part", "P1_p"]`, as was the main effect of condition in the N400, *F*`r table_joint$anov["Informed", "N400_df"]` = `r table_joint$anov["Informed", "N400_f"]`, *p* `r table_joint$anov["Informed", "N400_p"]`. Furthermore, the part × condition interaction was now observed reliably in all three components, all *F*s \> `r table_joint$anov["Pt. × con.", "P1_f"]`, all *p*s \< `r table_joint$anov["Pt. × con.", "P1_p"]`. While there was a main effect of experiment in the N400, *F*`r table_joint$anov["Experiment", "N400_df"]` = `r table_joint$anov["Experiment", "N400_f"]`, *p* = `r table_joint$anov["Experiment", "N400_p"]`, the absence of any significant interactions of experiment with part or condition indicated that the effects of our experimental manipulations did not differ between Experiments 1 and 2.

Based on the part × condition interaction, we again computed follow-up comparisons between the semantically inforemd and the naive condition within each part, now collapsed across the data from both experiments. This confirmed the absence of any reliable differences between the two conditions in Part I, all *p*s \> `r table_joint$conts["Part I", "N400_p"]`, the significant enhancement of the N1 component in Part II, while the semantic information was obtained, *b* = `r table_joint$conts["Part II", "N1_est"]` µV, *p* = `r table_joint$conts["Part II", "N1_p"]`, the significant reduction of the N400 component in Part II, while the semantic information was obtained, *b* = `r table_joint$conts["Part II", "N400_est"]` µV, *p* `r table_joint$conts["Part II", "N400_p"]`, and Part III, after the information had been obtained, *b* = `r table_joint$conts["Part III", "N400_est"]` µV, *p* = `r table_joint$conts["Part III", "N400_p"]`, as well as the significant enhancement of the P1 component in Part III, after the information had been obtained, *b* = `r table_joint$conts["Part III", "P1_est"]` µV, *p* = `r table_joint$conts["Part III", "P1_p"]`.

### Control Analysis

```{r control-analysis}
# Calculate average number of objects per condition in Experiments 1 and 2
stims_joint <- (stims_exp1 + stims_exp2) / 2

# Re-load saved ERPs from Experiments 1 and 2
data_joint_control <- bind_rows(
  readRDS("analysis/export/exp1-erps.RDS"),
  readRDS("analysis/export/exp2-erps.RDS") %>% mutate(participant = fct_relabel(participant, ~ paste0(., "_2"))),
  .id = "experiment"
) %>% mutate(experiment = as_factor(experiment))

# # Keep all three conditions
# data_joint_control %<>% filter(condition %in% c("Informed", "Naive", "Excl_informed")) %>% droplevels()
# contrasts_condition <- t(cbind(c("Informed" = 1, "Naive" = -1, "Excl_informed" = 0),
#                                c("Informed" = 1, "Naive" = 0, "Excl_informed" = -1)))
# contrasts(data_joint_control$condition) <- MASS::ginv(contrasts_condition)
# model_joint_control_N1 <- buildmer::buildmer(
#   N1 ~ part * condition * experiment + (part * condition | participant) + (part * condition * experiment | item),
#   REML = FALSE,
#   control = ctrl_params,
#   buildmerControl = buildmer::buildmerControl(
#     data = data_joint_control,
#     direction = c("order", "backward"),
#     elim = function(logp) exp(logp) >= .20,
#     calc.anova = TRUE,
#     ddf = "Satterthwaite"
#   )
# )
# emmeans::emm_options(lmer.df = "Satterthwaite", lmerTest.limit = Inf)
# emmeans::emmeans(model_joint_control_N1@model, pairwise ~ condition | part, infer = TRUE)$contrasts

# This time, we keep the unsuccessful ahas as the control condition
data_joint_control %<>% filter(condition %in% c("Informed", "Excl_informed")) %>% droplevels()

# Contrast coding for condition (informed-no_informed) and part (2-1, 3-1)
contrasts(data_joint_control$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_joint_control$part) <- MASS::ginv(contrasts_part)

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/joint-control-stats.RDS")) {
  # Read models from file
  models_joint_control <- readRDS("analysis/export/joint-control-stats.RDS")
} else {
  # Re-compute LMMs for Experiments 1 and 2 combined
  models_joint_control <- map(
    comps$name,
    compute_models,
    formula = form_joint,
    data = data_joint_control,
    control = ctrl_params,
    specs = specs_exp12
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/joint-control-stats.RDS")
}

# Create table but don't print
table_joint_control <- create_table(
  models = models_joint_control,
  stub_anova = c("Part", "Condition", "Experiment", "Pt. × con.", "Pt. × exp.", "Ins. × exp.", "Pt. × con. × exp."),
  stub_contrasts = c("Part I", "Part II", "Part III"),
  caption = "Results of linear mixed-effects regression models with a different baseline",
  note = "Pt. = part, con. = condition, exp. = experiment, est. = estimate, CI = confidence interval."
)
```

One may raise concerns whether the modulation of the N1 component in Part II genuinely reflects the semantically informed perception of the objects in the respective condition, or---as an alternative explanation---whether it could be driven by the objects in the other, semantically naive condition. Remember that these objects were preceded by non-matching keywords which were picked so that they could not be related to the object's visual features and their configuration. Thus, the modulation of the ERP components in Part II may reflect a mismatch response to those objects---reflecting, for example, the fact that the visual features of the object shown in Figure \@ref(fig:exp1-plot)A cannot be reconciled with the function of morsing messages. To preclude this alternative explanation, we repeated the analysis above using a different baseline against which the objects in the semantically informed condition were contrasted. Instead of the naive condition (where objects were presented with non-matching descriptions), we now used those objects which were preceded by matching descriptions (as in the informed condition), but which were excluded from the main analysis because participants indicated behaviorally that they did not understand the object they were seeing. Across both experiments, this was the case for `r scales::percent(stims_joint["Excl_informed"]/60, accuracy = 0.1)` of objects presented with matching descriptions as compared to `r scales::percent(stims_joint["Informed"]/60, accuracy = 0.1)` of objects which did indeed lead to semantically informed perception.[^1] Just as above, this control analysis revealed a robust N1 effect in Part II, *b* = `r table_joint_control$conts["Part II", "N1_est"]` µV, *p* `r table_joint_control$conts["Part II", "N1_p"]`. Thus, this enhanced negativity seems to be a genuine marker of semantically informed perception, no matter if compared to objects presented with misleading keywords or compared to objects presented with accurate keywords on which participants failed to capitalize. Note that the reduction of the N400 component in Part II also remained robust in this control analysis, *b* = `r table_joint_control$conts["Part II", "N400_est"]` µV, *p* `r table_joint_control$conts["Part II", "N400_p"]`.

[^1]: Note that the remaining objects to make it up to 100% were excluded as not being unfamiliar in Part I of the experiment (see Statistical Analysis)

## Discussion

Experiment 2, which was a direct replication of Experiment 1, confirmed the effects of obtaining semantic information about previously unfamiliar objects on ERPs associated with lower-level visual perception (P1), higher-level visual perception (N1), and semantic processing (N400). While the enhancement of the N1 component, being more negative for semantically informed as compared to naive perception, was present only during the critical presentation of the objects with their respective keywords (in Part II), the enhancement of the P1, being more positive in response to these same objects, emerged only after the information had been obtained and the objects were presented again (in Part III). This indicates a modulation of different stages of visual object perception through semantic knowledge, while and after an understanding of the object has been obtained. Finally, a sustained reduction of the N400 component in response to objects for which participants experienced semantically informed perception may reflect lowered semantic processing demands compared to unfamiliar objects which participants did not yet understand.

```{r exp3-preparation, eval=run_exp3}
# Read behavioral data for Experiment 3
data_exp3 <- list.files("data/exp3/RT", pattern = ".txt", full.names = TRUE) %>% map(read_behav)

# Check average number of stimuli per condition
(stims_exp3 <- map(data_exp3, function(x) {
  x %<>% filter(part != "IV")
  table(x$condition) / 3
}) %>% bind_rows() %>% colMeans())
```

```{python exp3-preprocessing, eval=run_exp3}
# List raw EEG filenames
fnames_exp3 = sorted(glob.glob("data/exp3/EEG/*.vhdr"))

# Import behavioral data from R
metadata_exp3 = r.data_exp3

# Update stimulus triggers
preproc_params["event_id"] = {
    "match/upright": 241,
    "match/inverted": 242,
    "mismatch/upright": 243,
    "mismatch/inverted": 244,
}

# Check if preprocessing was done already (delete the file to re-run)
if os.path.exists("analysis/export/exp3-epo.fif"):
    # Read preprocessed data from file
    epochs_exp3 = mne.read_epochs("analysis/export/exp3-epo.fif", preload=True)
else:
    # Preprocess EEG data for Experiment 3
    epochs_exp3 = [
        preproc(vhdr_fname=fname, metadata=meta, **preproc_params)
        for fname, meta in zip(fnames_exp3, metadata_exp3)
    ]
    # Combine epochs into a single data set
    epochs_exp3 = mne.concatenate_epochs(epochs_exp3)
    # Backup epochs and evokeds to the export folder
    epochs_exp3.save("analysis/export/exp3-epo.fif")

# # Get indices of bad epochs despite ICA
# rej_exp3 = epochs_exp3.drop_log
# rej_exp3[:] = [item for item in rej_exp3 if item != ['IGNORED']]
# rej_exp3 = [i for i in range(len(rej_exp3)) if rej_exp3[i] != []]
```

```{r exp3-analysis, eval=run_exp3}
# Re-import behavioral data and re-factorize some columns
data_exp3 <- py$epochs_exp3$metadata %>% mutate(
  part = factor(part, levels = c("I", "II", "III", "IV")),
  position = factor(position, levels = c("Inverted", "Upright")),
  condition = factor(condition, levels = c("Informed", "Naive", "Excl_informed", "Excl_naive", "Excl_known")),
  participant = factor(participant), item = factor(item)
)

# Check if ERPs were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp3-erps.RDS")) {
  # Read ERPs from file
  data_exp3 <- readRDS("analysis/export/exp3-erps.RDS")
} else {
  # Compute single-trial ERPs for Experiment 3
  data_exp3 <- pmap_dfc(comps, compute_erps, epochs = py$epochs_exp3$get_data(), els = els) %>%
    set_names(comps$name) %>%
    cbind(data_exp3, .) %T>%
    saveRDS("analysis/export/exp3-erps.RDS")
}

# Remove the epochs to free up memory
py_run_string("del epochs_exp3")
import("gc")$collect()

# Remove any trials from part IV
data_exp3 %<>% filter(part != "IV") %>% droplevels()

# Check number of rejected epochs
rejected_exp3 <- data_exp3 %>%
  group_by(participant) %>%
  tally() %>%
  mutate(n = 672 - n) %>%
  pull(n)
mean(rejected_exp3)
median(rejected_exp3)
range(rejected_exp3)

# Remove any trials excluded from conditions
data_exp3 %<>% filter(condition %in% c("Informed", "Naive")) %>% droplevels()

# Contrast coding for position (inverted-updright), condition (informed-naive), and part (2-1, 3-1)
t(contrasts_position <- t(cbind(c("Inverted" = 1, "Upright" = -1))))
contrasts(data_exp3$position) <- MASS::ginv(contrasts_position)
contrasts(data_exp3$condition) <- MASS::ginv(contrasts_condition)
contrasts(data_exp3$part) <- MASS::ginv(contrasts_part)

# New formula for LMMs
form_exp3 <- buildmer::tabulate.formula(
  ~ part * condition * position + (part * condition * position | participant) + (part * condition * position | item)
) %>% mutate(block = replace(block, is.na(grouping), "fixed"))

# New follow-up contrasts
specs_exp3 <- pairwise ~ condition | position * part

# Check if models were computed already (delete the file to re-run)
if (file.exists("analysis/export/exp3-stats.RDS")) {
  # Read models from file
  models_exp3 <- readRDS("analysis/export/exp3-stats.RDS")
} else {
  # Compute LMMs for Experiment 3
  models_exp3 <- map(
    comps$name,
    compute_models,
    formula = form_exp3,
    data = data_exp3,
    control = ctrl_params,
    specs = specs_exp3
  ) %>% set_names(comps$name) %T>%
    saveRDS("analysis/export/exp3-stats.RDS")
}
```

```{r exp3-table, include=TRUE, results="asis", eval=run_exp3}
# Create table of models for Experiment 3
table_exp3 <- create_table(
  models_exp3,
  stub_anova = c("Part", "Condition", "Position", "Pt. × con.", "Pt. × pos.", "Ins. × pos.", "Pt. × con. × pos."),
  stub_contrasts = c(
    "Part I, inverted", "Part I, upright", "Part II, inverted",
    "Part II, upright", "Part III, inverted", "Part III, upright"
  ),
  caption = "Results of linear mixed-effects regression models for Experiment 3",
  note = "Pt. = part, con. = condition, pos. = position, est. = estimate, CI = confidence interval."
)
```

# General Discussion

General discussion section goes here.

Although most of the previous studies in the object processing domain have focused on the P1 component as an index of semantic knowledge modulating perceptual processes, at least two studies provide evidence for top-down effects of meaningfulness on the visual N1 (or N170) component in response to schematic drawings of faces [@bentin2002] and objects [@beaucousin2011]. Using a similar design as in the present study, drawings of distored faces elicited a facelike (i.e. enlarged) N170 component only after participants were primed with a vaild hint---in this case the same face with all of its visual features in place [@bentin2002]. Similarly, N1 amplitudes in a global-local detection task where shown to be enhanced in response to line drawings of meaningful objects (a boat or a butterfly) as compared to visually matched by meaningless drawings of non-objects. Interestingly, this effect was reversed in polarity when participants had to focus on the smaller, constituent parts and ignoring the object which emerged out of their global arrangement, which may be interpreted as further evidence for a functional role of the N1 component in the holistic perception of an object and its early visual categorization, integrating across its visual features.

\newpage

# References

\setlength{\parindent}{-0.5in}

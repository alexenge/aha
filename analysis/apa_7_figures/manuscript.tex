% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%

% Store the code of efloat re-definitions into \efloatredefinitions
% (This code must be defined before the efloat package is loaded.)
\makeatletter
\newcommand\efloatredefinitions{}
\newcommand\efloat@AtBeginDocument{\g@addto@macro\efloatredefinitions}
\makeatother

\documentclass[
  english,
  man,12pt,twoside]{apa7}

\usepackage[capposition=top]{floatrow}

% Do the efloat re-definitions after the re-definitions of floatrow were done
%\show\efloatredefinitions
\AtBeginDocument{\efloatredefinitions} % or simply \efloatredefinitions after \begin{document}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdfauthor={Alexander Enge1},
  pdflang={en-EN},
  pdfkeywords={objects, semantic knowledge, visual perception, event-related potentials},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{}
\keywords{objects, semantic knowledge, visual perception, event-related potentials\newline\indent Word count: }
\usepackage{csquotes}
\raggedbottom
\linespread{1.5}
\fancyhead{}
\fancyhead[RO,LE]{\thepage}
\fancyhead[LO,RE]{Semantic Knowledge and Unfamiliar Objects}
\usepackage{setspace}
\AtBeginEnvironment{tabular}{\doublespacing}
\captionsetup[figure]{font={stretch=1,scriptsize}}
\usepackage[all]{nowidow}
\usepackage[bottom]{footmisc}
\interfootnotelinepenalty10000
\usepackage[export]{adjustbox}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[main=english]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Event-Related Potentials of the Semantically\\
Informed Perception of Unfamiliar Objects}
\author{Alexander Enge}
\date{}


\authornote{

\addORCIDlink{Alexander Enge}{0000-0003-0100-2297}, student no. 581331.

The preprocessed data and code for this study are openly available at \url{https://osf.io/uksbc/}.

We have no conflict of interest to disclose.

Correspondence concerning this article should be addressed to Alexander Enge, Rudower Chaussee 18, 12489 Berlin. E-mail: \href{mailto:alexander.enge@hu-berlin.de}{\nolinkurl{alexander.enge@hu-berlin.de}}

}

\affiliation{\vspace{0.5cm}Humboldt-Universit√§t zu Berlin}

\abstract{
Does our perception of an object change as soon as we discover what it is for? This question is relevant not only for our everyday lives, where we may encounter novel tools and gadgets as parts of our dynamic working and private environments; it pertains to the long-standing debate around the (im)penetrability of perception by ``higher'' cognitive capacities. In this experiment, we showed participants (\emph{n} = 24) pictures of 120 unfamiliar objects either together with valid information about their function---leading to semantically informed perception---or together with invalid information---resulting in naive perception. We measured event-related potentials (ERPs) to investigate at which stages in the visual processing hierarchy these two types of perceiving objects differed from one another. We found that semantically informed as compared to naive perception was associated with larger amplitudes in the N170 component and reduced amplitudes in the N400 component. When the same objects were presented once more (without any information), the N400 effect persisted and we now also observed enlarged amplitudes in the P1 component in response to objects for which semantically informed perception had taken place. We replicated these novel findings in an independent sample (\emph{n} = 24). Consistent with previous work, they suggest that obtaining semantic information about previously unfamiliar objects alters aspects of their lower-level visual perception (P1 component), higher-level visual perception (e.g., holistic perception; N170 component), and semantic processing (N400 component).
}



\begin{document}
\maketitle

\setcounter{page}{1}

How does our perception of an object change as soon as we discover what it is for? In this study, we presented participants with unfamiliar objects which were preceded either by valid information about their function, leading to semantically informed perception, or by misleading information, leading to naive perception. Semantically informed perception of objects was expected to differ from naive perception at certain stages in the processing hierarchy and we aimed to identify these stages by capitalizing on the high temopral resoulation of event-related brain potentials (ERPs).

The claim that higher-level cognitive capacities such as knowledge or language can modulate lower-level perceptual processes has been the source of a long-running debate. This debate revolves around the question of whether or not our perception of the (visual) world should be thought of as an encapsulated module whose processing is determined solely by the visual input reaching our retina. According to this view, higher-level mental states and functions are not able to penetrate perception and kick in only after lower-level perceptual analysis of the visual input has taken pace (e.g., Firestone \& Scholl, 2016; Fodor, 1984, 1983; Machery, 2015; Pylyshyn, 1999). Visual perception itself is treated as an encapsulated module that processes the retinal input in a feed-forward fashion, progressing from lower areas with smaller receptive field sizes to areas representing increasingly complex shapes and, eventually, whole objects (Goodale \& Milner, 1992; Marr, 1982).

Conversely, this position is challenged by the view that perceptual processing dynamically interacts with other aspects of cognition from early on (e.g., P. M. Churchland, 1988; P. S. Churchland et al., 1994; Lupyan, 2015; Teufel \& Nanay, 2017). The myriad feedback connections from areas higher up the visual hierarchy (e.g., medial- and inferotemporal areas MT and IT) to early visual areas (e.g., visual areas V1 and V2; Bullier, 2001; Gilbert \& Li, 2013), as well as behavioral evidence, have inspired theories of visual perception that emphasize the top-down influence of cognitive processes. The reverse hierarchy theory (Ahissar \& Hochstein, 2004; Hochstein \& Ahissar, 2002), for instance, assumes that conscious visual perception occurs initially at the level of whole objects or object categories. Only after this high-level interpretation of the stimulus has been obtained, its more for fine grained visual details---if relevant for the current task---are being accessed from lower-level areas via top-down connections. Along similar lines, the label feedback hypothesis (Lupyan, 2012) posits that the activation of an object's name (i.e., a high level property) transiently warps perceptual space so that its diagnostic visual features are being processed preferentially. Finally, an active role of top-down influences is also promoted by theories of vision falling under the umbrella of Bayesian inference and predictive coding (e.g., Lupyan, 2015; Panichello et al., 2013; Yuille \& Kersten, 2006).

Empirical findings from experimental psychology have recently added additional support to the view that perception can indeed be penetrated by other aspects of cognition (for review, see Collins \& Olson, 2014; Vetter \& Newen, 2014). These aspects of cognition may include transient states such as emotions (Bocanegra \& Zeelenberg, 2009; e.g., Phelps et al., 2016) or intentions (e.g., Balcetis \& Dunning, 2010; Cole et al., 2012) as well as learned capacities such as language (e.g., Boutonnet \& Lupyan, 2015; Maier \& Abdel Rahman, 2018; Mo et al., 2011) or declarative knowledge about faces (e.g., Abdel Rahman, 2011; Eiserbeck \& Abdel Rahman, 2020; Suess et al., 2015) and objects (e.g., Abdel Rahman \& Sommer, 2008; Lin \& Murphy, 1997; Weller et al., 2019). Despite this wealth of theory and empirical evidence, critics of the idea that cognitive functions can penetrate perception have remained skeptical, pointing out important methodological shortcomings in large parts of this literature (Firestone \& Scholl, 2016; Machery, 2015).

One productive way to investigate top-down effects of cognition on perception while addressing some of these concerns is to use neurophysiological recording methods such as ERPs (Luck, 2014): Their high temporal resolution allows for a direct test of which processing stages---perceptual and/or postperceptual---are influenced by higher-level capacities such as semantic knowledge. This is made possible by the fact that when participants are presented with visual stimuli, their averaged brain responses show characteristic deflections (ERP components), each of them with its typical polarity (positive or negative), peak latency, spatial scalp distribution, and functional role(s).

The visual P1 component of the ERP refers to a positive deflection peaking as early as 100--150 ms after stimulus onset at occipital channels. It is generated by multiple sources in the extrastriatal cortices of the middle occipital and fusiform gyri (Di Russo et al., 2001; Foxe \& Simpson, 2002; Mangun, 1995). The P1 is thought to reflect the processing of low-level visual characteristics of the stimulus (e.g., size, luminance, and contrast; Di Russo et al., 2001; Johannes et al., 1995; Luck, 2014), which is why it being modulated by higher-level cognitive capacities would challenge a purely bottom-up, encapsulated conception of visual perception. For instance, it is enhanced when participants pay attention to the spatial location (Luck et al., 2000; Mangun, 1995; Mangun \& Hillyard, 1991) or certain non-spatial features (Taylor, 2002) of the stimulus.

The visual N170 (or N1) component refers to the negative deflection following the P1 and is maximal around 150--200 ms after stimulus onset at occipito-temporal channels. Just as the P1, it is influenced by visual parameters of the stimulus (Johannes et al., 1995) and by selective attention, especially when participants are required to discriminate between different visual stimuli (Mangun \& Hillyard, 1991; Vogel \& Luck, 2000). Importantly, the N170 is enlarged (i.e.,~more negative) in response to faces (Bentin et al., 1996; Bruno Rossion \& Jacques, 2011) and other visual stimuli for which one happens to be an expert (e.g., birds, dogs, or cars; Gauthier, Curran, et al., 2003; Tanaka \& Curran, 2001). This category selectivity seems to reflect both the processing of individual visual features of the respective objects (including faces) as well as the holistic configuration of these features (Eimer et al., 2011; Jacques \& Rossion, 2010; B. Rossion et al., 1999; Sagiv \& Bentin, 2001). Thus, a modulation of the N170 by cognitive capacities such as knowledge would suggest these factors having an influence on higher-level, holistic perception of the visual stimulus.

Finally, the N400 component refers to more negative ERP amplitudes when stimuli require more as compared to less resources for semantic processing or integration (Kutas \& Federmeier, 2011; Lau et al., 2008; Rabovsky et al., 2018). It begins approximately 300 ms after stimulus onset and is most pronounced at centro-parietal channels. A modulation of the N400 component by cognitive capacities such as knowledge can be taken as evidence for a post-perceptual influence on stimulus processing.

Previous studies have focused on these and other ERP components to investigate which processing stages are indeed influenced by knowledge or expertise in the domain of visual objects (Abdel Rahman \& Sommer, 2008; Gratton et al., 2009; Maier et al., 2014; Maier \& Rahman, 2019; Bruno Rossion et al., 2004; B. Rossion et al., 2002; Samaha et al., 2018; Tanaka \& Curran, 2001; Weller et al., 2019). Together, they suggest that not only higher-level ERPs like the semantic N400 component are influenced by acquiring information about objects, but also earlier components that typically reflect bottom-up visual processing. This was the case, for instance, when participants were presented with a range of unfamiliar objects, receiving in-depth verbal descriptions about their function for half of the objects and irrelevant verbal information (i.e., cooking recipes) for the other half of the objects (Abdel Rahman \& Sommer, 2008, Experiment 1). After this learning phase, the same objects were presented in three different ERP tasks which did not require explicit access to any of the learned semantic information. In all three tasks, ERP amplitudes in response to objects for which in-depth knowledge had been acquired differed from those in response to objects for which this has not been the case. Crucially, these differences occurred not just in the N400 component, indexing a modulation of post-perceptual semantic processing, but also in the P1 component, suggesting a top-down modulation of lower-level perceptual processing. Interestingly, the ERPs in the in-depth knowledge condition were qualitatively similar to those for untrained but well-known objects. The modulation of the P1 component has recently been replicated when the same objects were presented under circumstances of limited attentional resources in an attentional blink paradigm (Weller et al., 2019). Here, the (neurophysiological) differences in P1 amplitudes between objects with in-depth versus minimal knowledge were correlated with (behavioral) differences in their detection rate during the attentional blink. This can be taken as further evidence of an influence of semantic knowledge on the early visual perception of unfamiliar objects.

A complementary approach has been taken in a recent study investigating the ERPs in response to familiar (rather than unfamiliar) objects which were rendered difficult to recognize by converting them to two-tone, ``Mooney'' images (Samaha et al., 2018, Experiment 4). In this experiment, participants were trained with meaningful verbal cues, telling them what kind of objects they should look for in the images, or with a non-meaningful perceptual task, familiarizing them with the images but not with their semantic content. When the EEG was measured in a delayed matching task later on, the two types of training led to differential effects, again suggesting a modulation of lower-level visual perception: Over left posterior electrodes, P1 amplitudes were larger and alpha power was higher for meaning-trained images of objects than for perceptually-trained images of objects. As before, these effects of perceiving objects in a semantically informed way seems to be behaviorally relevant, as participants tended respond faster and more accurately to meaning-trained as compared to perceptually-trained images in the matching task. This alternative approach further corroborates an early influence of semantic information on the visual perception of objects.

All the previously mentioned studies have in common that they investigated the effects of semantic knowledge on the visual perception of objects only \emph{after} an extensive learning phase had taken place (Abdel Rahman \& Sommer, 2008; Gauthier, James, et al., 2003; Maier et al., 2014; Maier \& Rahman, 2019; Bruno Rossion et al., 2004; B. Rossion et al., 2002; Samaha et al., 2018; Weller et al., 2019). Participants performed one or multiple training sessions during which they encountered each object together with its respective label or description multiple times. The EEG was usually not measured---or at least not analyzed and reported---during these training sessions, which sometimes took place on a separate day (Abdel Rahman \& Sommer, 2008; Maier et al., 2014; Maier \& Rahman, 2019) or were spread out across multiple days or weeks (Bruno Rossion et al., 2004; B. Rossion et al., 2002). While designs with such substantive training maximize the chances of detecting even subtle top-down effects of the knowledge acquired, they necessarily leave at least three conceptual questions unresolved.

The first question is if we can detect any electrophysiological correlates of semantic insight, that is,~the critical presentation during which an understanding of the previously unfamiliar object is happening (instead of asking if this understanding leads to differential effects in an orthogonal task later on). The second question is how much learning is actually necessary before reliable top-down effects of knowledge on ERP amplitudes can be obtained: Does it actually take dozens of repetitions per object or may a single exposure to the object together with the relevant semantic information be enough? The third and closely related question is about the nature of knowledge effects on perception: Are they reflecting genuine top-down effects of the semantic system altering perception online (while the object is being perceived), or do they merely reflect the (re-)activation of stored representations of the objects which have been altered over the course of the learning process? By measuring the ERPs before, during, and after participants received semantic hints about unfamiliar objects, we hope to provide tentative answers to all three of these questions.

In the two experiments subsequently reported, participants were presented with real-word objects that were presumed to be unfamiliar to most of them. They first viewed each of these objects without any semantic information. This first part served as a naive baseline to rule out that ERPs would differ in response to the objects based solely on low-level visual differences. Next, participants viewed each unfamiliar object for a second time, now preceded by verbal keywords. These keywords could either correctly describe the typcial function of the object, thus making it possible for participants to understand what kind of object they were viewing, or they could mislead participants by describing the function of a different object, thus keeping the perception of the object semantically naive. During this second part, we were able measure the online influence of semantic insight on object-evoked ERPs as it happened. Finally, the objects were presented for a third time (without keywords, as in the first part) to investigate downstream effects of having acquired semantic knowledge about them---thereby mimicking previous studies (e.g., Abdel Rahman \& Sommer, 2008; Samaha et al., 2018; Weller et al., 2019). In all three parts, we examined the influence of semantic information on ERPs associated with lower-level visual perception (P1 component, 100--150 ms), higher-level visual perception (N170 component, 150--200 ms), and semantic processing (N400 component, 400--700 ms).

\hypertarget{experiment-1}{%
\section{Experiment 1}\label{experiment-1}}

\hypertarget{method}{%
\subsection{Method}\label{method}}

\hypertarget{participants}{%
\subsubsection{Participants}\label{participants}}

Participants for Experiment 1 were 24 German native speakers (13 female, 11 male) with a mean age of 24 years (range 18 to 31) and no history of psychological disorder or treatment. No a priori power analysis was carried out and the sample size was chosen in line with other EEG studies in our lab at the time of data collection. All participants were right-handed according to the Edinburgh inventory (Oldfield, 1971) and reported normal or corrected-to-normal vision. They gave written informed consent before starting the experiment and received a compensation of ‚Ç¨8 per hour for participating.

\hypertarget{materials}{%
\subsubsection{Materials}\label{materials}}

Stimuli for Experiments 1 and 2 consisted of 240 grayscale photographs of real-world objects, 120 of which were well-known everyday objects (e.g., a bicycle, a toothbrush), serving as filler stimuli of no interest, whereas the other 120 were rare objects presumed to be unfamiliar to the majority of participants (e.g., a galvanometer, an udu drum). A list of these unfamiliar objects can be found in the appendix. All stimuli were presented on a light blue background with a size of 207 √ó 207 pixels on a 19-inch LCD monitor with a resolution of 1,280 √ó 1,024 pixels and a refresh rate of 75 Hz. At a standardized viewing distance of 90 cm, the images of the objects subtended approximately 3.9 degrees of participants' horizontal and vertical visual angle.

For each unfamiliar object, a pair of keywords---a noun and a verb---was selected, describing the object's typical function or use in a way that could typically be related to its visual features and their configuration (e.g., current--measuring, pottery--drumming). As our central experimental manipulation, the presentation of each unfamiliar object was preceded by its correctly matching keywords for half of the objects, whereas the other half were preceded by non-matching keywords belonging to one of the other objects. The matching keywords were expected to induce semantically informed perception (i.e.,~participants suddenly understanding what kind of object they were viewing), whereas the non-matching keywords were expected hamper such an understanding and keep the perception of the object semantically naive. All participants saw each unfamiliar object with only one type of keywords (matching or non-matching). This assignment of keywords and objects was counterbalanced across participants so that each object would be presented with matching keywords (leading to semantically informed perception) and non-matching keywords (leading to naive perception) to an equal number of participants. The experiment was programmed and displayed using Presentation¬Æ software (Neurobehavioral Systems, Inc., Berkeley, CA, www.neurobs.com).

\hypertarget{procedure}{%
\subsubsection{Procedure}\label{procedure}}

Each experimental session consisted of three parts (see Figure \ref{fig:exp1-plot}A). In the \emph{pre-insight} part, after written informed consent had been obtained and the EEG had been prepared, all 240 familiar and unfamiliar objects were presented once in random order and without any keywords. Each trial consisted of a fixation cross presented in the middle of the screen for 0.5 s, followed by the presentation of the object until participants made a response or until a time out after 3 s. The inter-trial interval until the presentation of the next fixation cross was 0.5 s and participants took a self-timed break after each block of 60 objects. The task, which was kept the same throughout all parts and experiments, was to classify each object using one of four response alternatives: (a) ``I know what this is or have a strong assumption,'' (b) ``I have an assumption what this is,'' (c) ``I have rather no assumption what this is,'' or (d) ``I don't know what this is and have no assumption.'' Participants were asked to respond as quickly and as accurately as possible by pressing one out of four buttons with the index or middle finger of their left or right hand, respectively. The mapping of the rating scale to the four buttons (left to right or right to left) was counterbalanced across participants.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/exp1-plot-1} 

}

\caption{Procedure and Results of Experiment 1}\label{fig:exp1-plot}
\floatfoot{\scriptsize\emph{Note.} (A) In the pre-insight part, participants were presented with 120 unfamiliar objects and indicated whether they knew what kind of object they were viewing. In the insight part, half of these objects were presented with matching keywords (in purple color for illustration), leading to semantically informed perception, and the other half with non-matching keywords (in petrol color for illustration), leading to naive perception. In the post-insight part, the same objects were presented again without the keywords. (B) ERP waveforms and scalp topographies are shown for objects with semantically informed versus naive perception within the three different parts. Semantically informed perception was associated with significantly more negative amplitudes in the N170 component in in the insight part, significantly less negative amplitudes in the N400 component in the insight and post-insight parts, and significantly more positive amplitudes in the P1 component in the post-insight part. Ampl. = amplitude.\newline*\emph{p} \textless{} .05. **\emph{p} \textless{} .01. ***\emph{p} \textless{} .001.}
\end{figure}

In the \emph{insight} part, the 120 unfamiliar objects were presented for a second time, now preceded either by matching keywords (leading to semantically informed perception) or by non-matching keywords (leading to naive perception). Each trial consisted of a fixation cross presented for 0.5 s, followed by the presentation of the keywords for 2.5 s. Then, an asterisk was presented in the middle of the screen for another 0.5 s, followed by the presentation of the object until a response was made or until a time out after 3 s. The objects were presented in blocks of 30 trials so that within each block (a) there were 15 objects from each of the two experimental conditions and (b) objects were heterogeneous in terms of their shape, visual complexity, and functional category (e.g., medical devices, musical instruments).

Finally, in the \emph{post-insight} part, the unfamiliar objects were presented for a third time with an identical trial structure as in the pre-insight part, that is, without any keywords. Note that the insight and post-insight parts were presented in an interleaved fashion so that after the presentation of one block of 30 objects in the insight part (with keywords), participants took a self-timed break and continued with the same block of 30 objects in the post-insight part (without keywords) before moving on to the next block consisting of 30 different objects. They continued like this until all four blocks were completed in both parts. In total, the experiment consisted of 480 trials (120 familiar objects in the pre-insight part and 120 unfamiliar objects in the pre-insight, insight, and post-insight parts). It took participants approximately 35 minutes to complete.

\hypertarget{eeg-recording-and-preprocessing}{%
\subsubsection{EEG Recording and Preprocessing}\label{eeg-recording-and-preprocessing}}

The continuous EEG was recorded from 62 Ag/AgCl scalp electrodes placed according to the extended 10--20 system (American Electroencephalographic Society, 1991) and referenced online to an external electrode placed on the left mastoid (M1). Two additional external electrodes were placed on the right mastoid (M2) and below the left eye (IO1), respectively. During the recording, electrode impedance was kept below 5 kŒ©. An online band-pass filter with a high-pass time-constant of 10 s (0.016 Hz) and a low-pass cutoff frequency of 1,000 Hz was applied before digitizing the signal at a sampling rate of 500 Hz.

Offline, the data were preprocessed using the MNE software (Version 0.21.0; Gramfort et al., 2013) in Python (Version 3.8.5; van Rossum \& Drake, 2009). First, all scalp electrodes were re-referenced to the common average. Next, artifacts resulting from blinks and eye movements were removed using independent component analysis (ICA). The first 15 components were extracted by the FastICA algorithm (Hyv√§rinen, 1999) after temporarily low-pass filtering the data at 1 Hz. Any components showing substantive correlations with either of two virtual EOG channels (VEOG: IO1 minus Fp1, HEOG: F9 minus F10) were removed automatically using the \emph{find\_bads\_eog} function. After artifact correction, a zero-phase, non-causal FIR filter with a lower pass-band edge at 0.1 Hz (transition bandwidth: 0.1 Hz) and an upper pass-band edge at 30 Hz (transition bandwidth: 7.5 Hz) was applied. Next, the continuous EEG was epoched into segments of 2,000 ms, starting 500 ms before the onset of the visual presentation of each unfamiliar object. The epochs were baseline-corrected by subtracting the average voltage during the 200 ms before stimulus onset. Epochs containing artifacts despite ICA, defined as peak-to-peak amplitudes exceeding 200 ¬µV, were removed from further analysis. This led to the exclusion of an average of 2.4 trials per participant (= 0.7\%; range 0 to 24 trials). Single-trial event-related potentials were computed as the mean amplitude across time windows and regions of interests (ROIs) defined a priori, namely 100--150 ms after object onset at electrodes PO3, PO4, POz, O1, O2, and Oz for the P1 component, 150--200 ms after object onset at electrodes P7, P8, PO7, PO8, PO9, and PO10 for the N170 component, and 400--700 ms after object onset at electrodes C1, C2, Cz, CP1, CP2, and CPz for the N400 component (see scalp topographies in Figure \ref{fig:exp1-plot}B).

\hypertarget{statistical-analysis}{%
\subsubsection{Statistical Analysis}\label{statistical-analysis}}

First, because we were interested in the effects of knowledge on perceiving \emph{unfamiliar} objects only, we excluded from all furthere analyses those objects which participants classified as being known to them in the pre-insight part (i.e.,~before any keywords were presented). This led to the exclusion of an average of 7.5 objects per participant (= 6.3\% of all unfamiliar objects). Next, to clearly delineate semantically informed and naive perception, the assignment of all other objects to one of these two conditions for statistical analyses was co-determined by our experimental manipulation (matching versus non-matching keywords in the insight part) and the behavioral responses of the participants themselves (see Figure \ref{fig:exp1-plot}A). Objects were assigned to the semantically informed condition only if they were presented with matching keywords \emph{and} if participants indicated knowing what the object was or having an assumption. This was the case for an average of 27.0 objects per participant (= 44.9\% of objects presented with matching keywords). Complementarily, objects were assigned to the naive condition only if they were presented with non-matching keywords \emph{and} if participants indicated not knowing what the object was or having rather no assumption. This was the case for an average of 48.8 objects per participant (= 81.2\% of objects presented with non-matching keywords). Although this assignment was based on the manipulation and responses in the second part---when insight was thought to occurr---the same assignment was used to analyze the data from the other two parts. This allowed us to test, on the one hand, if the objects from both conditions differed in important aspects even before any keywords were presented (pre-insight part) and, on the other hand, if the semantic understanding acquired in the insight part had any lasting effects on a subsequent, third presentation of the objects (post-insight part).

The event-related potentials in response to objects from both conditions and all three parts were analyzed on the single trial level using linear mixed-effects regression models (Baayen et al., 2008; Fr√∂mer et al., 2018). For the purpose of the present study, these models have at least two desirable properties compared to traditional approaches such as analyses of variance (ANOVAs) performed on by-participant grand averages. First, they can account not only for the non-independence of data points coming from the same participant but also simultaneously for the non-independence of data points coming from the same item. In contrast, the neglect of the item as a random variable in ANOVAs leads to anti-conservative test statistics and strictly does not allow for inferences beyond the stimulus set under study (B√ºrki et al., 2018; Judd et al., 2012). Second, mixed-effects models can flexibly deal with unbalanced designs in which the number of trials differs across (combinations of) conditions. Such a situation is inevitable in designs where the assignment of trials to conditions is (co-)determined by the experimental manipulation and the responses of the participants rather than by the experimental manipulation alone (e.g., Fr√∂ber et al., 2017).

Three separate models were computed predicting P1, N170, and N400 mean amplitudes, respectively. All models included three fixed effects: (a) the part of the experiment, coded as a repeated contrast (i.e.,~subtracting the first from the second part and the second from the third part, the intercept being the grand mean across all three parts), (b) the condition of the object, coded as a scaled sum contrast (i.e.,~subtracting the naive condition from the semantically informed condition, the intercept being the grand mean across both conditions), and (c) the two-way interaction of part and condition. For details on these and other contrast coding schemes in linear (mixed-effects) models, please refer to Schad et al. (2020). To determine the random effects structure, we always started with a maximal model containing by-participant and by-item random intercepts and random slopes for all fixed effects (Barr et al., 2013). We then performed a model selection algorithm as proposed by Matuschek et al. (2017) in order to increase statistical power and avoid overparameterization: Iteratively, each random effect was removed and the resulting, more parsimonious model was compared to the previous, more complex model by means of a likelihood ratio test. Only if the parsimonious model explained the data equally well as the complex model (determined by \emph{p} \textgreater{} .20; Matuschek et al., 2017) did we leave the random effect out, otherwise it was kept in the final model. All models were computed in R (Version 4.0.2; R Core Team, 2020) using the lme4 package (Version 1.1.23; Bates et al., 2015). The optimizer function \emph{bobyqa} with 20,000 iterations was used for maximum likelihood estimation. The model selection algorithm via likelihood ratio tests was performed using the buildmer package (Version 1.7.1; Voeten, 2020). Finally, to answer our research question of whether or not semantically informed perception had an influence on the ERP components within each part, planned follow-up comparisons were calculated, contrasting the informed against the naive condition within the pre-insight, insight, and post-insight parts. All \emph{p}-values were computed by approximating the relevant denominator degrees of freedom using Satterthwaite's method as implemented in the lmerTest package (Version 3.1.2; Kuznetsova et al., 2017).

The materials, single trial behavioral and ERP data, and all code for data analysis can be accessed via the Open Science Framework (\url{https://osf.io/uksbc/}). Unfortuntaly, lack of informed consent prevents us from sharing the raw EEG data.

\hypertarget{results}{%
\subsection{Results}\label{results}}

Single-trial ERPs were analyzed in response to unfamiliar objects before (pre-insight part), while (insight part), and after (post-insight part) participants obtained relevant semantic information about their function. Only in the insight part, half of the objects were preceded by a matching description, fostering semantically informed perception to occur, whereas the other half were preceded by a non-matching description, leading to naive perception of the object. The objects were analyzed according to this manipulation in combination with participants' self-report in the insight part (see Figure \ref{fig:exp1-plot}A), thereby making sure that semantically informed and naive perception did indeed occur. The analysis focused on differences between these two conditions in the P1 component (100--150 ms) as an index of lower-level visual perception, in the N170 component (150--200 ms) as index of higher-level visual perception, and in the N400 component (400--700 ms) as an index of semantic processing.

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:exp1-table}Results of linear mixed-effects regression models for Experiment 1}

\footnotesize{

\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{2}{c}{\textbf{P1}} & \multicolumn{2}{c}{\textbf{N170}} & \multicolumn{2}{c}{\textbf{N400}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}
\textit{Fixed effects} & \textit{F} (\textit{df}) & \textit{p} & \textit{F} (\textit{df}) & \textit{p} & \textit{F} (\textit{df}) & \textit{p}\\
\midrule
Part & 10.89 (2, 24.5) & < .001 & 14.18 (2, 24.9) & < .001 & 32.85 (2, 25.6) & < .001\\
Condition & 0.60 (1, 5343.5) & .438 & 0.30 (1, 24.2) & .586 & 13.00 (1, 24.4) & .001\\
Pt. √ó con. & 2.30 (2, 4605.0) & .100 & 4.84 (2, 4878.2) & .008 & 10.91 (2, 5120.9) & < .001\\
\textit{Informed $-$  naive} & Est. [95\% CI] & \textit{p} & Est. [95\% CI] & \textit{p} & Est. [95\% CI] & \textit{p}\\ \midrule
Pre-insight part & -0.03 [-0.53, 0.47] & .913 & -0.12 [-0.68, 0.44] & .665 & -0.20 [-0.65, 0.26] & .400\\
Insight part & -0.18 [-0.68, 0.32] & .490 & -0.64 [-1.20, -0.08] & .026 & 0.93 [0.48, 1.39] & < .001\\
Post-insight part & 0.55 [0.05, 1.05] & .031 & 0.43 [-0.14, 0.99] & .136 & 1.00 [0.54, 1.46] & < .001\\
\bottomrule
\addlinespace
\end{tabular}

}

\begin{tablenotes}[para]
\normalsize{\textit{Note.} Pt. = part, con. = condition, est. = estimate, CI = confidence interval.}
\end{tablenotes}

\end{threeparttable}
\end{center}

\end{table}

Averaged across conditions, P1, N170, and N400 amplitudes differed as a function of the part of the experiment, all \emph{F}s \textgreater{} 10.89, all \emph{p}s \textless{} .001 (see Table \ref{tab:exp1-table}). In addition, N400 amplitudes differed between the informed and the naive condition averaged across the three parts of the experiment, \emph{F}(1, 24.4) = 13.00, \emph{p} = .001. Crucially, the part √ó condition interaction was significant in the N170 component, \emph{F}(2, 4878.2) = 4.84, \emph{p} = .008, and in the N400 component, \emph{F}(2, 5120.9) = 10.91, \emph{p} \textless{} .001, while also being marginally significant in the P1 component, \emph{F}(2, 4605.0) = 2.30, \emph{p} = .100. To answer our main research question, we decomposed these interactions into the differences between the semantically informed condition and the naive condition within the three different parts of the experiment.

\hypertarget{erps-before-insight-was-occurring}{%
\subsubsection{ERPs Before Insight Was Occurring}\label{erps-before-insight-was-occurring}}

In the pre-insight part, when objects were unfamiliar to participants and presented without keywords, no differences emerged between the semantically informed and the naive condition in the P1, N170, or N400 component, all \emph{p}s \textgreater{} .400 (see Table \ref{tab:exp1-table} and Figure \ref{fig:exp1-plot}B). On the one hand, this was to be expected given that the critical presentation of the keywords (leading to semantically informed vs.~naive perception) had not yet taken place. On the other hand, the absence of reliable differences in this part can be taken as evidence---with the usual caveats when interpreting null effects---that any subsequent effect of the semantic information in the other two parts cannot be accounted for by visual differences between the objects in the two conditions. Although the presentation of matching or non-matching keywords for each object was counterbalanced across participants, the fact that different numbers of objects were assigned to the two conditions based on participants' self report would have made it possible for such visual differences to emerge as a confounding factor. If they did, however, one would expect to detect these differences even before any keywords were presented, which we have now seen was not the case.

\hypertarget{erps-while-insight-was-occurring}{%
\subsubsection{ERPs While Insight Was Occurring}\label{erps-while-insight-was-occurring}}

In the insight part, half of the unfamiliar objects were presented with matching keywords (for forming the semantically informed condition) and the other half were presented with non-matching keywords (for forming the naive condition). When semantic information informed the perception of the object, the amplitude of the N170 component was significantly enlarged (i.e.,~more negative), \emph{b} = -0.64 ¬µV, \emph{p} = .026, and the amplitude of the N400 component was significantly reduced (i.e.,~less negative), \emph{b} = 0.93 ¬µV, \emph{p} \textless{} .001, compared to when the object was viewed naively without relevant semantic information. As in the pre-insight part, there were no reliable differences in the P1 component, \emph{p} = .490.

\hypertarget{erps-after-insight-had-occurred}{%
\subsubsection{ERPs After Insight Had Occurred}\label{erps-after-insight-had-occurred}}

In the post-insight part, the unfamiliar objects were presented for a third time, again without the keywords (as in the pre-insight part), to test whether the semantic information had any lasting effects on the processing of the objects. As in the insight part, the N400 component remained significantly reduced during semantically informed as compared to naive perception, \emph{b} = 1.00 ¬µV, \emph{p} \textless{} .001, whereas the effect in the N170 component did not reoccur, \emph{p} = .136. Instead, we now observed an even earlier modulation in the P1 component which was significantly enlarged (i.e.,~more positive) in response to objects for which semantically informed perception had taken place, \emph{b} = 0.55 ¬µV, \emph{p} = .031.

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

In Experiment 1, we measured event-related brain potentials from participants viewing unfamiliar objects before (pre-insight part), while (insight part) and after (post-inisight part) they were able to understand what kind of object they saw. To induce this semantically informed perception, half of the objects in the insight part were preceded by matching verbal keywords about the object's typical function or use, whereas the other half were preceded by non-matching keywords, serving as a naive baseline condition.

In the insight part, we found that semantically informed perception was associated with enlarged amplitudes in the N170 component (150--200 ms after object onset) and reduced amplitudes in the N400 component (400--700 ms). When the same objects were presented once more in the post-insigt part, the reduction of the N400 component reoccurred and we also observed a modulation of the P1 component (100--150 ms), which was significantly larger for objects for whic semantically informed perception had taken place.

Because of the novelty of our experimental paradigm and the findings of Experiment 1, we ran a replication study to assess the robustness of these effects in another samples of participants.

\hypertarget{experiment-2}{%
\section{Experiment 2}\label{experiment-2}}

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

\hypertarget{participants-1}{%
\subsubsection{Participants}\label{participants-1}}

Participants for Experiment 2 were 24 German native speakers (15 female, 9 male) with a mean age of 26 years (range 19 to 29 years) who had not participated in Experiment 1. They had no history of psychological disorder or treatment, were right-handed, and reported normal or corrected-to-normal vision. They gave written informed consent before starting the experiment and received a compensation of ‚Ç¨8 per hour for participating.

\hypertarget{materials-procedure-and-analysis}{%
\subsubsection{Materials, Procedure, and Analysis}\label{materials-procedure-and-analysis}}

All materials, procedures, EEG-related methods, and statistical analyses were identical to Experiment 1. An average of 7.0 objects per participant (= 5.8\% of all unfamiliar objects) was classified as being known in the pre-insight part and excluded from all further analyses. Based on participants' responses in the insight part, an average of 26.4 objects were assigned to the semantically informed condition (= 44.0\% of objects presented with matching keywords) and an average of 49.2 objects were assigned to the naive condition (= 82.0\% of objects presented with a non-matching description). Automatic rejection of EEG epochs containing artifacts led to the exclusion of 11.0 trials per participant (= 3.0\%; range 0 to 85 trials).

\hypertarget{results-1}{%
\subsection{Results}\label{results-1}}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:exp2-table}Results of linear mixed-effects regression models for Experiment 2}

\footnotesize{

\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{2}{c}{\textbf{P1}} & \multicolumn{2}{c}{\textbf{N170}} & \multicolumn{2}{c}{\textbf{N400}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}
\textit{Fixed effects} & \textit{F} (\textit{df}) & \textit{p} & \textit{F} (\textit{df}) & \textit{p} & \textit{F} (\textit{df}) & \textit{p}\\
\midrule
Part & 17.45 (2, 5267.1) & < .001 & 16.57 (2, 25.1) & < .001 & 70.12 (2, 25.2) & < .001\\
Condition & 0.02 (1, 5276.9) & .900 & 0.98 (1, 5228.5) & .322 & 19.95 (1, 5242.5) & < .001\\
Pt. √ó con. & 3.60 (2, 5267.1) & .027 & 1.25 (2, 4756.5) & .287 & 7.92 (2, 4714.9) & < .001\\
\textit{Informed $-$  naive} & Est. [95\% CI] & \textit{p} & Est. [95\% CI] & \textit{p} & Est. [95\% CI] & \textit{p}\\ \midrule
Pre-insight part & -0.16 [-0.71, 0.40] & .582 & -0.03 [-0.53, 0.48] & .920 & 0.04 [-0.43, 0.50] & .877\\
Insight part & -0.48 [-1.03, 0.08] & .094 & -0.48 [-0.99, 0.03] & .064 & 1.33 [0.87, 1.79] & < .001\\
Post-insight part & 0.57 [0.01, 1.13] & .047 & 0.06 [-0.45, 0.57] & .818 & 0.47 [0.01, 0.93] & .047\\
\bottomrule
\addlinespace
\end{tabular}

}

\begin{tablenotes}[para]
\normalsize{\textit{Note.} Pt. = part, con. = condition, est. = estimate, CI = confidence interval.}
\end{tablenotes}

\end{threeparttable}
\end{center}

\end{table}

As in Experiment 1, P1, N170, and N400 amplitudes differed between the three different parts of the experiments, all \emph{F}s \textgreater{} 16.57, all \emph{p}s \textless{} .001 (see Table \ref{tab:exp2-table}). Also as in Experiment 1, N400 amplitudes differed between the informed and the naive condition averaged across parts, \emph{F}(1, 5242.5) = 19.95, \emph{p} \textless{} .001. The part √ó condition interaction was significant in the P1 component, \emph{F}(2, 5267.1) = 3.60, \emph{p} = .027, and in the N400 component, \emph{F}(2, 4714.9) = 7.92, \emph{p} \textless{} .001, but not in the N170 component, \emph{F}(2, 4756.5) = 1.25, \emph{p} = .287.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/exp2-plot-1} 

}

\caption{Results of Experiment 2}\label{fig:exp2-plot}
\floatfoot{\scriptsize\emph{Note.} ERP waveforms and scalp topographies are shown for objects for which participants experienced semantically informed versus naive perception within the pre-insight, insight, and post-insight parts of the experiment. In a direct replication of Experiment 1, the effect of semantic information on the N400 component in the insight and post-insight part and on the P1 component in post-inisght part remained statistically significant, while the effect on the N170 component in the insight part remained only marginally significant (\emph{p} = .064). Ampl. = amplitude.\newline*\emph{p} \textless{} .05. **\emph{p} \textless{} .01. ***\emph{p} \textless{} .001.}
\end{figure}

\hypertarget{erps-before-insight-was-occurring-1}{%
\subsubsection{ERPs Before Insight Was Occurring}\label{erps-before-insight-was-occurring-1}}

As in Experiment 1, no differences between objects in the semantically informed and the naive condition emerged in the P1, N170, or N400 component, all \emph{p}s \textgreater{} .582 (see Table \ref{tab:exp2-table} and Figure \ref{fig:exp2-plot}).

\hypertarget{erps-while-insight-was-occurring-1}{%
\subsubsection{ERPs While Insight Was Occurring}\label{erps-while-insight-was-occurring-1}}

As in Experiment 1, semantically informed as compared to naive perception (induced by matching vs.~non-matching keywords) was associated with a (marginally) significant enhancement of the N170 component, \emph{b} = -0.48 ¬µV, \emph{p} = .064, and a significant reduction of the N400 component, \emph{b} = 0.93 ¬µV, \emph{p} \textless{} .001.

\hypertarget{erps-after-insight-had-occurred-1}{%
\subsubsection{ERPs After Insight Had Occurred}\label{erps-after-insight-had-occurred-1}}

As in Experiment 1, the presentation of the same unfamiliar objects for a third time (without keywords, as in the pre-insight part) led to significantly larger amplitudes in the P1 component in response to objects for which semantically informed perception had occurred, \emph{b} = 0.57 ¬µV, \emph{p} = .047. Also, N400 amplitudes in response to these objects remained significantly reduced, \emph{b} = 0.47 ¬µV, \emph{p} = .047.

\hypertarget{joint-analysis-of-experiments-1-and-2}{%
\subsubsection{Joint Analysis of Experiments 1 and 2}\label{joint-analysis-of-experiments-1-and-2}}

In an attempt to maximize statistical power, we combined the ERP data sets from Experiments 1 and 2. This allowed us to determine (a) if the above effects---including the marginally significant ones---were reliable when tested in a larger sample, and (b) if there were significant differences in the ERP amplitudes between Experiments 1 and 2. Methods for statistical analysis were kept unchanged apart from the addition of a new factor denoting the experiment, coded as a scaled sum contrast {[}i.e.,~subtracting Experiment 1 from Experiment 2, the intercept being the grand mean across both experiments; Schad et al. (2020){]}. This factor and its possible interactions with part, condition, and part √ó condition were included in the linear mixed-effects regression models as fixed effects and as potential by-item random slopes. They were not included as by-participant random slopes since different participants took part in Experiments 1 and 2. Note that, as above, random effects were eventually included only if their omission led to a significant decline in model fit (Matuschek et al., 2017; Voeten, 2020).

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:joint-table}Results of linear mixed-effects regression models for Experiments 1 and 2 combined}

\footnotesize{

\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{2}{c}{\textbf{P1}} & \multicolumn{2}{c}{\textbf{N170}} & \multicolumn{2}{c}{\textbf{N400}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}
\textit{Fixed effects} & \textit{F} (\textit{df}) & \textit{p} & \textit{F} (\textit{df}) & \textit{p} & \textit{F} (\textit{df}) & \textit{p}\\
\midrule
Part & 12.34 (2, 49.1) & < .001 & 30.08 (2, 50.3) & < .001 & 94.61 (2, 50.6) & < .001\\
Condition & 0.15 (1, 10573.1) & .699 & 1.16 (1, 45.2) & .287 & 41.38 (1, 10593.6) & < .001\\
Experiment & 0.33 (1, 48.2) & .567 & 2.50 (1, 48.0) & .121 & 4.96 (1, 48.4) & .031\\
Pt. √ó con. & 3.90 (2, 9004.9) & .020 & 5.26 (2, 9904.1) & .005 & 15.34 (2, 10091.0) & < .001\\
Pt. √ó exp. & 0.04 (2, 49.1) & .966 & 0.58 (2, 50.3) & .565 & 2.55 (2, 50.6) & .088\\
Ins. √ó exp. & 0.43 (1, 10573.1) & .514 & 0.06 (1, 45.2) & .805 & 0.07 (1, 10593.6) & .785\\
Pt. √ó con. √ó exp. & 0.03 (2, 9004.9) & .968 & 0.58 (2, 9904.1) & .559 & 2.54 (2, 10091.0) & .079\\
\textit{Informed $-$  naive} & Est. [95\% CI] & \textit{p} & Est. [95\% CI] & \textit{p} & Est. [95\% CI] & \textit{p}\\ \midrule
Pre-insight part & -0.10 [-0.47, 0.28] & .616 & -0.10 [-0.48, 0.29] & .611 & -0.09 [-0.39, 0.22] & .577\\
Insight part & -0.24 [-0.62, 0.13] & .202 & -0.57 [-0.96, -0.18] & .004 & 1.10 [0.80, 1.41] & < .001\\
Post-insight part & 0.47 [0.09, 0.84] & .014 & 0.24 [-0.15, 0.63] & .225 & 0.73 [0.42, 1.04] & < .001\\
\bottomrule
\addlinespace
\end{tabular}

}

\begin{tablenotes}[para]
\normalsize{\textit{Note.} Pt. = part, con. = condition, exp. = experiment, est. = estimate, CI = confidence interval.}
\end{tablenotes}

\end{threeparttable}
\end{center}

\end{table}

As shown in Table \ref{tab:joint-table}, the main effect of the part of the experiment was significant in the P1, N170, and N400 component, all \emph{F}s \textgreater{} 12.34, all \emph{p}s \textless{} .001, as was the main effect of condition in the N400, \emph{F}(1, 10593.6) = 41.38, \emph{p} \textless{} .001. Furthermore, the part √ó condition interaction was now observed reliably in all three components, all \emph{F}s \textgreater{} 3.90, all \emph{p}s \textless{} .020. While there was a main effect of experiment in the N400, \emph{F}(1, 48.4) = 4.96, \emph{p} = .031, the absence of any significant interactions of experiment with part or condition indicated that the effects of our experimental manipulations did not differ between Experiments 1 and 2.

Based on the part √ó condition interaction, we again computed follow-up comparisons between the semantically informed and the naive condition within each part, now collapsed across the data from both experiments (see Figure \ref{fig:joint-plot}). This confirmed the absence of any reliable differences between the two conditions in the pre-insight part, all \emph{p}s \textgreater{} .577, the significant enhancement of the N170 component in the insight part, while the semantic information was obtained, \emph{b} = -0.57 ¬µV, \emph{p} = .004, the significant reduction of the N400 component in the insight part, while the semantic information was obtained, \emph{b} = 1.10 ¬µV, \emph{p} \textless{} .001, and in the post-insight part, after the information had been obtained, \emph{b} = 0.73 ¬µV, \emph{p} \textless{} .001, as well as the significant enhancement of the P1 component in the post-insight part, after the information had been obtained, \emph{b} = 0.47 ¬µV, \emph{p} = .014.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{manuscript_files/figure-latex/joint-plot-1} 

}

\caption{Measured and Modelled ERP Amplitudes for Experiments 1 and 2 Combined}\label{fig:joint-plot}
\floatfoot{\scriptsize\emph{Note.} The violins and boxplots show the distributions of the by-participant grand averaged ERPs during semantically informed perception (in purple color) and naive perception (in petrol color), separately for the pre-insight part (before insight was occurring), the insight part (while insight was occurring), and the post-insight (after insight had occurred). The yellow dots show the corresponding means of these conditions as predicted by linear mixed-effects modelling (see main text and Table \ref{tab:joint-table}), together with their respective 95\% confidence interval. Ampl. = amplitude, CI = confidence interval, LMM = linear mixed-effects model.\newline*\emph{p} \textless{} .05. **\emph{p} \textless{} .01. ***\emph{p} \textless{} .001.}
\end{figure}

\hypertarget{control-analysis}{%
\subsubsection{Control Analysis}\label{control-analysis}}

One may raise concerns whether the modulation of the N170 component in the insight part genuinely reflects the semantically informed perception of the objects in the respective condition, or---as an alternative explanation---whether it may be driven by the objects in the other, semantically naive condition. Remember that these objects were preceded by non-matching keywords which were picked so that they could not be related to the visual features of the object and their configuration. Thus, the modulation of the ERP components in the insight part could be a mismatch response to those objects---reflecting, for example, the fact that the visual features of the object shown in Figure \ref{fig:exp1-plot}A cannot be reconciled with the function of morsing messages. To preclude this alternative explanation, we repeated our analysis using a different baseline against which the objects in the semantically informed condition were contrasted. Instead of the naive condition (where objects were presented with non-matching descriptions), we now used those objects which were preceded by matching descriptions (as in the informed condition), but which were excluded from the main analysis because participants indicated behaviorally that they did not understand the object they were seeing. Across both experiments, this was the case for 49.6\% of objects presented with matching descriptions as compared to 44.4\% of objects which did indeed lead to semantically informed perception.\footnote{Note that these percentages do not add up to 100\% because some objects were excluded from all analyses because participants indicated knowing these objects in the pre-insight part, before any keywords had been presented (see Methods).} Just as above, this control analysis revealed a robust N170 effect in the insight part, \emph{b} = -1.00 ¬µV, \emph{p} \textless{} .001. Thus, this enhanced negativity seems to be a genuine marker of semantically informed perception, no matter if compared to objects presented with misleading keywords or compared to objects presented with accurate keywords but on which participants failed to capitalize. Note that the reduction of the N400 component in the insight part also remained robust in this control analysis, \emph{b} = 1.25 ¬µV, \emph{p} \textless{} .001.

\hypertarget{discussion-1}{%
\subsection{Discussion}\label{discussion-1}}

Experiment 2, which was a direct replication of Experiment 1, confirmed the effects of obtaining semantic information about previously unfamiliar objects on ERPs associated with lower-level visual perception (P1), higher-level visual perception (N170), and semantic processing (N400). Objects for which participants experienced semantically informed perception via matcing keywords elicited enlarged N170 amplitudes and reduced N400 amplitudes. When these objects were presented once more, they again elicited reduced N400 amplitudes as well as enlarged P1 amplitudes.

\hypertarget{general-discussion}{%
\section{General Discussion}\label{general-discussion}}

\hypertarget{summary-of-main-findings}{%
\paragraph{Summary of Main Findings}\label{summary-of-main-findings}}

Here we investigated if obtaining a semantic understanding of what a previously unfamiliar object is or does on how has an influence on how we perceive and process it. To this end, we have measured ERPs while participants watched unfamiliar objects before, while, and after receiving semantic information about them. For half of the objects, this information was matching the object, thus leading to semantically informed perception, whereas for the other half of the objects, it was misleading, thus leading to naive perception. We found semantically informed percpetion to be accompanied by enlarged (i.e.,more negative) N170 amplitudes and reduced (i.e.,less negative) N400 amplitudes. If the same objects were presented again without the semantic information, only the N400 component remaind significantly reduced and we also observed a modulation of the P1 component, which was enlarged (i.e.,more positive) in response to objects that had previously triggered semantically informed as compared to naive perception.

\hypertarget{interpretation-of-the-n170-effect}{%
\paragraph{Interpretation of the N170 Effect}\label{interpretation-of-the-n170-effect}}

\hypertarget{interpretation-of-the-n400-effect}{%
\paragraph{Interpretation of the N400 Effect}\label{interpretation-of-the-n400-effect}}

\hypertarget{interpretation-of-the-p1-effect}{%
\paragraph{Interpretation of the P1 Effect}\label{interpretation-of-the-p1-effect}}

\hypertarget{theoretical-considerations}{%
\paragraph{Theoretical Considerations}\label{theoretical-considerations}}

While the limited spatial resolution of the EEG does not allow for localizing these effects within the ventral stream of object recognition, there is converging evidence coming from fMRI studies that semantic information can feed back into the earliest of visual areas. (Hsieh et al., 2010) showed participants indiscernible two-tone (``Mooney'') versions of images before and after showing them the original versions. They found that the brain responses to the original image was correlated more strongly with the second presentation of the Mooney image (after insight had taken place) than with the first presentation of the Mooney image (before insight had taken place). Interestingly, this increase in representational similarity was not just observed in higher-level object-sensitive areas in the lateral occipital complex (LOC), but also in early retinotopic cortex (V1/V2/V3). Both of these cortical sites are consistent with the neural generators of the P1 and N170 components of the ERP, both of which we have found to be sensitive to the semantically informed perception of previously unfamiliar objects.

\hypertarget{limitations}{%
\paragraph{Limitations}\label{limitations}}

\hypertarget{conclusion-outlook}{%
\paragraph{Conclusion \& Outlook}\label{conclusion-outlook}}

\newpage

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\begin{center}
\begin{ThreePartTable}

\begin{longtable}{llll}\noalign{\getlongtablewidth\global\LTcapwidth=\longtablewidth}
\caption{\label{tab:appendix}Unfamiliar object stimuli}\\
\toprule
 & \# & Matching keywords & Non-matching keywords\\
\midrule
\endfirsthead
\caption*{\normalfont{Table \ref{tab:appendix} continued}}\\
\toprule
 & \# & Matching keywords & Non-matching keywords\\
\midrule
\endhead
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/1.png} & 1 & elektrische Spannung pr√ºfen & Makkaroni formen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/2.png} & 2 & Makkaroni formen & Kuh vom Zaun abhalten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/3.png} & 3 & Knochen s√§gen & Streckenma√ü Sonnenlicht nutzen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/4.png} & 4 & Unkraut j√§ten & Uhr mit W√§rme betreiben\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/5.png} & 5 & Mausefalle zuschnappen & Brillenglas zuschneiden\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/6.png} & 6 & Goldm√ºnzen wiegen & Farbe vom Fenster abschleifen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/7.png} & 7 & Knie fixieren & Buchstaben tippen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/8.png} & 8 & Eierkarton pressen & Tabak zermahlen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/9.png} & 9 & Baum erklettern & Rotation Ladung erzeugen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/10.png} & 10 & Buchstaben tippen & Pferdehuf Halt geben\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/11.png} & 11 & Akkordeon spielen & Seil schneiden\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/12.png} & 12 & K√∂rper trainieren & Buch offen halten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/13.png} & 13 & Farbe vom Fenster abschleifen & von Hand zentrifugieren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/14.png} & 14 & Au√üenbereich heizen & Rasierklinge sch√§rfen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/15.png} & 15 & Pflanzenteile vergr√∂√üern & Katzenklo sich selbst reinigen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/16.png} & 16 & Krawatten aufh√§ngen & Zeichnungen vermessen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/17.png} & 17 & Schallplatte abtasten & Ball katapultieren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/18.png} & 18 & Glas schneiden & Eier wiegen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/19.png} & 19 & Briketts pressen & Narkosemittel abgeben\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/20.png} & 20 & Orgelton erzeugen & Bandage rollen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/21.png} & 21 & Spannung erzeugen & Fu√üst√ºtze reiten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/22.png} & 22 & Narkosemittel abgeben & N√ºsse aufbrechen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/23.png} & 23 & Bandage rollen & Schnee rodeln\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/24.png} & 24 & Weinfass Loch einschlagen & Tier einfangen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/25.png} & 25 & Flaschenkorken einf√ºhren & Pferd im Moor laufen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/26.png} & 26 & Automat Eier ausbr√ºten & Windgeschwindigkeit messen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/27.png} & 27 & Korngarbe greifen & Treibhaus heizen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/28.png} & 28 & Brief wiegen & Korngarbe greifen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/29.png} & 29 & L√∂cher bohren & Sonnenlicht Intensit√§t messen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/30.png} & 30 & Angelschnur kurbeln & Glask√∂rper musizieren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/31.png} & 31 & Kurven malen & Nuss√∂l pressen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/32.png} & 32 & Radiofrequenz einstellen & Erektion helfen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/33.png} & 33 & Z√§pfchen pressen & Unkraut j√§ten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/34.png} & 34 & Fass √∂ffnen & Pflanzenteile vergr√∂√üern\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/35.png} & 35 & Bergbaustollen beleuchten & Knie fixieren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/36.png} & 36 & Kuh vom Zaun abhalten & Radiofrequenz einstellen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/37.png} & 37 & Saatgut gleichm√§√üig auss√§en & Fass √∂ffnen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/38.png} & 38 & Flaschen trocknen & Bleistift anspitzen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/39.png} & 39 & Uhr mit W√§rme betreiben & K√∂rper untersuchen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/40.png} & 40 & Tonpott trommeln & Piano stimmen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/41.png} & 41 & Sprengstoffexplosion ausl√∂sen & Fisch wiegen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/42.png} & 42 & Pferdehuf Halt geben & Z√§pfchen pressen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/43.png} & 43 & Bleistift anspitzen & Angel K√∂der markieren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/44.png} & 44 & Nuss√∂l pressen & Zeichen einbrennen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/45.png} & 45 & Rasierklinge sch√§rfen & Kurven malen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/46.png} & 46 & hei√üe Platten anheben & Kleidung im Eimer waschen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/47.png} & 47 & Film aufspulen & Mund offen halten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/48.png} & 48 & Waffe entflammen & Uhrzeit anzeigen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/49.png} & 49 & Mikroskop-Proben schneiden & hei√üe Platten anheben\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/50.png} & 50 & Glask√∂rper musizieren & Kork flach pressen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/51.png} & 51 & Dampf zerst√§uben & Buch binden\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/52.png} & 52 & Mandeln operieren & Film aufspulen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/53.png} & 53 & Toastbrot r√∂sten & Autobatterie Spannung testen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/54.png} & 54 & Fu√üst√ºtze reiten & Stromst√§rke messen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/55.png} & 55 & T√ºrgelenk Feuer √ºberstehen & Botschaft telegrafieren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/56.png} & 56 & Schnee rodeln & Dampf zerst√§uben\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/57.png} & 57 & N√ºsse aufbrechen & Ziegelsteine formen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/58.png} & 58 & Radiergummi mit Strom betreiben & M√ºnzen aufbewahren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/59.png} & 59 & Treibhaus heizen & Messer schleifen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/60.png} & 60 & Botschaft telegrafieren & Toastbrot r√∂sten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/61.png} & 61 & Angel K√∂der markieren & Baum erklettern\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/62.png} & 62 & Katzenklo sich selbst reinigen & Tankf√ºllstand messen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/63.png} & 63 & K√∂rper untersuchen & Schuhe auf Eis laufen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/64.png} & 64 & Brillenglas zuschneiden & Saatgut gleichm√§√üig auss√§en\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/65.png} & 65 & Draht wickeln & Bergbaustollen beleuchten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/66.png} & 66 & Buch offen halten & Sprengstoffexplosion ausl√∂sen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/67.png} & 67 & Feldanbau h√§ckseln & Draht wickeln\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/68.png} & 68 & Schnurlot absenken & Korken formen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/69.png} & 69 & Sternenbilder vermessen & Schnurlot absenken\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/70.png} & 70 & Nachricht morsen & Feldanbau h√§ckseln\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/71.png} & 71 & Musikger√§t stampfen & Goldm√ºnzen wiegen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/72.png} & 72 & Schuhe auf Eis laufen & Geschwindigkeit ermitteln\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/73.png} & 73 & Geschwindigkeit ermitteln & Mausefalle zuschnappen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/74.png} & 74 & Zeichen einbrennen & Luftdruck messen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/75.png} & 75 & Tabak zermahlen & Flaschen trocknen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/76.png} & 76 & Stromst√§rke messen & Mandeln operieren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/77.png} & 77 & Tier einfangen & Maiskolben entk√∂rnen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/78.png} & 78 & Messer schleifen & Schritte vergr√∂√üern\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/79.png} & 79 & Leierkasten klingen & Fass anheben\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/80.png} & 80 & Buch binden & L√∂cher bohren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/81.png} & 81 & Kork flach pressen & Elektroschock spielen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/82.png} & 82 & Tabletten zerteilen & Blumentopf sich selbst w√§ssern\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/83.png} & 83 & Fass anheben & Mikroskop-Proben schneiden\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/84.png} & 84 & Pferd im Moor laufen & Luft abpumpen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/85.png} & 85 & altes Ritual hacken & Spannung erzeugen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/86.png} & 86 & Kleidung im Eimer waschen & T√ºrgelenk Feuer √ºberstehen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/87.png} & 87 & Maiskolben entk√∂rnen & Brief wiegen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/88.png} & 88 & Ball katapultieren & Radiergummi mit Strom betreiben\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/89.png} & 89 & Zeichnungen vermessen & Brenner l√∂ten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/90.png} & 90 & Elektroschock spielen & Weinfass Loch einschlagen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/91.png} & 91 & Lieferungen abz√§hlen & Au√üenbereich heizen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/92.png} & 92 & Rotation Ladung erzeugen & Musikger√§t stampfen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/93.png} & 93 & Herz durch Maschine ersetzen & Kerzen l√∂schen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/94.png} & 94 & Seil schneiden & Akkordeon spielen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/95.png} & 95 & Fisch wiegen & Herz durch Maschine ersetzen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/96.png} & 96 & Kerzen l√∂schen & K√∂rper trainieren\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/97.png} & 97 & Korken formen & Sternenbilder vermessen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/98.png} & 98 & Erektion helfen & Waffe werfen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/99.png} & 99 & Streckenma√ü Sonnenlicht nutzen & Kartoffeln stampfen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/100.png} & 100 & Luftdruck messen & Knochen s√§gen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/101.png} & 101 & Piano stimmen & Eierkarton pressen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/102.png} & 102 & von Hand zentrifugieren & Lieferungen abz√§hlen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/103.png} & 103 & Kartoffeln stampfen & Nachricht morsen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/104.png} & 104 & Waffe werfen & elektrische Spannung pr√ºfen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/105.png} & 105 & Tankf√ºllstand messen & Tonpott trommeln\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/106.png} & 106 & Uhrzeit anzeigen & altes Ritual hacken\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/107.png} & 107 & Windgeschwindigkeit messen & Waffe entflammen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/108.png} & 108 & Schl√ºsselloch stanzen & Automat Eier ausbr√ºten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/109.png} & 109 & Ziegelsteine formen & Schallplatte abtasten\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/110.png} & 110 & Becher Schall auffangen & Schl√ºsselloch stanzen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/111.png} & 111 & Luft abpumpen & Briketts pressen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/112.png} & 112 & Mund offen halten & Orgelton erzeugen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/113.png} & 113 & Autobatterie Spannung testen & Becher Schall auffangen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/114.png} & 114 & Sonnenlicht Intensit√§t messen & Krawatten aufh√§ngen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/115.png} & 115 & Kutschrad anschlie√üen & Angelschnur kurbeln\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/116.png} & 116 & Eier wiegen & Leierkasten klingen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/117.png} & 117 & Blumentopf sich selbst w√§ssern & Glas schneiden\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/118.png} & 118 & Schritte vergr√∂√üern & Tabletten zerteilen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/119.png} & 119 & M√ºnzen aufbewahren & Kutschrad anschlie√üen\\
\includegraphics[valign=c, scale=0.2]{../materials/unfamiliar/120.png} & 120 & Brenner l√∂ten & Flaschenkorken einf√ºhren einf√ºhren\\
\bottomrule
\end{longtable}

\end{ThreePartTable}
\end{center}

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\setlength{\parindent}{-0.5in}
%\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-abdelrahman2011}{}%
Abdel Rahman, R. (2011). Facing good and evil: Early brain signatures of affective biographical knowledge in face recognition. \emph{Emotion}, \emph{11}(6), 1397--1405. \url{https://doi.org/10.1037/a0024717}

\leavevmode\hypertarget{ref-abdelrahman2008}{}%
Abdel Rahman, R., \& Sommer, W. (2008). Seeing what we know and understand: How knowledge shapes perception. \emph{Psychonomic Bulletin \& Review}, \emph{15}(6), 1055--1063. \url{https://doi.org/10.3758/PBR.15.6.1055}

\leavevmode\hypertarget{ref-ahissar2004}{}%
Ahissar, M., \& Hochstein, S. (2004). The reverse hierarchy theory of visual perceptual learning. \emph{Trends in Cognitive Sciences}, \emph{8}(10), 457--464. \url{https://doi.org/10.1016/j.tics.2004.08.011}

\leavevmode\hypertarget{ref-americanelectroencephalographicsociety1991}{}%
American Electroencephalographic Society. (1991). {American Electroencephalographic Society} guidelines for standard electrode position nomenclature. \emph{Journal of Clinical Neurophysiology}, \emph{8}(2), 200--202.

\leavevmode\hypertarget{ref-baayen2008}{}%
Baayen, R. H., Davidson, D. J., \& Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. \emph{Journal of Memory and Language}, \emph{59}(4), 390--412. \url{https://doi.org/10.1016/j.jml.2007.12.005}

\leavevmode\hypertarget{ref-balcetis2010}{}%
Balcetis, E., \& Dunning, D. (2010). Wishful seeing: More desired objects are seen as closer. \emph{Psychological Science}, \emph{21}(1), 147--152. \url{https://doi.org/10.1177/0956797609356283}

\leavevmode\hypertarget{ref-barr2013}{}%
Barr, D. J., Levy, R., Scheepers, C., \& Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. \emph{Journal of Memory and Language}, \emph{68}(3), 255--278. \url{https://doi.org/10.1016/j.jml.2012.11.001}

\leavevmode\hypertarget{ref-R-lme4}{}%
Bates, D., M√§chler, M., Bolker, B., \& Walker, S. (2015). Fitting linear mixed-effects models using {lme4}. \emph{Journal of Statistical Software}, \emph{67}(1), 1--48. \url{https://doi.org/10.18637/jss.v067.i01}

\leavevmode\hypertarget{ref-bentin1996}{}%
Bentin, S., Allison, T., Puce, A., Perez, E., \& McCarthy, G. (1996). Electrophysiological studies of face perception in humans. \emph{Journal of Cognitive Neuroscience}, \emph{8}(6), 551--565. \url{https://doi.org/10.1162/jocn.1996.8.6.551}

\leavevmode\hypertarget{ref-bocanegra2009}{}%
Bocanegra, B. R., \& Zeelenberg, R. (2009). Emotion improves and impairs early vision. \emph{Psychological Science}, \emph{20}(6), 707--713. \url{https://doi.org/10.1111/j.1467-9280.2009.02354.x}

\leavevmode\hypertarget{ref-boutonnet2015}{}%
Boutonnet, B., \& Lupyan, G. (2015). Words jump-start vision: A label advantage in object recognition. \emph{Journal of Neuroscience}, \emph{35}(25), 9329--9335. \url{https://doi.org/10.1523/JNEUROSCI.5111-14.2015}

\leavevmode\hypertarget{ref-bullier2001}{}%
Bullier, J. (2001). Integrated model of visual processing. \emph{Brain Research Reviews}, \emph{36}(2), 96--107. \url{https://doi.org/10.1016/S0165-0173(01)00085-6}

\leavevmode\hypertarget{ref-buxfcrki2018}{}%
B√ºrki, A., Frossard, J., \& Renaud, O. (2018). Accounting for stimulus and participant effects in event-related potential analyses to increase the replicability of studies. \emph{Journal of Neuroscience Methods}, \emph{309}, 218--227. \url{https://doi.org/10.1016/j.jneumeth.2018.09.016}

\leavevmode\hypertarget{ref-churchland1988}{}%
Churchland, P. M. (1988). Perceptual plasticity and theoretical neutrality: A reply to {Jerry} {Fodor}. \emph{Philosophy of Science}, \emph{55}(2), 167--187. \url{https://doi.org/10.1086/289425}

\leavevmode\hypertarget{ref-churchland1994}{}%
Churchland, P. S., Ramachandran, V. S., \& Sejnowski, T. J. (1994). A critique of pure vision. In C. Koch \& J. L. Davis (Eds.), \emph{Computational neuroscience. Large-scale neuronal theories of the brain} (pp. 23--60). MIT Press.

\leavevmode\hypertarget{ref-cole2012}{}%
Cole, S., Balcetis, E., \& Dunning, D. (2012). Affective signals of threat increase perceived proximity. \emph{Psychological Science}. \url{https://doi.org/10.1177/0956797612446953}

\leavevmode\hypertarget{ref-collins2014}{}%
Collins, J. A., \& Olson, I. R. (2014). Knowledge is power: How conceptual knowledge transforms visual cognition. \emph{Psychonomic Bulletin \& Review}, \emph{21}(4), 843--860. \url{https://doi.org/10.3758/s13423-013-0564-3}

\leavevmode\hypertarget{ref-dirusso2001}{}%
Di Russo, F., Mart√≠nez, A., Sereno, M. I., Pitzalis, S., \& Hillyard, S. A. (2001). Cortical sources of the early components of the visual evoked potential. \emph{Human Brain Mapping}, \emph{15}(2), 95--111. \url{https://doi.org/10.1002/hbm.10010}

\leavevmode\hypertarget{ref-eimer2011}{}%
Eimer, M., Gosling, A., Nicholas, S., \& Kiss, M. (2011). The {N170} component and its links to configural face processing: A rapid neural adaptation study. \emph{Brain Research}, \emph{1376}, 76--87. \url{https://doi.org/10.1016/j.brainres.2010.12.046}

\leavevmode\hypertarget{ref-eiserbeck2020}{}%
Eiserbeck, A., \& Abdel Rahman, R. (2020). Visual consciousness of faces in the attentional blink: Knowledge-based effects of trustworthiness dominate over appearance-based impressions. \emph{Consciousness and Cognition}, \emph{83}, 102977. \url{https://doi.org/10.1016/j.concog.2020.102977}

\leavevmode\hypertarget{ref-firestone2016}{}%
Firestone, C., \& Scholl, B. J. (2016). Cognition does not affect perception: Evaluating the evidence for {``top-down''} effects. \emph{Behavioral and Brain Sciences}, \emph{39}. \url{https://doi.org/10.1017/S0140525X15000965}

\leavevmode\hypertarget{ref-fodor1984}{}%
Fodor, J. A. (1984). Observation reconsidered. \emph{Philosophy of Science}, \emph{51}(1), 23--43. \url{https://doi.org/10.1086/289162}

\leavevmode\hypertarget{ref-fodor1983}{}%
Fodor, J. A. (1983). \emph{The modularity of mind}. MIT Press.

\leavevmode\hypertarget{ref-foxe2002}{}%
Foxe, J. J., \& Simpson, G. V. (2002). Flow of activation from {V1} to frontal cortex in humans. \emph{Experimental Brain Research}, \emph{142}(1), 139--150. \url{https://doi.org/10.1007/s00221-001-0906-7}

\leavevmode\hypertarget{ref-fruxf6ber2017}{}%
Fr√∂ber, K., St√ºrmer, B., Fr√∂mer, R., \& Dreisbach, G. (2017). The role of affective evaluation in conflict adaptation: An {LRP} study. \emph{Brain and Cognition}, \emph{116}, 9--16. \url{https://doi.org/10.1016/j.bandc.2017.05.003}

\leavevmode\hypertarget{ref-fruxf6mer2018}{}%
Fr√∂mer, R., Maier, M., \& Abdel Rahman, R. (2018). Group-level {EEG}-processing pipeline for flexible single trial-based analyses including linear mixed models. \emph{Frontiers in Neuroscience}, \emph{12}. \url{https://doi.org/10.3389/fnins.2018.00048}

\leavevmode\hypertarget{ref-gauthier2003a}{}%
Gauthier, I., Curran, T., Curby, K. M., \& Collins, D. (2003). Perceptual interference supports a non-modular account of face processing. \emph{Nature Neuroscience}, \emph{6}(4), 428--432. \url{https://doi.org/10.1038/nn1029}

\leavevmode\hypertarget{ref-gauthier2003}{}%
Gauthier, I., James, T. W., Curby, K. M., \& Tarr, M. J. (2003). The influence of conceptual knowledge on visual discrimination. \emph{Cognitive Neuropsychology}, \emph{20}(3-6), 507--523. \url{https://doi.org/10.1080/02643290244000275}

\leavevmode\hypertarget{ref-gilbert2013}{}%
Gilbert, C. D., \& Li, W. (2013). Top-down influences on visual processing. \emph{Nature Reviews Neuroscience}, \emph{14}(5), 350--363. \url{https://doi.org/10.1038/nrn3476}

\leavevmode\hypertarget{ref-goodale1992}{}%
Goodale, M. A., \& Milner, A. D. (1992). Separate visual pathways for perception and action. \emph{Trends in Neurosciences}, \emph{15}(1), 20--25. \url{https://doi.org/10.1016/0166-2236(92)90344-8}

\leavevmode\hypertarget{ref-gramfort2013}{}%
Gramfort, A., Luessi, M., Larson, E., Engemann, D. A., Strohmeier, D., Brodbeck, C., Goj, R., Jas, M., Brooks, T., Parkkonen, L., \& al., et. (2013). MEG and {EEG} data analysis with {MNE-Python}. \emph{Frontiers in Neuroscience}, \emph{7}. \url{https://doi.org/10.3389/fnins.2013.00267}

\leavevmode\hypertarget{ref-gratton2009}{}%
Gratton, C., Evans, K. M., \& Federmeier, K. D. (2009). See what {I} mean? An {ERP} study of the effect of background knowledge on novel object processing. \emph{Memory \& Cognition}, \emph{37}(3), 277--291. \url{https://doi.org/10.3758/MC.37.3.277}

\leavevmode\hypertarget{ref-hochstein2002}{}%
Hochstein, S., \& Ahissar, M. (2002). View from the top: Hierarchies and reverse hierarchies in the visual system. \emph{Neuron}, \emph{36}(5), 791--804. \url{https://doi.org/10.1016/S0896-6273(02)01091-7}

\leavevmode\hypertarget{ref-hsieh2010}{}%
Hsieh, P.-J., Vul, E., \& Kanwisher, N. (2010). Recognition alters the spatial pattern of {fMRI} activation in early retinotopic cortex. \emph{Journal of Neurophysiology}, \emph{103}(3), 1501--1507. \url{https://doi.org/10.1152/jn.00812.2009}

\leavevmode\hypertarget{ref-hyvuxe4rinen1999}{}%
Hyv√§rinen, A. (1999). Fast and robust fixed-point algorithms for independent component analysis. \emph{IEEE Transactions on Neural Networks}, \emph{10}(3), 626--634. \url{https://doi.org/10.1109/72.761722}

\leavevmode\hypertarget{ref-jacques2010}{}%
Jacques, C., \& Rossion, B. (2010). Misaligning face halves increases and delays the {N170} specifically for upright faces: Implications for the nature of early face representations. \emph{Brain Research}, \emph{1318}, 96--109. \url{https://doi.org/10.1016/j.brainres.2009.12.070}

\leavevmode\hypertarget{ref-johannes1995}{}%
Johannes, S., M√ºnte, T. F., Heinze, H. J., \& Mangun, G. R. (1995). Luminance and spatial attention effects on early visual processing. \emph{Cognitive Brain Research}, \emph{2}(3), 189--205. \url{https://doi.org/10.1016/0926-6410(95)90008-X}

\leavevmode\hypertarget{ref-judd2012}{}%
Judd, C. M., Westfall, J., \& Kenny, D. A. (2012). Treating stimuli as a random factor in social psychology: A new and comprehensive solution to a pervasive but largely ignored problem. \emph{Journal of Personality and Social Psychology}, \emph{103}(1), 54--69. \url{https://doi.org/10.1037/a0028347}

\leavevmode\hypertarget{ref-kutas2011}{}%
Kutas, M., \& Federmeier, K. D. (2011). Thirty years and counting: Finding meaning in the {N400} component of the event-related brain potential ({ERP}). \emph{Annual Review of Psychology}, \emph{62}, 621--647. \url{https://doi.org/10.1146/annurev.psych.093008.131123}

\leavevmode\hypertarget{ref-R-lmerTest}{}%
Kuznetsova, A., Brockhoff, P. B., \& Christensen, R. H. B. (2017). {lmerTest} package: Tests in linear mixed effects models. \emph{Journal of Statistical Software}, \emph{82}(13), 1--26. \url{https://doi.org/10.18637/jss.v082.i13}

\leavevmode\hypertarget{ref-lau2008}{}%
Lau, E. F., Phillips, C., \& Poeppel, D. (2008). A cortical network for semantics: (De)constructing the {N400}. \emph{Nature Reviews Neuroscience}, \emph{9}(12), 920--933. \url{https://doi.org/10.1038/nrn2532}

\leavevmode\hypertarget{ref-lin1997}{}%
Lin, E. L., \& Murphy, G. L. (1997). Effects of background knowledge on object categorization and part detection. \emph{Journal of Experimental Psychology: Human Perception and Performance}, \emph{23}(4), 1153--1169. \url{https://doi.org/10.1037/0096-1523.23.4.1153}

\leavevmode\hypertarget{ref-luck2014}{}%
Luck, S. J. (2014). Overview of common ERP components. In \emph{An introduction to the event-related potential technique} (2nd ed., pp. 71--118). MIT Press.

\leavevmode\hypertarget{ref-luck2000}{}%
Luck, S. J., Woodman, G. F., \& Vogel, E. K. (2000). Event-related potential studies of attention. \emph{Trends in Cognitive Sciences}, \emph{4}(11), 432--440. \url{https://doi.org/10.1016/S1364-6613(00)01545-X}

\leavevmode\hypertarget{ref-lupyan2015}{}%
Lupyan, G. (2015). Cognitive penetrability of perception in the age of prediction: Predictive systems are penetrable systems. \emph{Review of Philosophy and Psychology}, \emph{6}(4), 547--569. \url{https://doi.org/10.1007/s13164-015-0253-4}

\leavevmode\hypertarget{ref-lupyan2012}{}%
Lupyan, G. (2012). Linguistically modulated perception and cognition: The label-feedback hypothesis. \emph{Frontiers in Psychology}, \emph{3}. \url{https://doi.org/10.3389/fpsyg.2012.00054}

\leavevmode\hypertarget{ref-machery2015}{}%
Machery, E. (2015). Cognitive penetrability: A no-progress report. In J. Zeimbekis \& A. Raftopoulos (Eds.), \emph{The cognitive penetrability of perception: New philosophical perspectives}. Oxford University Press. \url{https://doi.org/10.1093/acprof:oso/9780198738916.003.0002}

\leavevmode\hypertarget{ref-maier2018}{}%
Maier, M., \& Abdel Rahman, R. (2018). Native language promotes access to visual consciousness. \emph{Psychological Science}, \emph{29}(11), 1757--1772. \url{https://doi.org/10.1177/0956797618782181}

\leavevmode\hypertarget{ref-maier2014}{}%
Maier, M., Glage, P., Hohlfeld, A., \& Abdel Rahman, R. (2014). Does the semantic content of verbal categories influence categorical perception? An {ERP} study. \emph{Brain and Cognition}, \emph{91}, 1--10. \url{https://doi.org/10.1016/j.bandc.2014.07.008}

\leavevmode\hypertarget{ref-maier2019}{}%
Maier, M., \& Rahman, R. A. (2019). No matter how: Top-down effects of verbal and semantic category knowledge on early visual perception. \emph{Cognitive, Affective, \& Behavioral Neuroscience}, \emph{19}(4), 859--876. \url{https://doi.org/10.3758/s13415-018-00679-8}

\leavevmode\hypertarget{ref-mangun1995}{}%
Mangun, G. R. (1995). Neural mechanisms of visual selective attention. \emph{Psychophysiology}, \emph{32}(1), 4--18. \url{https://doi.org/10.1111/j.1469-8986.1995.tb03400.x}

\leavevmode\hypertarget{ref-mangun1991}{}%
Mangun, G. R., \& Hillyard, S. A. (1991). Modulations of sensory-evoked brain potentials indicate changes in perceptual processing during visual-spatial priming. \emph{Journal of Experimental Psychology. Human Perception and Performance}, \emph{17}(4), 1057--1074. \url{https://doi.org/10.1037//0096-1523.17.4.1057}

\leavevmode\hypertarget{ref-marr1982}{}%
Marr, D. (1982). \emph{Vision: A computational investigation into the human representation and processing of visual information}. W.H. Freeman.

\leavevmode\hypertarget{ref-matuschek2017}{}%
Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., \& Bates, D. (2017). Balancing {Type I} error and power in linear mixed models. \emph{Journal of Memory and Language}, \emph{94}, 305--315. \url{https://doi.org/10.1016/j.jml.2017.01.001}

\leavevmode\hypertarget{ref-mo2011}{}%
Mo, L., Xu, G., Kay, P., \& Tan, L.-H. (2011). Electrophysiological evidence for the left-lateralized effect of language on preattentive categorical perception of color. \emph{Proceedings of the National Academy of Sciences}, \emph{108}(34), 14026--14030. \url{https://doi.org/10.1073/pnas.1111860108}

\leavevmode\hypertarget{ref-oldfield1971}{}%
Oldfield, R. C. (1971). The assessment and analysis of handedness: The {Edinburgh} inventory. \emph{Neuropsychologia}, \emph{9}(1), 97--113. \url{https://doi.org/10.1016/0028-3932(71)90067-4}

\leavevmode\hypertarget{ref-panichello2013}{}%
Panichello, M. F., Cheung, O. S., \& Bar, M. (2013). Predictive feedback and conscious visual experience. \emph{Frontiers in Psychology}, \emph{3}. \url{https://doi.org/10.3389/fpsyg.2012.00620}

\leavevmode\hypertarget{ref-phelps2016}{}%
Phelps, E. A., Ling, S., \& Carrasco, M. (2016). Emotion facilitates perception and potentiates the perceptual benefits of attention. \emph{Psychological Science}. \url{https://journals.sagepub.com/doi/10.1111/j.1467-9280.2006.01701.x}

\leavevmode\hypertarget{ref-pylyshyn1999}{}%
Pylyshyn, Z. (1999). Is vision continuous with cognition? The case for cognitive impenetrability of visual perception. \emph{Behavioral and Brain Sciences}, \emph{22}(3), 341--365. \url{https://doi.org/10.1017/S0140525X99002022}

\leavevmode\hypertarget{ref-R-base}{}%
R Core Team. (2020). \emph{R: A language and environment for statistical computing}. R Foundation for Statistical Computing. \url{https://www.R-project.org/}

\leavevmode\hypertarget{ref-rabovsky2018}{}%
Rabovsky, M., Hansen, S. S., \& McClelland, J. L. (2018). Modelling the {N400} brain potential as change in a probabilistic representation of meaning. \emph{Nature Human Behaviour}, \emph{2}(9), 693--705. \url{https://doi.org/10.1038/s41562-018-0406-4}

\leavevmode\hypertarget{ref-rossion1999}{}%
Rossion, B., Delvenne, J.-F., Debatisse, D., Goffaux, V., Bruyer, R., Crommelinck, M., \& Gu√©rit, J.-M. (1999). Spatio-temporal localization of the face inversion effect: An event-related potentials study. \emph{Biological Psychology}, \emph{50}(3), 173--189. \url{https://doi.org/10.1016/S0301-0511(99)00013-7}

\leavevmode\hypertarget{ref-rossion2002}{}%
Rossion, B., Gauthier, I., Goffaux, V., Tarr, M. J., \& Crommelinck, M. (2002). Expertise training with novel objects leads to left-lateralized facelike electrophysiological responses. \emph{Psychological Science}. \url{https://journals.sagepub.com/doi/10.1111/1467-9280.00446}

\leavevmode\hypertarget{ref-rossion2011}{}%
Rossion, Bruno, \& Jacques, C. (2011). The {N170}: Understanding the time course of face perception in the human brain. In E. S. Kappenman \& S. J. Luck (Eds.), \emph{The {Oxford} handbook of event-related potential components} (pp. 115--142). Oxford University Press. \url{https://doi.org/10.1093/oxfordhb/9780195374148.001.0001}

\leavevmode\hypertarget{ref-rossion2004}{}%
Rossion, Bruno, Kung, C.-C., \& Tarr, M. J. (2004). Visual expertise with nonface objects leads to competition with the early perceptual processing of faces in the human occipitotemporal cortex. \emph{Proceedings of the National Academy of Sciences}, \emph{101}(40), 14521--14526. \url{https://doi.org/10.1073/pnas.0405613101}

\leavevmode\hypertarget{ref-sagiv2001}{}%
Sagiv, N., \& Bentin, S. (2001). Structural encoding of human and schematic faces: Holistic and part-based processes. \emph{Journal of Cognitive Neuroscience}, \emph{13}(7), 937--951. \url{https://doi.org/10.1162/089892901753165854}

\leavevmode\hypertarget{ref-samaha2018}{}%
Samaha, J., Boutonnet, B., Postle, B. R., \& Lupyan, G. (2018). Effects of meaningfulness on perception: Alpha-band oscillations carry perceptual expectations and influence early visual responses. \emph{Scientific Reports}, \emph{8}(1), 1--14. \url{https://doi.org/10.1038/s41598-018-25093-5}

\leavevmode\hypertarget{ref-schad2020}{}%
Schad, D. J., Vasishth, S., Hohenstein, S., \& Kliegl, R. (2020). How to capitalize on a priori contrasts in linear (mixed) models: A tutorial. \emph{Journal of Memory and Language}, \emph{110}, 104038. \url{https://doi.org/10.1016/j.jml.2019.104038}

\leavevmode\hypertarget{ref-suess2015}{}%
Suess, F., Rabovsky, M., \& Abdel Rahman, R. (2015). Perceiving emotions in neutral faces: Expression processing is biased by affective person knowledge. \emph{Social Cognitive and Affective Neuroscience}, \emph{10}(4), 531--536. \url{https://doi.org/10.1093/scan/nsu088}

\leavevmode\hypertarget{ref-tanaka2001}{}%
Tanaka, J. W., \& Curran, T. (2001). A neural basis for expert object recognition. \emph{Psychological Science}, \emph{12}(1), 43--47. \url{https://doi.org/10.1111/1467-9280.00308}

\leavevmode\hypertarget{ref-taylor2002}{}%
Taylor, M. J. (2002). Non-spatial attentional effects on {P1}. \emph{Clinical Neurophysiology}, \emph{113}(12), 1903--1908. \url{https://doi.org/10.1016/S1388-2457(02)00309-7}

\leavevmode\hypertarget{ref-teufel2017}{}%
Teufel, C., \& Nanay, B. (2017). How to (and how not to) think about top-down influences on visual perception. \emph{Consciousness and Cognition}, \emph{47}, 17--25. \url{https://doi.org/10.1016/j.concog.2016.05.008}

\leavevmode\hypertarget{ref-vanrossum2009}{}%
van Rossum, G., \& Drake, F. L. (2009). \emph{Python 3 reference manual}. CreateSpace.

\leavevmode\hypertarget{ref-vetter2014}{}%
Vetter, P., \& Newen, A. (2014). Varieties of cognitive penetration in visual perception. \emph{Consciousness and Cognition}, \emph{27}, 62--75. \url{https://doi.org/10.1016/j.concog.2014.04.007}

\leavevmode\hypertarget{ref-R-buildmer}{}%
Voeten, C. C. (2020). \emph{{buildmer}: Stepwise elimination and term reordering for mixed-effects regression}. \url{https://CRAN.R-project.org/package=buildmer}

\leavevmode\hypertarget{ref-vogel2000}{}%
Vogel, E. K., \& Luck, S. J. (2000). The visual {N1} component as an index of a discrimination process. \emph{Psychophysiology}, \emph{37}(2), 190--203.

\leavevmode\hypertarget{ref-weller2019}{}%
Weller, P. D., Rabovsky, M., \& Abdel Rahman, R. (2019). Semantic knowledge enhances conscious awareness of visual objects. \emph{Journal of Cognitive Neuroscience}, \emph{31}(8), 1216--1226. \url{https://doi.org/10.1162/jocn_a_01404}

\leavevmode\hypertarget{ref-yuille2006}{}%
Yuille, A., \& Kersten, D. (2006). Vision as bayesian inference: Analysis by synthesis? \emph{Trends in Cognitive Sciences}, \emph{10}(7), 301--308. \url{https://doi.org/10.1016/j.tics.2006.05.002}

\end{CSLReferences}


\end{document}
